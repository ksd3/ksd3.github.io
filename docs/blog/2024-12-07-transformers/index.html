<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-12-07">
<meta name="description" content="An implementation and demonstration of the transformer architecture from scratch using as few predefined libraries as possible.">

<title>Transformers from scratch in numpy – Kshitij Duraphe</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-be53553f563a7f8912e850d522ba7b71.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../textbooks.html"> 
<span class="menu-text">Textbooks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../talks.html"> 
<span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cv.html"> 
<span class="menu-text">Resume/CV</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#welcome" id="toc-welcome" class="nav-link active" data-scroll-target="#welcome">Welcome</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#what-is-the-fundamental-principle-of-a-transformer" id="toc-what-is-the-fundamental-principle-of-a-transformer" class="nav-link" data-scroll-target="#what-is-the-fundamental-principle-of-a-transformer">1. What is the fundamental principle of a transformer?</a></li>
  <li><a href="#in-very-general-terms-what-does-a-transformer-do" id="toc-in-very-general-terms-what-does-a-transformer-do" class="nav-link" data-scroll-target="#in-very-general-terms-what-does-a-transformer-do">2. In very general terms, what does a transformer do?</a></li>
  <li><a href="#what-specific-tasks-do-we-need-to-do-in-order-to-implement-a-transformer" id="toc-what-specific-tasks-do-we-need-to-do-in-order-to-implement-a-transformer" class="nav-link" data-scroll-target="#what-specific-tasks-do-we-need-to-do-in-order-to-implement-a-transformer">3. What specific tasks do we need to do in order to implement a transformer?</a></li>
  </ul></li>
  <li><a href="#preliminaries" id="toc-preliminaries" class="nav-link" data-scroll-target="#preliminaries">Preliminaries</a>
  <ul class="collapse">
  <li><a href="#tokenization" id="toc-tokenization" class="nav-link" data-scroll-target="#tokenization">Tokenization</a></li>
  <li><a href="#feeding-these-words-to-a-computer" id="toc-feeding-these-words-to-a-computer" class="nav-link" data-scroll-target="#feeding-these-words-to-a-computer">Feeding these words to a computer</a></li>
  <li><a href="#sequence-prediction" id="toc-sequence-prediction" class="nav-link" data-scroll-target="#sequence-prediction">Sequence prediction</a></li>
  <li><a href="#what-attention-does" id="toc-what-attention-does" class="nav-link" data-scroll-target="#what-attention-does">What attention does</a></li>
  <li><a href="#reconstructing-word-pairs-from-encoded-vectors" id="toc-reconstructing-word-pairs-from-encoded-vectors" class="nav-link" data-scroll-target="#reconstructing-word-pairs-from-encoded-vectors">Reconstructing word pairs from encoded vectors</a></li>
  <li><a href="#embeddings" id="toc-embeddings" class="nav-link" data-scroll-target="#embeddings">Embeddings</a></li>
  <li><a href="#attention" id="toc-attention" class="nav-link" data-scroll-target="#attention">Attention</a></li>
  <li><a href="#a-transformer-from-scratch-in-pure-numpy" id="toc-a-transformer-from-scratch-in-pure-numpy" class="nav-link" data-scroll-target="#a-transformer-from-scratch-in-pure-numpy">A transformer from scratch in pure numpy</a></li>
  <li><a href="#observations" id="toc-observations" class="nav-link" data-scroll-target="#observations">Observations</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Transformers from scratch in numpy</h1>
  <div class="quarto-categories">
    <div class="quarto-category">machine-learning</div>
    <div class="quarto-category">tutorial</div>
    <div class="quarto-category">deep-learning</div>
  </div>
  </div>

<div>
  <div class="description">
    An implementation and demonstration of the transformer architecture from scratch using as few predefined libraries as possible.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 7, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="welcome" class="level1">
<h1>Welcome</h1>
<p>Transformers are a type of <em>sequence to sequence</em> model, i.e., given a sequence of characters, which may be split into words, transformers are able to convert that sequence to another sequence in a way that preserves the original sequence’s ‘meaning’ <em>without using any predefined rules</em>. An example of a sequence-to-sequence task is text translation, such as converting the English sentence “I ate an apple” into the Italian equivalent: “Ho mangiato una mela.”</p>
<p>Transformers are an example of an <em>encoder-decoder</em> architecture. Encoder-decoder architectures take input data, squeeze it into a kind of secret code (often called ‘creating a latent representation’), and sometimes decode the squeezed data to perform useful tasks. An example of an encoder-decoder architecture and a task it may be used on would be using an <a href="https://en.wikipedia.org/wiki/Autoencoder">autoencoder</a> to denoise images.</p>
<p>This page contains an implementation and demonstration of the transformer architecture <strong>from scratch</strong> using as few predefined libraries as possible in order to give the reader an understanding of what really goes on in each step of the process. We avoid using predefined models or implementations of algorithms as much as possible.</p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>We address the following questions:</p>
<ol type="1">
<li>What is the fundamental principle of a transformer?</li>
<li>In very general terms, what does a transformer do?</li>
<li>What specific tasks do we need to do in order to implement a transformer?</li>
</ol>
<section id="what-is-the-fundamental-principle-of-a-transformer" class="level3">
<h3 class="anchored" data-anchor-id="what-is-the-fundamental-principle-of-a-transformer">1. What is the fundamental principle of a transformer?</h3>
<p>The fundamental principle of a transformer is that one-hot vectors can be used to look up particular rows of a matrix, and you can exploit this to selectively extract, combine, and mask information from your input to produce better outputs<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. A lot of readers, especially NLP enthusiasts, may immediately have a problem with this statement. After all, we have not used the most famous term associated with a transformer (attention) while stating this fundamental principle. We also did not say anything about feature embeddings, long-range dependencies, contextual relationships, and encodings - all terms that are used when talking/reading about transformers. This is done for two reasons. First, I believe that it is extremely important to understand what exactly is going on in terms of as many elementary operations as possible. I believe that this necessarily precludes using domain-specific jargon. Second, I am tired of reading innumerable blogs<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, code comments on GitHub<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, and slides that fail to give you understanding<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
</section>
<section id="in-very-general-terms-what-does-a-transformer-do" class="level3">
<h3 class="anchored" data-anchor-id="in-very-general-terms-what-does-a-transformer-do">2. In very general terms, what does a transformer do?</h3>
<p>A transformer takes in a <em>sequence</em> of <em>elements</em> (this sequence is often long), figures out how different elements in that sequence are related, then squeezes that sequence into a list of numbers that capture any inherent ‘meaning’<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. It then takes that list of numbers and unsqueezes it into a different sequence that tries to preserve the ‘meaning’ of the original sequence. As shown in the welcome section, transformers can be used for language translation.</p>
</section>
<section id="what-specific-tasks-do-we-need-to-do-in-order-to-implement-a-transformer" class="level3">
<h3 class="anchored" data-anchor-id="what-specific-tasks-do-we-need-to-do-in-order-to-implement-a-transformer">3. What specific tasks do we need to do in order to implement a transformer?</h3>
<ol type="1">
<li>Figure out a way to split individual elements in a sequence (hereafter ‘words and punctuation in a sentence’) and find a way to feed them to the computer.</li>
<li>Make the computer squeeze these words into a list of numbers it can understand.</li>
<li>Make the computer unsqueeze the list of numbers into words and form sentences in another language.</li>
</ol>
</section>
</section>
<section id="preliminaries" class="level2">
<h2 class="anchored" data-anchor-id="preliminaries">Preliminaries</h2>
<p>Suppose our input language only has four words (‘My’, ‘rabbit’, ‘likes’, ‘bananas’), and no punctuation at all. Sentences from our language could be ‘rabbit likes bananas’ or ‘My rabbit likes bananas’ or ‘likes My bananas’ or ‘bananas rabbit My’. For the sake of sanity, let’s assume that our language only has the sentence ‘My rabbit likes bananas’. We want to translate this into Italian: ‘Al mio coniglio piacciono le banane’. How do we feed our initial sentence to a computer?</p>
<section id="tokenization" class="level3">
<h3 class="anchored" data-anchor-id="tokenization">Tokenization</h3>
<p>We first need to split the sentence into individual words. Because our language does not have any sort of punctuation, we can do what is called whitespace tokenization. This is the most natural way of splitting an English sentence - assume that individual words are separated by a blank space, read through the sentence, and store all characters between two whitespaces as a single word <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List <span class="co">#for clean function annotation</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> whitespace_tokenizer(sentence: <span class="bu">str</span>) <span class="op">-&gt;</span> List[<span class="bu">str</span>]:</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Function that reads a string/sentence and outputs a list of strings, where each output string is a word in that sentence. Each word is considered to be delimited by whitespaces.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Input:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">        sentence: str - assumed nonempty for explanation purposes</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Output:</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">        list of strings</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    tokenized_sentence<span class="op">=</span>[] <span class="co">#final output, a list of words</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    current_word<span class="op">=</span>[] <span class="co">#list to store the current word</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">#The technique to whitespace tokenize the sentence is to iterate through it, and store each non-whitespace character in current_word. </span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Once a whitespace is encountered, append the contents of current_word to tokenized_sentence and clear current_word</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> sentence:</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i<span class="op">==</span><span class="st">" "</span>:</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> current_word:</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>                tokenized_sentence.append(<span class="st">''</span>.join(current_word)) <span class="co">#append to the list of tokens</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>                current_word<span class="op">=</span>[]<span class="co">#reset current_word</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>            current_word.append(i)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">#this still leaves the final word in, so add it last</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> current_word:</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        tokenized_sentence.append(<span class="st">''</span>.join(current_word))</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">#delete the current word from memory explicitly (not required)</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">del</span> current_word</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenized_sentence</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>english_sentence<span class="op">=</span><span class="st">"My rabbit likes bananas"</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Your list of tokens for the English sentence is:"</span>, whitespace_tokenizer(english_sentence))</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>english_tokenized_sentence<span class="op">=</span>whitespace_tokenizer(english_sentence) <span class="co">#save the tokenization in a list</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>italian_sentence<span class="op">=</span><span class="st">"Al mio coniglio piacciono le banane"</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Your list of tokens for the Italian sentence is:"</span>, whitespace_tokenizer(italian_sentence))</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>italian_tokenized_sentence<span class="op">=</span>whitespace_tokenizer(italian_sentence)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>Your list of tokens for the English sentence is: ['My', 'rabbit', 'likes', 'bananas']
Your list of tokens for the Italian sentence is: ['Al', 'mio', 'coniglio', 'piacciono', 'le', 'banane']</code></pre>
</section>
<section id="feeding-these-words-to-a-computer" class="level3">
<h3 class="anchored" data-anchor-id="feeding-these-words-to-a-computer">Feeding these words to a computer</h3>
<p>How do we feed these words into a computer? One way of doing it would be by assigning each individual word to a real number: [‘My’, ‘rabbit’, ‘likes’, ‘bananas’] -&gt; [‘935.88’, ‘-28124.4483957’, ‘3’, ‘-2’]. This is inefficient, as the amount of precision you would need to implement would increase computatational costs and storage requirements. A better way of storing a word would be to store it in a vector. Here is how we can do this. Given a vector, stored as a column matrix with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> rows, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> is the number of words in your vocabulary, replace one of the zeros with 1 such that the position of the 1 is unique for that particular word. Then, stack those vectors side by side to form a matrix where each row and column has only one 1 and all other elements are zero. Such vectors are called ‘one-hot’ vectors and this is a type of encoding called <strong>one-hot encoding</strong>.</p>
<p>This is illustrated below:</p>
‘My’=$
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
0\\
0\\
1\\
0
\end{pmatrix}</annotation></semantics></math>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>,</mo><mi>′</mi><mi>r</mi><mi>a</mi><mi>b</mi><mi>b</mi><mi>i</mi><mi>t</mi><mi>′</mi><mo>=</mo></mrow><annotation encoding="application/x-tex">, 'rabbit'=</annotation></semantics></math>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
0\\
1\\
0\\
0
\end{pmatrix}</annotation></semantics></math>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>,</mo><mi>′</mi><mi>l</mi><mi>i</mi><mi>k</mi><mi>e</mi><mi>s</mi><mi>′</mi><mo>=</mo></mrow><annotation encoding="application/x-tex">, 'likes'=</annotation></semantics></math>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
1\\
0\\
0\\
0
\end{pmatrix}</annotation></semantics></math>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>,</mo><mi>′</mi><mi>b</mi><mi>a</mi><mi>n</mi><mi>a</mi><mi>n</mi><mi>a</mi><mi>s</mi><mi>′</mi><mo>=</mo></mrow><annotation encoding="application/x-tex">, 'bananas'=</annotation></semantics></math>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
0\\
0\\
0\\
1
\end{pmatrix}</annotation></semantics></math>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>,</mo><mi>′</mi><mi>M</mi><mi>y</mi><mi>r</mi><mi>a</mi><mi>b</mi><mi>b</mi><mi>i</mi><mi>t</mi><mi>l</mi><mi>i</mi><mi>k</mi><mi>e</mi><mi>s</mi><mi>b</mi><mi>a</mi><mi>n</mi><mi>a</mi><mi>n</mi><mi>a</mi><mi>s</mi><mi>′</mi><mo>=</mo></mrow><annotation encoding="application/x-tex">,
'My rabbit likes bananas'=</annotation></semantics></math>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}</annotation></semantics></math>
<p>$</p>
<p>Let’s call the last matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math>, for ‘word matrix’. Observe that when any one-hot encoded is multiplied with another matrix, by <a href="https://www.dummies.com/article/academics-the-arts/math/pre-calculus/how-to-multiply-matrices-by-each-other-167710/">the rules of matrix multiplication</a>, the column of the second matrix corresponding to the position of the 1 in the column vector is ‘pulled out’<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. This is illustrated below:</p>
$
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
0 &amp; 0 &amp; 1 &amp; 0
\end{pmatrix}</annotation></semantics></math>
$ $
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.4</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mn>9</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn><mo>+</mo><mi>i</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>29</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>12</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>45.328539</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn><mi>i</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>32</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>32</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>12</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>43</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.482</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.212</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>241</mn></mtd><mtd columnalign="center" style="text-align: center"><msup><mi>e</mi><mi>π</mi></msup></mtd><mtd columnalign="center" style="text-align: center"><mi>T</mi><mi>R</mi><mi>E</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mi>T</mi><mi>R</mi><mi>E</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
0.2 &amp; 0.4 &amp; -9 &amp; 1+i \\
29 &amp; 12 &amp; 45.328539 &amp; 0 \\
2i &amp; 32 &amp; 2 &amp; 32 \\
12 &amp; 43 &amp; 0.482 &amp; 0.212 \\
241 &amp; e^{\pi} &amp; TREE(3) &amp; -TREE(3)
\end{pmatrix}</annotation></semantics></math>
<p>$ <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>−</mi><mn>9</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>45.328539</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.482</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>T</mi><mi>R</mi><mi>E</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">=\begin{pmatrix}
-9 \\
45.328539 \\
2 \\
0.482\\
TREE(3)
\end{pmatrix}</annotation></semantics></math></p>
<p>If you’re able to construct this second matrix, then it can potentially lead to something interesting<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. There are of course other ways to encode words, and for practical language tasks you take someone else’s encoding and use it, but it is important to understand the core principle.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Dict, Tuple</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_concatenated_matrix_from_tokens(tokens: List[<span class="bu">str</span>]) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Function that creates a concatenated one-hot encoded matrix from a tokenized sentence.</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Input:</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">        tokens: List containing tokens</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Output:</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">        tokenized_matrix: A 2-D one-hot encoded np.ndarray of tokens. Each row and column contains only one 1. Always square.</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">#The idea is to simply generate a diagonal matrix which will be one-hot encoded by definition</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Create a dictionary to map each token to a unique index</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    token_to_index<span class="op">=</span>{token: idx <span class="cf">for</span> idx, token <span class="kw">in</span> <span class="bu">enumerate</span>(tokens)} <span class="co">#get index-token pair from the input list</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Initialize the matrix with zeros</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    tokenized_matrix<span class="op">=</span>np.zeros((<span class="bu">len</span>(tokens), <span class="bu">len</span>(tokens)), dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Populate the one-hot encoded matrix</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> token <span class="kw">in</span> tokens:</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        index<span class="op">=</span>token_to_index[token]</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        tokenized_matrix[index][index]<span class="op">=</span><span class="dv">1</span>  <span class="co">#Set the diagonal element to 1</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenized_matrix, token_to_index <span class="co">#this second variable is returned to pull out a random token and corresponding vector later</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>english_onehot_matrix, english_token_to_index<span class="op">=</span>create_concatenated_matrix_from_tokens(english_tokenized_sentence)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The English one-hot encoded matrix is:</span><span class="ch">\n</span><span class="st">"</span>, english_onehot_matrix)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="co">#let's pull out a random token and a one-hot encoded vector to see how it pulls out specific features</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_random_token_and_vector(token_to_index: Dict[<span class="bu">str</span>, <span class="bu">int</span>], one_hot_matrix: np.ndarray) <span class="op">-&gt;</span> Tuple[<span class="bu">str</span>, np.ndarray]:</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="co">    Function that pulls a random token and its corresponding one-hot vector.</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a><span class="co">    Input:</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="co">        token_to_index: Dictionary mapping tokens to their indices</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="co">        one_hot_matrix: 2D NumPy array containing one-hot vectors</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="co">    Output:</span></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a><span class="co">        A tuple containing the random token and its corresponding one-hot vector</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Randomly select a token</span></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>    random_token<span class="op">=</span>random.choice(<span class="bu">list</span>(token_to_index.keys()))</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Get the corresponding one-hot vector</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>    one_hot_vector<span class="op">=</span>one_hot_matrix[token_to_index[random_token]]</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> random_token, one_hot_vector</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>random_token, corresponding_one_hot_vector<span class="op">=</span>get_random_token_and_vector(english_token_to_index, english_onehot_matrix)</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Let's pick a random token:"</span>, random_token, <span class="st">"</span><span class="ch">\n</span><span class="st">The corresponding one-hot vector is:"</span>, corresponding_one_hot_vector)</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a><span class="co">#generate a random matrix to demonstrate pulling out certain columns/rows</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>random_matrix<span class="op">=</span>np.random.rand(<span class="bu">len</span>(english_tokenized_sentence), <span class="bu">len</span>(english_tokenized_sentence))</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Multiplying an example random matrix</span><span class="ch">\n</span><span class="st">"</span>, random_matrix, <span class="st">"</span><span class="ch">\n</span><span class="st">by"</span>, random_token<span class="op">+</span><span class="st">"'s one-hot vector"</span>, corresponding_one_hot_vector, <span class="st">"</span><span class="ch">\n</span><span class="st">pulls out the row"</span>, np.matmul(corresponding_one_hot_vector, random_matrix), <span class="st">"</span><span class="ch">\n</span><span class="st">and multiplying by the tranpose of that vector pulls out the column:</span><span class="ch">\n</span><span class="st">"</span>, np.matmul(random_matrix, corresponding_one_hot_vector[:, np.newaxis]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>The English one-hot encoded matrix is:
 [[1 0 0 0]
 [0 1 0 0]
 [0 0 1 0]
 [0 0 0 1]]
Let's pick a random token: bananas 
The corresponding one-hot vector is: [0 0 0 1]
Multiplying an example random matrix
 [[0.48871169 0.52574402 0.78833029 0.7045616 ]
 [0.76188795 0.13720883 0.39406852 0.02774654]
 [0.27090269 0.35964049 0.20715361 0.3064574 ]
 [0.23701048 0.67718606 0.87441259 0.05116356]] 
by bananas's one-hot vector [0 0 0 1] 
pulls out the row [0.23701048 0.67718606 0.87441259 0.05116356] 
and multiplying by the tranpose of that vector pulls out the column:
 [[0.7045616 ]
 [0.02774654]
 [0.3064574 ]
 [0.05116356]]</code></pre>
</section>
<section id="sequence-prediction" class="level3">
<h3 class="anchored" data-anchor-id="sequence-prediction">Sequence prediction</h3>
<p>An immediate application of this specific kind of matrix multiplication is as follows. Suppose we have the following sentence in our four-word language: ‘My rabbit’. Our task is to predict the next word that comes after it<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>. One easy way of doing this is by observing that we only have four options. We can construct the following four sentences: | Next Word | Potential next (possible incomplete) sentence | |—-|—-| |My|My rabbit My| |rabbit|My rabbit rabbit| |likes|My rabbit likes| |bananas|My rabbit bananas|</p>
<p>How do we decide what word comes next? Well, we can’t decide on our own. Perhaps, to an alien whose language consists of only four words that sound exactly like English words, the sentence ‘My rabbit My’ would translate to English as ‘I am in need of two oranges and a deck of playing cards.’ The sentence ‘My rabbit rabbit’ would translate to ‘I am on fire’. The sentence ‘My rabbit rabbit rabbit rabbit bananas bananas rabbit bananas My likes likes bananas’ would translate to ‘Yes’ (remember, I have not put any limits on the length of the sentences!). The point of these examples is to show you that there is no way for us to predict the next word unless we have some idea of what it is going to be. One way to solve this problem is for a third party (say a talking dog) to step in and say, “I’ve been around these aliens, and I’ve observed that whenever they begin a sentence with ‘My rabbit’, the next word is ‘bananas’ 10% of the time, ‘likes’ 85% of the time, ‘rabbit’ 5% of the time, but ‘My’ never comes after ‘rabbit’. Is there some way for you to use this information? Also, whatever I say is always true.”</p>
<p>Since we have no better option, let’s trust the talking dog. We can in fact use its information in the following way. We can construct the following vector that shows the probability of predicting the next word after ‘rabbit’, if spoken by an alien.</p>
$<br>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtable><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">bananas</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">likes</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">rabbit</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">My</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd></mtr></mtable><mspace width="1.0em"></mspace><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.85</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.05</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{aligned}
&amp;\text{bananas}\\
&amp;\text{likes}\\
&amp;\text{rabbit}\\
&amp;\text{My}\\
\end{aligned}
\quad
\begin{bmatrix}
0.1 \\ 0.85 \\ 0.05 \\ 0
\end{bmatrix}</annotation></semantics></math>
<p><br>
$</p>
<p>This is somewhat useful. We know that there is a high chance that the next word in the sentence will be ‘likes’, so the possible incomplete sentence will now probably be ‘My rabbit likes’. But wait a minute. This vector of probabilities is like the column we pulled out of the matrix above. Is it possible to reconstruct this matrix? We can certainly do so - just assume that the dog is always true and start interrogating the dog about the probabilities of the next word <em>after</em> each word in the language, <em>regardless of the context</em>. Let’s assume the dog is happy to tell us this, so we now have the following matrix:</p>
$<br>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal"></mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">rabbit</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">My</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.9</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.07</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.03</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">rabbit</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.85</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.05</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">My</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.01</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.79</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{array}{r|cccc}
\text{} &amp; \text{bananas} &amp; \text{likes} &amp; \text{rabbit} &amp; \text{My} \\
\hline
\text{bananas} &amp; 0.1 &amp; 0 &amp; 0 &amp; 0 \\
\text{likes}   &amp; 0.9 &amp; 0 &amp; 0.07 &amp; 0.03  \\
\text{rabbit}  &amp; 0.1 &amp; 0.85 &amp; 0.05 &amp; 0 \\
\text{My}      &amp; 0.2 &amp; 0.01 &amp; 0.79 &amp; 0\\
\end{array}</annotation></semantics></math>
<p><br>
$</p>
<p>The matrix is read row-first, column-second i.e.&nbsp;the probability that the word ‘likes’ occurs after ‘rabbit’ is 0.85.</p>
<p>This tells us something about how the language is constructed. We know that if we hear an alien say ‘rabbit’, there is a very high chance that it will say ‘likes’ next. If we hear it say ‘likes’, there is a very high chance it will say ‘bananas’ next. There is also a very small chance it will say ‘My’ after ‘likes’, but it will never say ‘likes’ after ‘likes’. Therefore, <strong>to a first order</strong>, we can construct this matrix of probabilities that tells us what the next word in the language is going to be. In more formal terms, this is the <strong>stochastic matrix</strong> of a <strong>first-order Markov chain</strong>. It is first-order because the next word in the language only depends on the current word of the language.</p>
<p>But wait. Languages tend to have meaning when several words are used together. For example, in English, the word ‘cold’ refers to something whose molecules have a lower average kinetic energy than a reference object. However, the phrase ‘cold call’ means unsolicited phone calls typically made for business purposes. If you only know that the current word in the sentence is ‘cold’ and your probability matrix says that the word ‘call’ appears after ‘cold’ 70% of the time, you may say that the sentence ‘The water is cold.’ is incomplete and would complete it by saying ‘The water is cold call.’, which makes no sense<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>. What do we do?</p>
<p>The natural approach is to say, “I know combinations of words tend to change the meaning of a phrase<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>, but I don’t have any idea what constitutes a phrase in my unknown language, nor do I know if the ‘meaning’ of the sentence itself changes if a two words are present in adjacent positions<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>. Let me do the same thing I did for my first-order Markov chain. Instead of asking the talking dog the probabilities of the next word after my current word, I will look at the probabilities of the next word after my current word <strong>if another word is present in the sentence</strong>.”</p>
<p>Specifically, you can ask the talking dog the questions “If ‘rabbit likes’ is present in the sentence, what is the probability that the next word is ‘bananas’? What about ‘rabbit’, ‘My’, and ‘likes’? If ‘My rabbit’ is present, what is are the probabilities for the next word?” and construct the same matrix as we did above. Since we are looking at <em>every</em> pair of words, the number of rows of the matrix quickly grows in size. If there are 5 words in the language, the number of two-word pairs is 20 (obtained from $ 5 $) since the order matters. If there are 100 words, there are 4950 pairs. If there are 260,000 words (a quick Google search tells me that this is roughly the number of words in Italian) then there are 33799870000 pairs. And this is just for consecutive word pairings! If we attempt to look even further back i.e.&nbsp;three-word pairs, there will be even more. It is easy to see that the amount of space required to store this prediction matrix grows exponentially<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>.</p>
$<br>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal"></mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">rabbit</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">My</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">likes bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>T</mi><mi>R</mi><mi>E</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>4</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>T</mi><mi>R</mi><mi>E</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>5</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.02</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">My rabbit</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.826</mn></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>T</mi><mi>R</mi><mi>E</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>T</mi><mi>R</mi><mi>E</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>4</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mn>0.03</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">rabbit likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.024682</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.5</mn></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mi>π</mi><mrow><mn>10</mn><mi>e</mi></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">My bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.004</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.12</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.0018256151</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">and so on..</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{array}{r|cccc}
\text{} &amp; \text{bananas} &amp; \text{likes} &amp; \text{rabbit} &amp; \text{My} \\
\hline
\text{likes bananas} &amp; 0 &amp; \frac{TREE(4)}{TREE(5)} &amp; 0 &amp; 0.02 \\
\text{My rabbit}   &amp; 0 &amp; 0.826 &amp; \frac{TREE(3)}{TREE(4)} &amp; 0.03  \\
\text{rabbit likes}  &amp; 0.024682 &amp; 0.5 &amp; \frac{\pi}{10e} &amp; 0 \\
\text{My bananas}      &amp; 0.004 &amp; 0 &amp; 0.12 &amp; 0.0018256151\\
\text{and so on..}
\end{array}</annotation></semantics></math>
<p><br>
$</p>
<p>Given a sufficiently large prediction matrix containing all possible words and combinations of all possible lengths, we are able to predict the next word. Note that we have not said anything about <em>actually choosing the next word in this situation</em>, as this leads to problems. One problem is that we still do not know how to deal with cases where there is an equal chance of two words appearing after our current word. Let’s ignore this for now and focus on the biggest one: We want to avoid actually constructing any such matrix. Let’s try another trick. Let’s say, “The next word in a sentence is easier to predict if another word appears before the current word, <strong>but not necessarily directly before it</strong>. It may happen sometimes, but there is no reason why it should be like this. Here is my hypothesis. I think that it is easier to predict the next word in the sentence given a probability matrix containing all possible combinations of words <strong>where the second word is the current word</strong>.” This would look something like:</p>
$<br>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal"></mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">rabbit</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">My</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">likes bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.01</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">My bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.03</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">rabbit bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.00000023</mn></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mi>e</mi><mi>π</mi></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">bananas bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.004</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.046826</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.12</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.5151</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">and so on...</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{array}{r|cccc}
\text{} &amp; \text{bananas} &amp; \text{likes} &amp; \text{rabbit} &amp; \text{My} \\
\hline
\text{likes bananas} &amp; 0 &amp; 0 &amp; 0 &amp; 0.01 \\
\text{My bananas}   &amp; 0 &amp; 0 &amp; 0 &amp; 0.03  \\
\text{rabbit bananas}  &amp; 0.1 &amp; 0.00000023 &amp; \frac{e}{\pi} &amp; 0 \\
\text{bananas bananas}      &amp; 0.004 &amp; 0.046826 &amp; 0.12 &amp; 0.5151\\
\text{and so on...}
\end{array}</annotation></semantics></math>
<p><br>
$</p>
<p>This is much better to work with. Note that this is no longer a representation of a Markov chain, as we cannot simply look at the row corresponding to the current word and predict the next one. What can we do instead? We can say: “Okay, let’s say that these probabilities represent how much these pairs contribute to the next word in the sequence. We call these probabilities as <strong>votes</strong> and to predict the next word, we can <strong>sum over each column</strong> and compare these sums to determine the next word.” This is good, because now we are capturing <strong>long-range/skip dependencies</strong> in the language/sequence. Each row now represents one of many <strong>features</strong> that can describe our sequence at a particular point.</p>
<p>This is more clearly illustrated when you have, say, only two possible sentences in the language, but the main takeaway from actually doing this task for a set vocabulary and finite amount of sentences in the language is the observation that <strong>many elements in this probability matrix do not matter</strong>. They can either be so small that they are practically zero, or something like 0.5, which means that the next word is equally likely to appear regardless of the sentence, so it may not matter too much. What we are really interested in are elements we can distinguish. For example, suppose that the two sentences that were possible in our language were ‘My rabbit likes bananas’ and ‘My bananas likes rabbit’. If we had the incomplete sentence ‘My rabbit likes’, then we could ask the talking dog to give us this matrix, and what we would see is that the matrix has a large number of zeros but a 1 for ‘bananas’, enabling us to do this sum-over-columns technique to accurately predict the next word, even with a deep dependency. To be fair, this example is a bit contrived and longer sentences would illustrate the point much more easily.</p>
<p>This is still pretty bad. Real languages have a large number of words. Our talking dog could have only been around aliens who lived on a certain continent of the alien planet, which led to them developing their own dialect. If you think about it for just a little bit, it is easy to see that this sum-over-columns approach can end up telling us that the next word in the incomplete sentence ‘Japan is east of’ can be ‘China’, with a vote total of 2339, and ‘Mongolia’, with a vote total of 2340. Sure, we can still pick ‘Mongolia’ as the next word, but such a small difference can naturally be induced by statistical noise, unknowingly biased probability matrices, and other factors (such as us messing up the addition!). Are there ways to overcome this?</p>
<p>One approach is to modify the values in the columns before you sum them up, in a way that allows us to differentiate between them even more. One way to do this is to simply sum all the values and divide each value by the sum, to get a fractional representation. This is not very helpful - it preserves the same relation between the numbers in terms of scaling. Converting a column of [1,2,3] to [0.1666, 0.3334, 0.5] preserves the scaling. To overcome this, we utilize the <a href="https://en.wikipedia.org/wiki/Independence_of_irrelevant_alternatives">independence from irrelevant alternatives</a> axiom of decision theory, which states that irrelevant choices should not affect the relative probability of choosing between the things you really want to choose between. In mathematical terms, this means that if you have a set of numbers <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">x \in X</annotation></semantics></math> and you want to decide between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_1</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>2</mn></msub><annotation encoding="application/x-tex">x_2</annotation></semantics></math> but <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>3</mn></msub><mi>.</mi><mi>.</mi><mi>.</mi><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_3...x_n</annotation></semantics></math> are small values that are affecting your confidence, you can suppress <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>3</mn></msub><mi>.</mi><mi>.</mi><mi>.</mi><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_3...x_n</annotation></semantics></math> by replacing each variable in the following way:</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>→</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>=</mo><mi>n</mi></mrow></msubsup><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">x_i\rightarrow\frac{e^{x_i}}{\sum_{i=1}^{i=n}e^{x_i}}</annotation></semantics></math></p>
<p>This is the famous <strong>softmax</strong> function which is more or less used to convert a probability distribution to another probability distribution<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>. The important thing is that the softmax function suppressed irrelevant values (as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mi>k</mi></msup><mo>→</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">e^k \rightarrow 1</annotation></semantics></math> as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">k \rightarrow 0</annotation></semantics></math>).</p>
<p>However, the softmax function is also not applicable to our scenario. Suppose we did actually end up converting the votes to a probability distribution and summing them. What would it actually look like? Let’s do an example below:</p>
$
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.5</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>→</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.15706</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.258948</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.426933</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.15706</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
0 \\ 0.5 \\ 1 \\ 0
\end{pmatrix}
\rightarrow
\begin{pmatrix}
0.15706 \\ 0.258948 \\ 0.426933 \\ 0.15706
\end{pmatrix}</annotation></semantics></math>
<p>$</p>
<p>The sum of the softmaxed vector elements is 1. This is correct, because we did just convert it to a probability distribution. So this approach, while it did ‘suppress’ the smaller values, does not actually help us with voting. What can we do?</p>
<p>“Okay,” we say. “Let’s do something else. Instead of attempting to modify every value, let’s just discard the values that aren’t important<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>. First, let’s look at how to extract specific features from our matrix. We know one-hot encoded vectors pull relevant rows/columns out of the matrix, so let’s make a one-hot encoded vector to pull out the relevant features in the matrix in the following way. We construct a vector initially filled with zeros featuring all possible pairs in sentence where the second word is the current word, and the first word has all other words (possibly including the current word, depending on the dimensions of our matrix). Then, if the first word appears before the current word in the sentence, set that element to 1. This vector allows us to pull out the features of our probability matrix that are ‘active’ until that current point.”</p>
<p>This would look like:</p>
$<br>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtable><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">My likes</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">rabbit likes</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">bananas likes</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd></mtr></mtable><mspace width="1.0em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{aligned}
&amp;\text{My likes}\\
&amp;\text{rabbit likes}\\
&amp;\text{bananas likes}\\
\end{aligned}
\quad
\begin{pmatrix}
1 \\ 1 \\ 0
\end{pmatrix}</annotation></semantics></math>
^<br>
$ $<br>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal"></mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">rabbit</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">My</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">My likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">rabbit likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.03</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">bananas likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.00000023</mn></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mi>e</mi><mi>π</mi></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">and so on...</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{array}{r|cccc}
\text{} &amp; \text{bananas} &amp; \text{likes} &amp; \text{rabbit} &amp; \text{My} \\
\hline
\text{My likes} &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
\text{rabbit likes}   &amp; 1 &amp; 0 &amp; 0 &amp; 0.03  \\
\text{bananas likes}  &amp; 1 &amp; 0.00000023 &amp; \frac{e}{\pi} &amp; 0 \\
\text{and so on...}
\end{array}</annotation></semantics></math>
<p><br>
$</p>
<p>Note the transpose sign. We can see that the matrix multiplication will <strong>suppress</strong> those elements in the pulled out feature vectors where pairs taking into account words appearing after the current word in the sequence will be suppressed i.e.&nbsp;we cannot use knowledge of the entire sentence to predict the next sentence. We have now ‘suppressed the future’, but we still need to figure out what feature elements in our sequence are important. This is still an unknown, but what we can do is use <em>another</em> one-hot encoded vector to multiply this suppressed vector, to suppress even more. That is: we can compute the <strong>pairwise product</strong> to return a vector after multiplying our two vectors. Where can we get this second one-hot encoded vector? Let’s assume that the talking dog gave this to us. The point is that if we manage to suppress information then our voting becomes much stronger, as a lot of elements will be 0. The trick is now to find out how to create this second vector so that we suppress irrelevant information.</p>
<p>Incidentally, the second form of suppression is the idea behind <strong>attention</strong>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Dict, Tuple</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">#remember that the english sentence is "My rabbit likes bananas"</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_biased_probability_matrix(size: <span class="bu">int</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Function to generate a square probability matrix where each row and column </span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">    has one value significantly higher than the others.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Input:</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">        size: The number of rows and columns in the square matrix</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Output:</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co">        biased_matrix: A 2-D np.ndarray where each row and column has one high-probability value</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">#the technique is to generate a uniform matrix and randomly assign biased high probability values in each row and column</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    biased_matrix<span class="op">=</span>np.random.uniform(<span class="fl">0.01</span>, <span class="fl">0.05</span>, (size, size))</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">#generate high probability values for each row and column</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    high_probabilities<span class="op">=</span>np.random.uniform(<span class="fl">0.7</span>, <span class="fl">0.9</span>, size<span class="op">=</span>size)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">#ghuffle indices to randomly distribute the high probabilities across columns</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    indices<span class="op">=</span>np.arange(size)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    np.random.shuffle(indices)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">#assign one high probability per row and column</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(size):</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        biased_matrix[i, indices[i]]<span class="op">=</span>high_probabilities[i]</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">#normalize each row to sum to 1</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    biased_matrix<span class="op">=</span>biased_matrix<span class="op">/</span>biased_matrix.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> biased_matrix</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>size<span class="op">=</span><span class="dv">4</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>example_probability_matrix<span class="op">=</span>generate_biased_probability_matrix(size)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"As an example, a second-order probability matrix with skip dependencies given to us by the talking dog can be this:"</span>)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(example_probability_matrix)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(numbers: List[<span class="bu">int</span>])<span class="op">-&gt;</span>List[<span class="bu">int</span>]:</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a><span class="co">    Function to softmax a set of numbers</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="co">    Input:</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a><span class="co">        numbers: a list of integers</span></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a><span class="co">    Output:</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a><span class="co">        The list, softmaxed</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>    exponential_list<span class="op">=</span>np.exp(numbers)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>    softmaxed_numbers<span class="op">=</span>[np.exp(number)<span class="op">/</span><span class="bu">sum</span>(exponential_list) <span class="cf">for</span> number <span class="kw">in</span> numbers]</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> softmaxed_numbers</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> digram_one_hot_encoding(sentence: <span class="bu">str</span>, tokens: List[<span class="bu">str</span>], index_of_word: <span class="bu">int</span>) <span class="op">-&gt;</span> Tuple[np.ndarray, np.ndarray]:</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a><span class="co">    Generate one-hot encoding vectors for digrams based on a user-defined index.</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a><span class="co">    Input:</span></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a><span class="co">        sentence: Original sentence (used for context if needed).</span></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a><span class="co">        tokens: List of words (tokens) in the sentence.</span></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a><span class="co">        index_of_word: Index of the target word in the tokens list. Zero-indexed</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a><span class="co">    Output:</span></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a><span class="co">        A tuple containing:</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a><span class="co">            - A NumPy array of digrams (other words paired with the target word).</span></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a><span class="co">            - A 1D NumPy array where each element is 1 if the other word appears before the target word, 0 otherwise.</span></span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> index_of_word<span class="op">&lt;</span><span class="dv">0</span> <span class="kw">or</span> index_of_word<span class="op">&gt;=</span><span class="bu">len</span>(tokens):</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Invalid user-defined index. Must be within the range of the tokens list."</span>)</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>    <span class="co">#extract the target word. This of course assumes that the tokenization is sequential, but for illustrative purposes, it is fine</span></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>    target_word <span class="op">=</span> tokens[index_of_word]</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>    <span class="co">#generate digrams and the one-hot vector. The idea is that if the word appears before our word then set the index to 1, else 0</span></span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>    digrams<span class="op">=</span>[<span class="ss">f"</span><span class="sc">{</span>token<span class="sc">}</span><span class="ss">,</span><span class="sc">{</span>target_word<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> idx,token <span class="kw">in</span> <span class="bu">enumerate</span>(tokens) <span class="cf">if</span> idx<span class="op">!=</span>index_of_word]</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>    one_hot_vector<span class="op">=</span>np.array([<span class="dv">1</span> <span class="cf">if</span> idx<span class="op">&lt;</span>index_of_word <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(tokens)) <span class="cf">if</span> idx<span class="op">!=</span>index_of_word],dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(digrams), one_hot_vector</span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Our sentence is:"</span>, english_sentence)</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a><span class="co">#lets take index 2 ('likes' in "My rabbit likes bananas")</span></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>index_of_word<span class="op">=</span><span class="dv">2</span></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>incomplete_sentence<span class="op">=</span><span class="st">" "</span>.join([word <span class="cf">for</span> index,word <span class="kw">in</span> <span class="bu">enumerate</span>(english_tokenized_sentence) <span class="cf">if</span> index<span class="op">&lt;</span>index_of_word<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"We want an incomplete sentence. Our generation task is to predict the next word in:"</span>, incomplete_sentence)</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a><span class="co">#generate the digrams (word pairs)</span></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>digrams,digram_onehot_vector<span class="op">=</span>digram_one_hot_encoding(english_sentence,english_tokenized_sentence,index_of_word)</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Digrams:"</span>)</span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(digrams)</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"One-hot vector (without any future dependency):"</span>)</span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(digram_onehot_vector)</span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a><span class="co">#next, we create the attention mask by hand. specifically, we generate a ones vector equal to the size of the number of words in our sentence</span></span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a><span class="co">#then we randomly pick 2</span></span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>attention_mask<span class="op">=</span>np.ones((<span class="bu">len</span>(whitespace_tokenizer(incomplete_sentence))), dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>zero_indices <span class="op">=</span> np.random.choice(<span class="bu">len</span>(whitespace_tokenizer(incomplete_sentence)), size<span class="op">=</span>random.randrange(<span class="dv">0</span>,<span class="bu">len</span>(whitespace_tokenizer(incomplete_sentence))), replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a>attention_mask[zero_indices]<span class="op">=</span><span class="dv">0</span></span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Example attention mask: "</span>, attention_mask.T)</span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention applied to the non-future dependency capturing one-hot vector:"</span>, attention_mask<span class="op">*</span>digram_onehot_vector)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>As an example, a second-order probability matrix with skip dependencies given to us by the talking dog can be this:
[[0.03888508 0.03133871 0.88147493 0.04830127]
 [0.89819378 0.03108927 0.03605569 0.03466125]
 [0.02927141 0.88815681 0.02795252 0.05461926]
 [0.01763851 0.04167578 0.05967272 0.881013  ]]
Our sentence is: My rabbit likes bananas
We want an incomplete sentence. Our generation task is to predict the next word in: My rabbit likes
Digrams:
['My,likes' 'rabbit,likes' 'bananas,likes']
One-hot vector (without any future dependency):
[1 1 0]
Example attention mask:  [0 0 1]
Attention applied to the non-future dependency capturing one-hot vector: [0 0 0]</code></pre>
</section>
<section id="what-attention-does" class="level3">
<h3 class="anchored" data-anchor-id="what-attention-does">What attention does</h3>
<p>We have so far our non-future dependent feature vector. We have used it so far in conjunction with the probability matrix to predict the next step. If we want to suppress the feature vector with attention, does it make sense to use <em>another matrix</em> in the same way? Let’s assume that we have a bunch of attention masks/vectors. We can stack them either vertically or horizontally (depending on how exactly we want to implement our lookup) and generate a <em>matrix of attention masks</em>. We can then send our feature vector through the attention matrix and then send the result of that product into our probability matrix to predict the next word.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> send_vector_through_two_matrices(vector: np.ndarray, probability_matrix: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Function to send an input vector through an attention matrix and then a probability matrix</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">        vector: a 1D NumPy ndarray</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Output:</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">        a 1D NumPy ndarray of the same length as the input after being sent through two matrices</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">#the idea is to generate a 1d array of ones, replace half of the elements with 0, and shuffle and reshape it</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Your input vector is:</span><span class="ch">\n</span><span class="st">"</span>, vector)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    total_elements<span class="op">=</span>vector.shape[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    half<span class="op">=</span>total_elements<span class="op">//</span><span class="dv">2</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    flat<span class="op">=</span>np.ones(total_elements, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    flat[:half]<span class="op">=</span><span class="dv">0</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    np.random.shuffle(flat)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    example_attention_matrix<span class="op">=</span>flat.reshape((vector.shape[<span class="dv">0</span>],vector.shape[<span class="dv">0</span>]))</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"An example attention matrix is:</span><span class="ch">\n</span><span class="st">"</span>, example_attention_matrix)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    result_1<span class="op">=</span>np.matmul(vector, example_attention_matrix)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"After multiplying, you get:</span><span class="ch">\n</span><span class="st">"</span>,result_1)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"After multiplying the result with the probability matrix, you get"</span>, np.matmul(result_1, probability_matrix))</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>example_probability_matrix<span class="op">=</span>np.random.rand(<span class="bu">len</span>(digram_onehot_vector), <span class="bu">len</span>(digram_onehot_vector))</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"An example probability matrix is:</span><span class="ch">\n</span><span class="st">"</span>, example_probability_matrix)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>send_vector_through_two_matrices(digram_onehot_vector, example_probability_matrix)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>An example probability matrix is:
 [[0.6600921  0.31488209 0.01236613]
 [0.51302175 0.04638934 0.92027961]
 [0.38042472 0.01756476 0.29279993]]
Your input vector is:
 [1 1 0]
An example attention matrix is:
 [[1 1 0]
 [1 0 1]
 [0 0 1]]
After multiplying, you get:
 [2 1 1]
After multiplying the result with the probability matrix, you get [2.21363067 0.69371827 1.2378118 ]</code></pre>
</section>
<section id="reconstructing-word-pairs-from-encoded-vectors" class="level3">
<h3 class="anchored" data-anchor-id="reconstructing-word-pairs-from-encoded-vectors">Reconstructing word pairs from encoded vectors</h3>
<p>What do we do with the result of the attention step? Sure, we have a vector that has encoded word pairs (a second-order model), but we don’t yet have a way to deconstruct that vector back into a word pair. How do we do this? So far, matrix multiplication has enabled us to encode sentences into vectors and selectively mask the irrelevant word pairs. Can we apply matrix multiplication to <em>decode</em> a word pair? The answer is yes. Matrix multiplications are in fact what <strong>neural networks</strong> do.</p>
<section id="neural-networks" class="level4">
<h4 class="anchored" data-anchor-id="neural-networks">Neural Networks</h4>
<p>Neural networks are a deep learning architecture based on the neuron-synapse structure of the human brain. Neural networks consist of a series of blocks called artificial neurons (hereafter just ‘neuron’) stacked vertically in layers. Each neuron has the possibility to receive an input and pass along an output to another neuron. To decide whether it passes along an output, a neuron sums up all of its inputs (which are weighted by the value of the connection along which the output travels) and applies a function, called an <em>activation function</em>, to that sum. Depending on the result of the activation function, the neuron sends an output to one or more neurons depending on how many it connects to. Mathematically, passing data through a neuron is equivalent to applying the mathematical function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>i</mi><mo>=</mo><mi>n</mi></mrow></msubsup><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\sum_{i=0}^{i=n} w_i x_i)</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> is the activation function, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_i</annotation></semantics></math> is the weight along an input path, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics></math> the actual value being sent along that input path.</p>
<p>Neural networks are equivalent to matrix multiplication. Why is this so? Suppose there are two layers in our neural network. The first layer has 3 neurons, and the second layer has two neurons. Let’s name the latter two <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding="application/x-tex">n_1</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding="application/x-tex">n_2</annotation></semantics></math>. Also, let’s assume that each neuron in the first layer sends is connected to each neuron in the second layer. Therefore, the outputs of the first layer are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>x</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">x_1,x_2,x_3</annotation></semantics></math> and the weights of the paths along which they are sent are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>11</mn></msub><mo>,</mo><msub><mi>w</mi><mn>12</mn></msub><mo>,</mo><msub><mi>w</mi><mn>21</mn></msub><mo>,</mo><msub><mi>w</mi><mn>22</mn></msub><mo>,</mo><msub><mi>w</mi><mn>31</mn></msub><mo>,</mo><msub><mi>w</mi><mn>32</mn></msub></mrow><annotation encoding="application/x-tex">w_{11}, w_{12}, w_{21}, w_{22}, w_{31}, w_{32}</annotation></semantics></math> for each neuron-neuron path.</p>
<p>The input to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding="application/x-tex">n_1</annotation></semantics></math> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>11</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>21</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><msub><mi>w</mi><mn>31</mn></msub><msub><mi>x</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">w_{11}x_1+w_{21}x_2+w_{31}x_3</annotation></semantics></math>, and the input to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding="application/x-tex">n_2</annotation></semantics></math> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>12</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>22</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>32</mn></msub><msub><mi>x</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">w_{12}x_1+w_{22}x_1+w_{32}x_3</annotation></semantics></math>. Writing these out in the form of a system of linear expressions:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>w</mi><mn>11</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>21</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><msub><mi>w</mi><mn>31</mn></msub><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>w</mi><mn>12</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>22</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>32</mn></msub><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
w_{11}x_1+w_{21}x_2+w_{31}x_3 \\
w_{12}x_1+w_{22}x_1+w_{32}x_3
\end{aligned}</annotation></semantics></math></p>
<p>it is easy to see that this is in fact a matrix multiplication:<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">WX</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>w</mi><mn>11</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>w</mi><mn>21</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>w</mi><mn>31</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>w</mi><mn>21</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>w</mi><mn>22</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>w</mi><mn>32</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>X</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">W=\begin{pmatrix}w_{11}&amp;w_{21}&amp;w_{31}\\w_{21}&amp;w_{22}&amp;w_{32}\end{pmatrix}, X=\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}</annotation></semantics></math>. Each layer also tends to have a <strong>bias</strong> term, so the input to a layer can be represented as the equation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mi>X</mi><mo>+</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">WX+B</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> is the bias matrix (usually a column vector containing the same value). The activation function is applied to this resultant vector. This means that <strong>the output of a layer of a neural network can be represented by a vector</strong>.</p>
<p>#### Properties of neural networks</p>
<ol type="1">
<li>Neural networks are <strong>universal function approximators</strong>. This means that given a large-enough network with nonlinear activation functions, neural networks can model <strong>any</strong> mapping between elements of a domain <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> and a codomain <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>. However, this does NOT tell us how many neurons and layers we need or what the activation function is.</li>
<li>Neural networks can model nonlinear relationships between elements. While the discussion of linear decision boundaries is beyond the scope of this tutorial, it is enough to know that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, the activation function, is usually chosen to be something like <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> or the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a>.</li>
<li>Since neural networks are just matrix multiplication, they are extremely fast to train on computers<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></li>
</ol>
<p>#### Activation functions</p>
<p>If the activation function is linear (such as a simple multiplier <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>2</mn><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x)=2x</annotation></semantics></math>), the neural network cannot learn linear relationships, no matter how big you make it and how long you train it for. Nonlinear activation functions are necessary to learn nonlinear relationships i.e.&nbsp;relationships between two variables that cannot be explained by a matrix multiplication (attention is linear!). Activation functions like ReLU and the sigmoid function are chosen not only because they are easy to compute, but because of a certain requirement explained below.</p>
</section>
<section id="training-a-neural-network" class="level4">
<h4 class="anchored" data-anchor-id="training-a-neural-network">Training a neural network</h4>
<p>Since each layer of a neural network can be expressed as a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, a neural network can be thought of as a large composite function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>W</mi><mn>1</mn></msub><msub><mi>f</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>.</mi><mi>.</mi><mi>.</mi><msub><mi>f</mi><mi>n</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>W</mi><mi>n</mi></msub><msub><mi>X</mi><mi>n</mi></msub><mo>+</mo><msub><mi>B</mi><mi>N</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mi>.</mi><mi>.</mi><mo>+</mo><msub><mi>B</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mi>B</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_1(W_1f_2(...f_n(W_nX_n+B_N)...+B_1)+B_0)</annotation></semantics></math>. Given a training set of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">{(x_i,y_i)}</annotation></semantics></math> pairs, the loss of the model is a <em>cost function</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">C(y_i,g(x_i))</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(x_i)</annotation></semantics></math> is the prediction of the neural network (the large composite function defined above) for the input variable <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics></math>. We want to minimize this cost function, as it means our neural network has learned the relationship between the input and output variables. This is done with an algorithm called <strong>backpropagation</strong>.</p>
<p>Backpropagation is an algorithm that utilizes the technique of gradient descent - given a cost function, we calculate its gradient with respect to the weights and biases of the neural network. According to the learning rate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>, gradient descent updates the weights and biases of the neural network according to the rule <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mi>/</mi><msub><mi>B</mi><mtext mathvariant="normal">next</mtext></msub><mo>=</mo><mi>W</mi><mi>/</mi><msub><mi>B</mi><mtext mathvariant="normal">current</mtext></msub><mo>−</mo><mi>α</mi><mfrac><mrow><mi>∂</mi><mi>C</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>,</mo><mi>W</mi><mi>/</mi><msub><mi>B</mi><mtext mathvariant="normal">current</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>∂</mi><mi>W</mi><mi>/</mi><mi>B</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">
W/B_{\text{next}}=W/B_{\text{current}}-\alpha\frac{\partial C(X,W/B_{\text{current}})}{\partial W/B}
</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>/</mi><annotation encoding="application/x-tex">/</annotation></semantics></math> is read as ‘OR’. We <em>subtract</em> the gradient because the gradient denotes the direction of maximum increase, so the direction of maximum decrease would be the direction opposite to it. We apply this many times (this is therefore a <strong>greedy</strong> algorithm) to find the local minimum of the function i.e.&nbsp;the values of all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math> and all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> where the cost function is minimized. After each step of updating the gradients, we have to compute the prediction of the network again, in order to prepare for the next step. This is called the <strong>forward pass</strong> or <strong>forward step</strong> through the network, and must be computed repeatedly, making the process a back-and-forth.</p>
</section>
<section id="calculating-backpropagation" class="level4">
<h4 class="anchored" data-anchor-id="calculating-backpropagation">Calculating backpropagation</h4>
<p>Let us consider a neural network set up in the following way. We have <strong>3</strong> input neurons (that is, 3 input variables) and <strong>1</strong> hidden layer with 4 neurons, and <strong>one</strong> output neuron. For the sake of this example, assume that every neuron in one layer is connected to every neuron in the next layer and every neuron in the previous layer. Such a neural network is called a <strong>fully-connected neural network</strong>.</p>
<p>We have already seen how matrices can represent the input to a layer. Let’s represent the output of a layer by a vector after an activation function is applied to each neuron<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>. We will now define several variables that mathematically represent each layer. For each layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>, we have:</p>
<p>$$ n_l \</p>
<p>w_l n_{l} n{l-1} \</p>
<p>b_l n_l \</p>
<p>a_l \</p>
<p>z_l a_l b_l \</p>
<p>g_l $$</p>
<p>We can write down some straightforward formluae after these definitions.</p>
<p>$$ z_l=w_la_l+b_l\</p>
<p>a_l=g_l(z_l)\ $$</p>
<p>The next question is choosing an appropriate cost function for our task. Let us think about our task for a moment. Since we have been using probabilities all along to predict the next word in our sequence, it is appropriate to use a cost function that tells us how good our probability prediction is. The classical cost function that is used to explain backpropagation is the <strong>squared error function</strong>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">real value</mtext><mo>−</mo><mtext mathvariant="normal">predicted value</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><annotation encoding="application/x-tex">(\text{real value}-\text{predicted value})^2</annotation></semantics></math>. This function is natural because it is simply the difference between what we predict and what the truth is, and it is squared for many reasons such as being the variance of the unbiased estimator (if used in its mean-square form) and also being easily differentiable<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>. But we effectively want to measure the difference between a <em>predicted probability distribution</em> and the <em>real probability distribution</em>, as we are predicting the next word in a sentence. This requires having a maximum likelihood estimate of the parameters, and when working with Bernoulli-distributed variables (such as one-hot encoded vectors) the <strong>cross-entropy loss</strong> function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>−</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mi>l</mi><mi>n</mi><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>l</mi><mi>n</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">-(yln\hat{y}+(1-y)ln(1-\hat{y}))</annotation></semantics></math> minimizes the maximum likelihood estimate<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>.</p>
<p>Let’s see how the derivative is calculated.</p>
<p>$$ = \[5pt]</p>
<p>= $$</p>
<p>by a simple application of the <a href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a>. This can easily be extended by observing that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mn>3</mn></msub><annotation encoding="application/x-tex">z_3</annotation></semantics></math> is a function of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding="application/x-tex">a_2</annotation></semantics></math>, which is a function of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mn>2</mn></msub><annotation encoding="application/x-tex">z_2</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mn>2</mn></msub><annotation encoding="application/x-tex">z_2</annotation></semantics></math> is a function of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>2</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">w_2, b_2, a_1</annotation></semantics></math>.</p>
<p>$$ = \[5pt]</p>
<p>= $$</p>
<p>and similarly for the first (input) layer, named layer 0.</p>
<p>The next task is setting up a way to recursively calculate the derivative of the cost function for any arbitrary layer’s weight and bias. The general equation for this is</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mfrac><mrow><mi>∂</mi><mi>C</mi></mrow><mrow><mi>∂</mi><msub><mi>w</mi><mi>l</mi></msub></mrow></mfrac></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>C</mi></mrow><mrow><mi>∂</mi><msub><mi>z</mi><mi>l</mi></msub></mrow></mfrac><mfrac><mrow><mi>∂</mi><msub><mi>z</mi><mi>l</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>w</mi><mi>l</mi></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mfrac><mrow><mi>∂</mi><mi>C</mi></mrow><mrow><mi>∂</mi><msub><mi>b</mi><mi>l</mi></msub></mrow></mfrac></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>C</mi></mrow><mrow><mi>∂</mi><msub><mi>z</mi><mi>l</mi></msub></mrow></mfrac><mfrac><mrow><mi>∂</mi><msub><mi>z</mi><mi>l</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>b</mi><mi>l</mi></msub></mrow></mfrac></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\frac{\partial C}{\partial w_l}&amp;=\frac{\partial C}{\partial z_l}\frac{\partial z_l}{\partial w_l} \\[5pt]
\frac{\partial C}{\partial b_l}&amp;=\frac{\partial C}{\partial z_l}\frac{\partial z_l}{\partial b_l}
\end{aligned}</annotation></semantics></math></p>
<p>There are two observations we can make from this. The first is that it is straightfoward to numerically calculate the partial derivative for the last/output layer, and we can store this value in order to avoid repeated computation wherever possible. The second is that you need to calculate the change in the gradient for the last layer, then use that changed gradient for the layer before that one, and so on, ‘back-propagating’ the errors.</p>
<p>Let’s choose a nice activation function such as the sigmoid function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mi>−</mi><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma(x)=\frac{1}{1+e^{-x}}</annotation></semantics></math> for this. Let us precompute the partial derivative of the output layer, since we’ll be needing it. Note that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>⊙</mi><annotation encoding="application/x-tex">\odot</annotation></semantics></math> denotes the Hadamard, or element-wise product.</p>
<p>$$ == ’(z_o) \[5pt]</p>
<p>=-(-) \[5pt]</p>
<p>’(z_o)=a_l(1-a_l) \[5pt]</p>
<p>y \[5pt]</p>
<p>=a_o-y $$</p>
<p>This is a very nice result. It is now easy to see that for any inner layer, we can repeatedly apply the chain rule to derive the partial derivatives. If you go about doing this you end up with the following results:</p>
<p>$$ =w_{l+1}^T ’(z_l) \[5pt]</p>
<p>=a_{l-1} \[5pt]</p>
<p>a_{l-1}^T \[5pt]</p>
<p>=1 \[5pt]</p>
<p>= $$</p>
<p>A more complete derivation can be found <a href="https://towardsdatascience.com/deriving-backpropagation-with-cross-entropy-loss-d24811edeaf9">here</a>, but the fundamental idea is the same.</p>
<p>We can now implement this in Python and train the neural network from scratch.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co">Neural Network Implementation in NumPy</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">Inputs:</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">    None</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">Outputs:</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Fully functional neural network trained on synthetic data</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Apply the sigmoid activation function element-wise</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co">        x: a NumPy ndarray, the input array</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Outputs:</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co">        a NumPy ndarray with sigmoid applied element-wise</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>x))<span class="co">#sigmoid formula</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid_prime(x: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the derivative of the sigmoid function element-wise</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="co">        x: a NumPy ndarray, the input array</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="co">    Outputs:</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="co">        a NumPy ndarray with the derivative of sigmoid applied element-wise</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sigmoid(x)<span class="op">*</span>(<span class="fl">1.0</span><span class="op">-</span>sigmoid(x))<span class="co">#sigmoid derivative formula</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetwork:</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="co">    Define a simple feedforward neural network</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,architecture: np.ndarray):</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="co">        initializer for the neural network class</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a><span class="co">            architecture: a NumPy array representing the number of neurons in each layer</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.L<span class="op">=</span>architecture.size<span class="op">-</span><span class="dv">1</span><span class="co">#number of layers (excluding input layer)</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n<span class="op">=</span>architecture<span class="co">#number of neurons in each layer</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parameters<span class="op">=</span>{}<span class="co">#dictionary to store weights, biases, and activations</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>        <span class="co">#initialize weights and biases for each layer</span></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="va">self</span>.L<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.parameters[<span class="st">'W'</span><span class="op">+</span><span class="bu">str</span>(i)]<span class="op">=</span>np.random.randn(<span class="va">self</span>.n[i],<span class="va">self</span>.n[i<span class="op">-</span><span class="dv">1</span>])<span class="op">*</span><span class="fl">0.01</span><span class="co">#small random weights</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.parameters[<span class="st">'b'</span><span class="op">+</span><span class="bu">str</span>(i)]<span class="op">=</span>np.ones((<span class="va">self</span>.n[i],<span class="dv">1</span>))<span class="co">#biases initialized to 1</span></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.parameters[<span class="st">'z'</span><span class="op">+</span><span class="bu">str</span>(i)]<span class="op">=</span>np.ones((<span class="va">self</span>.n[i],<span class="dv">1</span>))<span class="co">#pre-activation values initialized to 1</span></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.parameters[<span class="st">'a'</span><span class="op">+</span><span class="bu">str</span>(i)]<span class="op">=</span>np.ones((<span class="va">self</span>.n[i],<span class="dv">1</span>))<span class="co">#activations initialized to 1</span></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parameters[<span class="st">'a0'</span>]<span class="op">=</span>np.ones((<span class="va">self</span>.n[<span class="dv">0</span>],<span class="dv">1</span>))<span class="co">#input layer activation</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parameters[<span class="st">'C'</span>]<span class="op">=</span><span class="dv">1</span><span class="co">#placeholder for cost value</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.derivatives<span class="op">=</span>{}<span class="co">#dictionary to store derivatives</span></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_propagate(<span class="va">self</span>,X: np.ndarray):</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a><span class="co">        Perform forward propagation</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a><span class="co">            X: a column vector representing one training example</span></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a><span class="co">            None</span></span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parameters[<span class="st">'a0'</span>]<span class="op">=</span>X<span class="co">#set input layer activation</span></span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="va">self</span>.L<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.parameters[<span class="st">'z'</span><span class="op">+</span><span class="bu">str</span>(l)]<span class="op">=</span>np.dot(<span class="va">self</span>.parameters[<span class="st">'W'</span><span class="op">+</span><span class="bu">str</span>(l)],<span class="va">self</span>.parameters[<span class="st">'a'</span><span class="op">+</span><span class="bu">str</span>(l<span class="op">-</span><span class="dv">1</span>)])<span class="op">+</span><span class="va">self</span>.parameters[<span class="st">'b'</span><span class="op">+</span><span class="bu">str</span>(l)]<span class="co">#W*a+b</span></span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.parameters[<span class="st">'a'</span><span class="op">+</span><span class="bu">str</span>(l)]<span class="op">=</span>sigmoid(<span class="va">self</span>.parameters[<span class="st">'z'</span><span class="op">+</span><span class="bu">str</span>(l)])<span class="co">#apply sigmoid activation</span></span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_cost(<span class="va">self</span>,y: np.ndarray):</span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a><span class="co">        function to compute the cost for one training example</span></span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a><span class="co">            y: the true label for the input sample</span></span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a><span class="co">            None</span></span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parameters[<span class="st">'C'</span>]<span class="op">=-</span>(y<span class="op">*</span>np.log(<span class="va">self</span>.parameters[<span class="st">'a'</span><span class="op">+</span><span class="bu">str</span>(<span class="va">self</span>.L)])<span class="op">+</span>(<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span>np.log(<span class="dv">1</span><span class="op">-</span><span class="va">self</span>.parameters[<span class="st">'a'</span><span class="op">+</span><span class="bu">str</span>(<span class="va">self</span>.L)]))<span class="co">#binary cross-entropy loss</span></span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_derivatives(<span class="va">self</span>,y: np.ndarray):</span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a><span class="co">        function to compute gradients for all parameters</span></span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a><span class="co">            y: the true label for the input sample</span></span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a><span class="co">            None</span></span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.derivatives[<span class="st">'dz'</span><span class="op">+</span><span class="bu">str</span>(<span class="va">self</span>.L)]<span class="op">=</span><span class="va">self</span>.parameters[<span class="st">'a'</span><span class="op">+</span><span class="bu">str</span>(<span class="va">self</span>.L)]<span class="op">-</span>y<span class="co">#last layer gradient</span></span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.derivatives[<span class="st">'dW'</span><span class="op">+</span><span class="bu">str</span>(<span class="va">self</span>.L)]<span class="op">=</span>np.dot(<span class="va">self</span>.derivatives[<span class="st">'dz'</span><span class="op">+</span><span class="bu">str</span>(<span class="va">self</span>.L)],<span class="va">self</span>.parameters[<span class="st">'a'</span><span class="op">+</span><span class="bu">str</span>(<span class="va">self</span>.L<span class="op">-</span><span class="dv">1</span>)].T)<span class="co">#last layer weights gradient</span></span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.derivatives[<span class="st">'db'</span><span class="op">+</span><span class="bu">str</span>(<span class="va">self</span>.L)]<span class="op">=</span><span class="va">self</span>.derivatives[<span class="st">'dz'</span><span class="op">+</span><span class="bu">str</span>(<span class="va">self</span>.L)]<span class="co">#last layer bias gradient</span></span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.L<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb9-93"><a href="#cb9-93" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.derivatives[<span class="st">'dz'</span><span class="op">+</span><span class="bu">str</span>(l)]<span class="op">=</span>np.dot(<span class="va">self</span>.parameters[<span class="st">'W'</span><span class="op">+</span><span class="bu">str</span>(l<span class="op">+</span><span class="dv">1</span>)].T,<span class="va">self</span>.derivatives[<span class="st">'dz'</span><span class="op">+</span><span class="bu">str</span>(l<span class="op">+</span><span class="dv">1</span>)])<span class="op">*</span>sigmoid_prime(<span class="va">self</span>.parameters[<span class="st">'z'</span><span class="op">+</span><span class="bu">str</span>(l)])<span class="co">#hidden layer gradient</span></span>
<span id="cb9-94"><a href="#cb9-94" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.derivatives[<span class="st">'dW'</span><span class="op">+</span><span class="bu">str</span>(l)]<span class="op">=</span>np.dot(<span class="va">self</span>.derivatives[<span class="st">'dz'</span><span class="op">+</span><span class="bu">str</span>(l)],<span class="va">self</span>.parameters[<span class="st">'a'</span><span class="op">+</span><span class="bu">str</span>(l<span class="op">-</span><span class="dv">1</span>)].T)<span class="co">#hidden layer weights gradient</span></span>
<span id="cb9-95"><a href="#cb9-95" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.derivatives[<span class="st">'db'</span><span class="op">+</span><span class="bu">str</span>(l)]<span class="op">=</span><span class="va">self</span>.derivatives[<span class="st">'dz'</span><span class="op">+</span><span class="bu">str</span>(l)]<span class="co">#hidden layer bias gradient</span></span>
<span id="cb9-96"><a href="#cb9-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-97"><a href="#cb9-97" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_parameters(<span class="va">self</span>,alpha: <span class="bu">float</span>):</span>
<span id="cb9-98"><a href="#cb9-98" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-99"><a href="#cb9-99" aria-hidden="true" tabindex="-1"></a><span class="co">        function to update network parameters using gradient descent</span></span>
<span id="cb9-100"><a href="#cb9-100" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb9-101"><a href="#cb9-101" aria-hidden="true" tabindex="-1"></a><span class="co">            alpha: learning rate</span></span>
<span id="cb9-102"><a href="#cb9-102" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb9-103"><a href="#cb9-103" aria-hidden="true" tabindex="-1"></a><span class="co">            None</span></span>
<span id="cb9-104"><a href="#cb9-104" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-105"><a href="#cb9-105" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="va">self</span>.L<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb9-106"><a href="#cb9-106" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.parameters[<span class="st">'W'</span><span class="op">+</span><span class="bu">str</span>(l)]<span class="op">-=</span>alpha<span class="op">*</span><span class="va">self</span>.derivatives[<span class="st">'dW'</span><span class="op">+</span><span class="bu">str</span>(l)]<span class="co">#update weights</span></span>
<span id="cb9-107"><a href="#cb9-107" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.parameters[<span class="st">'b'</span><span class="op">+</span><span class="bu">str</span>(l)]<span class="op">-=</span>alpha<span class="op">*</span><span class="va">self</span>.derivatives[<span class="st">'db'</span><span class="op">+</span><span class="bu">str</span>(l)]<span class="co">#update biases</span></span>
<span id="cb9-108"><a href="#cb9-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-109"><a href="#cb9-109" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>,x: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb9-110"><a href="#cb9-110" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-111"><a href="#cb9-111" aria-hidden="true" tabindex="-1"></a><span class="co">        function to predict the output for a given input</span></span>
<span id="cb9-112"><a href="#cb9-112" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb9-113"><a href="#cb9-113" aria-hidden="true" tabindex="-1"></a><span class="co">            x: a column vector representing one input sample</span></span>
<span id="cb9-114"><a href="#cb9-114" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb9-115"><a href="#cb9-115" aria-hidden="true" tabindex="-1"></a><span class="co">            a NumPy array representing the predicted output</span></span>
<span id="cb9-116"><a href="#cb9-116" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-117"><a href="#cb9-117" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.forward_propagate(x)<span class="co">#perform forward propagation</span></span>
<span id="cb9-118"><a href="#cb9-118" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.parameters[<span class="st">'a'</span><span class="op">+</span><span class="bu">str</span>(<span class="va">self</span>.L)]<span class="co">#return output layer activation</span></span>
<span id="cb9-119"><a href="#cb9-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-120"><a href="#cb9-120" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>,X: np.ndarray,Y: np.ndarray,num_iter: <span class="bu">int</span>,alpha: <span class="bu">float</span><span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb9-121"><a href="#cb9-121" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-122"><a href="#cb9-122" aria-hidden="true" tabindex="-1"></a><span class="co">        function to train the neural network</span></span>
<span id="cb9-123"><a href="#cb9-123" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb9-124"><a href="#cb9-124" aria-hidden="true" tabindex="-1"></a><span class="co">            X: a NumPy array where each row is a training example</span></span>
<span id="cb9-125"><a href="#cb9-125" aria-hidden="true" tabindex="-1"></a><span class="co">            Y: a NumPy array of true labels</span></span>
<span id="cb9-126"><a href="#cb9-126" aria-hidden="true" tabindex="-1"></a><span class="co">            num_iter: number of iterations</span></span>
<span id="cb9-127"><a href="#cb9-127" aria-hidden="true" tabindex="-1"></a><span class="co">            alpha: learning rate</span></span>
<span id="cb9-128"><a href="#cb9-128" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb9-129"><a href="#cb9-129" aria-hidden="true" tabindex="-1"></a><span class="co">            None</span></span>
<span id="cb9-130"><a href="#cb9-130" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-131"><a href="#cb9-131" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(num_iter):</span>
<span id="cb9-132"><a href="#cb9-132" aria-hidden="true" tabindex="-1"></a>            c<span class="op">=</span><span class="dv">0</span><span class="co">#cumulative cost</span></span>
<span id="cb9-133"><a href="#cb9-133" aria-hidden="true" tabindex="-1"></a>            n_c<span class="op">=</span><span class="dv">0</span><span class="co">#correct predictions count</span></span>
<span id="cb9-134"><a href="#cb9-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-135"><a href="#cb9-135" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">0</span>]):</span>
<span id="cb9-136"><a href="#cb9-136" aria-hidden="true" tabindex="-1"></a>                x<span class="op">=</span>X[i].reshape((X[i].size,<span class="dv">1</span>))<span class="co">#reshape input to column vector</span></span>
<span id="cb9-137"><a href="#cb9-137" aria-hidden="true" tabindex="-1"></a>                y<span class="op">=</span>Y[i]<span class="co">#true label</span></span>
<span id="cb9-138"><a href="#cb9-138" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.forward_propagate(x)<span class="co">#forward propagation</span></span>
<span id="cb9-139"><a href="#cb9-139" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.compute_cost(y)<span class="co">#compute cost</span></span>
<span id="cb9-140"><a href="#cb9-140" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.compute_derivatives(y)<span class="co">#compute gradients</span></span>
<span id="cb9-141"><a href="#cb9-141" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.update_parameters(alpha)<span class="co">#update parameters</span></span>
<span id="cb9-142"><a href="#cb9-142" aria-hidden="true" tabindex="-1"></a>                c<span class="op">+=</span><span class="va">self</span>.parameters[<span class="st">'C'</span>]<span class="co">#accumulate cost</span></span>
<span id="cb9-143"><a href="#cb9-143" aria-hidden="true" tabindex="-1"></a>                y_pred<span class="op">=</span><span class="va">self</span>.predict(x)<span class="co">#make prediction</span></span>
<span id="cb9-144"><a href="#cb9-144" aria-hidden="true" tabindex="-1"></a>                y_pred<span class="op">=</span>(y_pred<span class="op">&gt;</span><span class="fl">0.5</span>)<span class="co">#convert probability to binary</span></span>
<span id="cb9-145"><a href="#cb9-145" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> y_pred<span class="op">==</span>y:</span>
<span id="cb9-146"><a href="#cb9-146" aria-hidden="true" tabindex="-1"></a>                    n_c<span class="op">+=</span><span class="dv">1</span><span class="co">#correct prediction count</span></span>
<span id="cb9-147"><a href="#cb9-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-148"><a href="#cb9-148" aria-hidden="true" tabindex="-1"></a>            c<span class="op">=</span>c<span class="op">/</span>X.shape[<span class="dv">0</span>]<span class="co">#average cost</span></span>
<span id="cb9-149"><a href="#cb9-149" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">'Iteration:'</span>,<span class="bu">iter</span>)</span>
<span id="cb9-150"><a href="#cb9-150" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Cost:"</span>,c)</span>
<span id="cb9-151"><a href="#cb9-151" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Accuracy:"</span>,(n_c<span class="op">/</span>X.shape[<span class="dv">0</span>])<span class="op">*</span><span class="dv">100</span>)</span>
<span id="cb9-152"><a href="#cb9-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-153"><a href="#cb9-153" aria-hidden="true" tabindex="-1"></a><span class="co">#generate synthetic data</span></span>
<span id="cb9-154"><a href="#cb9-154" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)<span class="co">#reproducibility</span></span>
<span id="cb9-155"><a href="#cb9-155" aria-hidden="true" tabindex="-1"></a>X<span class="op">=</span>np.random.rand(<span class="dv">200</span>,<span class="dv">7</span>)<span class="co">#200 samples, 7 features</span></span>
<span id="cb9-156"><a href="#cb9-156" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>(np.<span class="bu">sum</span>(X,axis<span class="op">=</span><span class="dv">1</span>)<span class="op">&gt;</span><span class="fl">3.5</span>).astype(<span class="bu">int</span>).reshape(<span class="dv">200</span>,<span class="dv">1</span>)<span class="co">#labels based on sum of features</span></span>
<span id="cb9-157"><a href="#cb9-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-158"><a href="#cb9-158" aria-hidden="true" tabindex="-1"></a><span class="co">#split data into training and testing sets</span></span>
<span id="cb9-159"><a href="#cb9-159" aria-hidden="true" tabindex="-1"></a>split_ratio<span class="op">=</span><span class="fl">0.7</span><span class="co">#70% training data</span></span>
<span id="cb9-160"><a href="#cb9-160" aria-hidden="true" tabindex="-1"></a>split_index<span class="op">=</span><span class="bu">int</span>(X.shape[<span class="dv">0</span>]<span class="op">*</span>split_ratio)</span>
<span id="cb9-161"><a href="#cb9-161" aria-hidden="true" tabindex="-1"></a>indices<span class="op">=</span>np.arange(X.shape[<span class="dv">0</span>])</span>
<span id="cb9-162"><a href="#cb9-162" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(indices)</span>
<span id="cb9-163"><a href="#cb9-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-164"><a href="#cb9-164" aria-hidden="true" tabindex="-1"></a>X_train,X_test<span class="op">=</span>X[indices[:split_index]],X[indices[split_index:]]</span>
<span id="cb9-165"><a href="#cb9-165" aria-hidden="true" tabindex="-1"></a>y_train,y_test<span class="op">=</span>y[indices[:split_index]],y[indices[split_index:]]</span>
<span id="cb9-166"><a href="#cb9-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-167"><a href="#cb9-167" aria-hidden="true" tabindex="-1"></a><span class="co">#define architecture</span></span>
<span id="cb9-168"><a href="#cb9-168" aria-hidden="true" tabindex="-1"></a>architecture<span class="op">=</span>np.array([<span class="dv">7</span>,<span class="dv">5</span>,<span class="dv">1</span>])<span class="co">#7 input features, 5 hidden neurons, 1 output</span></span>
<span id="cb9-169"><a href="#cb9-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-170"><a href="#cb9-170" aria-hidden="true" tabindex="-1"></a><span class="co">#initialize and train the neural network</span></span>
<span id="cb9-171"><a href="#cb9-171" aria-hidden="true" tabindex="-1"></a>nn<span class="op">=</span>NeuralNetwork(architecture)</span>
<span id="cb9-172"><a href="#cb9-172" aria-hidden="true" tabindex="-1"></a>nn.fit(X_train,y_train,num_iter<span class="op">=</span><span class="dv">50</span>,alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb9-173"><a href="#cb9-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-174"><a href="#cb9-174" aria-hidden="true" tabindex="-1"></a><span class="co">#evaluate the model</span></span>
<span id="cb9-175"><a href="#cb9-175" aria-hidden="true" tabindex="-1"></a>correct_predictions<span class="op">=</span><span class="dv">0</span></span>
<span id="cb9-176"><a href="#cb9-176" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X_test.shape[<span class="dv">0</span>]):</span>
<span id="cb9-177"><a href="#cb9-177" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>X_test[i].reshape((X_test[i].size,<span class="dv">1</span>))<span class="co">#reshape to column vector</span></span>
<span id="cb9-178"><a href="#cb9-178" aria-hidden="true" tabindex="-1"></a>    y_true<span class="op">=</span>y_test[i]<span class="co">#true label</span></span>
<span id="cb9-179"><a href="#cb9-179" aria-hidden="true" tabindex="-1"></a>    y_pred<span class="op">=</span>nn.predict(x)<span class="co">#prediction</span></span>
<span id="cb9-180"><a href="#cb9-180" aria-hidden="true" tabindex="-1"></a>    y_pred<span class="op">=</span>(y_pred<span class="op">&gt;</span><span class="fl">0.5</span>).astype(<span class="bu">int</span>)<span class="co">#convert to binary</span></span>
<span id="cb9-181"><a href="#cb9-181" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> y_pred<span class="op">==</span>y_true:</span>
<span id="cb9-182"><a href="#cb9-182" aria-hidden="true" tabindex="-1"></a>        correct_predictions<span class="op">+=</span><span class="dv">1</span><span class="co">#count correct predictions</span></span>
<span id="cb9-183"><a href="#cb9-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-184"><a href="#cb9-184" aria-hidden="true" tabindex="-1"></a><span class="co">#calculate test accuracy</span></span>
<span id="cb9-185"><a href="#cb9-185" aria-hidden="true" tabindex="-1"></a>test_accuracy<span class="op">=</span>(correct_predictions<span class="op">/</span>X_test.shape[<span class="dv">0</span>])<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-186"><a href="#cb9-186" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test Accuracy:"</span>,test_accuracy)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>Iteration: 0
Cost: [[0.72566273]]
Accuracy: 64.28571428571429
Iteration: 1
Cost: [[0.71523798]]
Accuracy: 66.42857142857143
Iteration: 2
Cost: [[0.70509586]]
Accuracy: 66.42857142857143
Iteration: 3
Cost: [[0.6864599]]
Accuracy: 67.85714285714286
Iteration: 4
Cost: [[0.65347682]]
Accuracy: 75.71428571428571
Iteration: 5
Cost: [[0.60664836]]
Accuracy: 82.14285714285714
Iteration: 6
Cost: [[0.55558826]]
Accuracy: 91.42857142857143
Iteration: 7
Cost: [[0.5072171]]
Accuracy: 93.57142857142857
Iteration: 8
Cost: [[0.46296521]]
Accuracy: 95.0
Iteration: 9
Cost: [[0.42292781]]
Accuracy: 96.42857142857143
Iteration: 10
Cost: [[0.38716204]]
Accuracy: 97.85714285714285
Iteration: 11
Cost: [[0.35557885]]
Accuracy: 97.85714285714285
Iteration: 12
Cost: [[0.32788988]]
Accuracy: 97.85714285714285
Iteration: 13
Cost: [[0.30368962]]
Accuracy: 97.85714285714285
Iteration: 14
Cost: [[0.28254309]]
Accuracy: 97.85714285714285
Iteration: 15
Cost: [[0.26403564]]
Accuracy: 97.85714285714285
Iteration: 16
Cost: [[0.24779326]]
Accuracy: 97.85714285714285
Iteration: 17
Cost: [[0.23348834]]
Accuracy: 98.57142857142858
Iteration: 18
Cost: [[0.22083889]]
Accuracy: 98.57142857142858
Iteration: 19
Cost: [[0.20960497]]
Accuracy: 98.57142857142858
Iteration: 20
Cost: [[0.19958386]]
Accuracy: 98.57142857142858
Iteration: 21
Cost: [[0.190605]]
Accuracy: 98.57142857142858
Iteration: 22
Cost: [[0.18252507]]
Accuracy: 98.57142857142858
Iteration: 23
Cost: [[0.17522359]]
Accuracy: 98.57142857142858
Iteration: 24
Cost: [[0.16859908]]
Accuracy: 98.57142857142858
Iteration: 25
Cost: [[0.16256584]]
Accuracy: 98.57142857142858
Iteration: 26
Cost: [[0.15705123]]
Accuracy: 98.57142857142858
Iteration: 27
Cost: [[0.15199344]]
Accuracy: 98.57142857142858
Iteration: 28
Cost: [[0.14733967]]
Accuracy: 98.57142857142858
Iteration: 29
Cost: [[0.14304459]]
Accuracy: 98.57142857142858
Iteration: 30
Cost: [[0.13906911]]
Accuracy: 98.57142857142858
Iteration: 31
Cost: [[0.13537942]]
Accuracy: 98.57142857142858
Iteration: 32
Cost: [[0.1319461]]
Accuracy: 98.57142857142858
Iteration: 33
Cost: [[0.12874349]]
Accuracy: 98.57142857142858
Iteration: 34
Cost: [[0.12574908]]
Accuracy: 98.57142857142858
Iteration: 35
Cost: [[0.12294312]]
Accuracy: 99.28571428571429
Iteration: 36
Cost: [[0.12030816]]
Accuracy: 99.28571428571429
Iteration: 37
Cost: [[0.11782877]]
Accuracy: 99.28571428571429
Iteration: 38
Cost: [[0.11549126]]
Accuracy: 99.28571428571429
Iteration: 39
Cost: [[0.11328345]]
Accuracy: 100.0
Iteration: 40
Cost: [[0.11119447]]
Accuracy: 100.0
Iteration: 41
Cost: [[0.1092146]]
Accuracy: 100.0
Iteration: 42
Cost: [[0.10733513]]
Accuracy: 100.0
Iteration: 43
Cost: [[0.10554823]]
Accuracy: 100.0
Iteration: 44
Cost: [[0.10384686]]
Accuracy: 100.0
Iteration: 45
Cost: [[0.10222465]]
Accuracy: 100.0
Iteration: 46
Cost: [[0.10067586]]
Accuracy: 100.0
Iteration: 47
Cost: [[0.09919528]]
Accuracy: 100.0
Iteration: 48
Cost: [[0.09777818]]
Accuracy: 100.0
Iteration: 49
Cost: [[0.09642027]]
Accuracy: 100.0
Test Accuracy: 93.33333333333333</code></pre>
<p>Here’s a neat observation. If a neural network is simple a two-layer network with non-linear activation, <em>it is simply a matrix multiplication to new inputs</em>. Therefore, we now have a way to <strong>learn</strong> that second-order probability matrix!</p>
<p>Given a neural network which can learn vector-vector relationships, it is easy to see that we can reconstruct our word-pair combinations from a vector that is the result of an attention step. Suppose we have a vector corresponding to the words ‘My’, ‘rabbit’, and ‘likes’, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}1\\1\\1 \end{pmatrix}</annotation></semantics></math>, the result of making it non-future-dependent is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}1\\1\\0 \end{pmatrix}</annotation></semantics></math>, and our attention step results in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}1\\0\\0 \end{pmatrix}</annotation></semantics></math>. We now pass this as <em>input</em> to a neural network and compute the output. The output will simply map our input vector to an output vector. The insight is that you can train the network to accurately map the result of our attention step to word pairs! The next obvious question is how to generate these word pairs. But before that, we need to notice that our neural network gets trained quickly when the input-output training sets have a small number of elements. It quickly becomes unwieldy when you think about practical languages, like the 260,000 Italian words mentioned above. This moves us on to our next topic, <strong>embeddings</strong>.</p>
</section>
</section>
<section id="embeddings" class="level3">
<h3 class="anchored" data-anchor-id="embeddings">Embeddings</h3>
<p>To make our neural network work well, we need a large amount of input-output data. This is impractical at the scale of even small, real languages - there are simply too many words. Generating one-hot encoding matrices by vertically stacking the vectors, even with techniques to store sparse matrices, is still impractical once we think about storing word pairs and triplets. We need some way to reduce these one-hot matrices in size so storing them becomes more efficient. This is the same problem we tried to solve with neural networks: converting an input vector into another vector. In our case, we want to convert a large vector to a smaller vector such that enough information is retained. This smaller vector will be called the <strong>embedding</strong> vector.</p>
<p>Based on all that we’ve seen so far, it is obvious that this conversion will be done with matrix multiplication. The question is how to make this new matrix. We can do the same thing we did before (training a neural network) or we can do something completely different. Here is an example. Suppose we want to embed ‘My’, ‘rabbit’, ‘likes’, and ‘bananas’ into 2 dimensions. We know that their one-hot encoding is an identity matrix, possibly with its columns shuffled. We can arbitrarily define a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">4 \times 2</annotation></semantics></math> matrix that will project this matrix into a smaller matrix. We can then say that column 1 (representing, say, ‘rabbit’) of the initial matrix is now replaced by column 1 of the new matrix. This is perfectly fine. But is this meaningful?</p>
<p>What do we want from a ‘good’ embedding? Broadly, a good embedding should be useful for practical tasks. There is no use in embedding words and making the transformer’s neural network harder to train. We might want to apply clustering algorithms to word embeddings to find out, for example, how many nouns there are in a large <em>corpus</em> (plural <em>corpora</em>) of text. We might also want to know what words are related in an unknown language. For someone trying to embed English, making sure that the embeddings for ‘rabbit’ and ‘hare’ are closer(i.e.&nbsp;their difference is closer to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mn>0</mn><mo accent="true">→</mo></mover><annotation encoding="application/x-tex">\vec{0}</annotation></semantics></math>) than the embeddings for ‘rabbit’ and ‘desk’ is important if training a model to explain what it says in pictures of lagomorphs in office environments. Embeddings should probably also capture <strong>context awareness</strong> - ‘hot dog’ must have a different embedding compared to both ‘hot’ and ‘dog’. They should also not be too low-dimensional; we might lose important information.</p>
<p>It is beyond the scope of this tutorial to discuss good embedding algorithms. Fortunately, there is a straightforward algorithm we can use to embed our four-word language. Let’s map them on the unit circle with a randomly generated matrix, as we have been doing so far.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">#define words</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>words<span class="op">=</span>[<span class="st">'My'</span>,<span class="st">'rabbit'</span>,<span class="st">'likes'</span>,<span class="st">'bananas'</span>]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">#ensure that the words are sufficiently apart for better visibility-say 30 degrees</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>angles<span class="op">=</span>np.sort(np.random.choice(np.linspace(<span class="dv">0</span>,<span class="dv">2</span><span class="op">*</span>np.pi,<span class="dv">360</span>,endpoint<span class="op">=</span><span class="va">False</span>),<span class="bu">len</span>(words),replace<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">#compute unit circle coordinates</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>unit_circle_vectors<span class="op">=</span>np.array([[np.cos(angle),np.sin(angle)] <span class="cf">for</span> angle <span class="kw">in</span> angles])</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">#define initial one-hot vectors</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>one_hot_vectors<span class="op">=</span>np.eye(<span class="bu">len</span>(words), dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co">#plot embeddings</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    x,y<span class="op">=</span>unit_circle_vectors[i]</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    plt.scatter(x,y,label<span class="op">=</span>word)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    plt.text(x<span class="op">+</span><span class="fl">0.05</span>,y<span class="op">+</span><span class="fl">0.05</span>,word,fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">#draw the arrow</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    plt.arrow(<span class="dv">0</span>,<span class="dv">0</span>,x,y,head_width<span class="op">=</span><span class="fl">0.05</span>,head_length<span class="op">=</span><span class="fl">0.1</span>,fc<span class="op">=</span><span class="st">'blue'</span>,ec<span class="op">=</span><span class="st">'red'</span>,alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the unit circle</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>theta<span class="op">=</span>np.linspace(<span class="dv">0</span>,<span class="dv">2</span><span class="op">*</span>np.pi,<span class="dv">100</span>)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>circle_x<span class="op">=</span>np.cos(theta)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>circle_y<span class="op">=</span>np.sin(theta)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>plt.plot(circle_x,circle_y,color<span class="op">=</span><span class="st">'gray'</span>,linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"2D Embedding of a 4-Word Language on Unit Circle"</span>)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"X-axis"</span>)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Y-axis"</span>)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span class="dv">0</span>,color<span class="op">=</span><span class="st">'gray'</span>,linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="dv">0</span>,color<span class="op">=</span><span class="st">'gray'</span>,linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'equal'</span>)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Print initial one-hot vectors and unit circle embeddings</span></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Initial One-Hot Vectors:"</span>)</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>one_hot_vectors[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Embedded Vectors on Unit Circle:"</span>)</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>unit_circle_vectors[i]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/transformers_13_0.png" class="img-fluid figure-img"></p>
<figcaption>png</figcaption>
</figure>
</div>
<pre><code>Initial One-Hot Vectors:
My:[1 0 0 0]
rabbit:[0 1 0 0]
likes:[0 0 1 0]
bananas:[0 0 0 1]

Embedded Vectors on Unit Circle:
My:[0.9961947  0.08715574]
rabbit:[-0.54463904 -0.83867057]
likes:[ 0.60181502 -0.79863551]
bananas:[ 0.99254615 -0.12186934]</code></pre>
<section id="giving-important-to-the-position-of-words-in-a-sentence-while-embedding-text" class="level4">
<h4 class="anchored" data-anchor-id="giving-important-to-the-position-of-words-in-a-sentence-while-embedding-text">Giving important to the position of words in a sentence while embedding text</h4>
<p>Let’s think a bit about our two-word non-future-dependent vectors. The only condition we have applied so far is that the value in the column matrix row wherever a word appears ahead of our current word should be 0. If a word appeared 1364 places before our current word but its value was deemed ‘important’ by attention, its corresponding row value would be 1. This is impractical. We know that it is unlikely that a word appearing 1364 places before the current word affects it. How can we quantify this?</p>
<p>The solution is to do it heuristically. Let’s figure out what exactly the task is. We have to add some additional information in a word’s embedding that denotes the position of the word in a sentence. This additional information is called the <strong>positional encoding</strong>. We want to satisfy a few criteria:</p>
<ol type="1">
<li>The encoding must be unique for each word in the sequence even if that word appears again. The sentence ‘My rabbit likes bananas but my friend’s rabbit doesn’t.’ should have different encoding values for the first and second occurrences of ‘rabbit’.</li>
<li>If we have to add positional encoding to sentences of different lengths, the ‘distance’ between two pieces of information should remain constant. This means that ‘My rabbit likes bananas. My friend’s rabbit does not like bananas.’ should encode the first and second occurrences of ‘rabbit’ in a way that the difference between the additional information added should be the same as the difference between ‘likes’ and ‘does’. This ensures that the two sentences are recognized as part of a ‘speech’.</li>
<li>We should be able to generalize to any sentence length easily with <em>bounded</em> and <em>deterministic</em> values (i.e.&nbsp;do not train a neural network).</li>
</ol>
<p>Essentially, we need to find a function whose codomain is a vector of the same size of the embedding that is:</p>
<ol type="1">
<li>Easy to compute</li>
<li>Periodic</li>
<li>Has bounded values.</li>
</ol>
<p>and iterate through the sentence, computing the function at the index of every word. To add the information to the word embedding, we can literally add the two vectors. This encodes positional information in the embedding.</p>
<p>A function that satisfies these criteria is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><mi>ℕ</mi><mo>→</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">f: \mathbb{N} \rightarrow \mathbb{R}^d</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="0.222em"></mspace><msub><mi>f</mi><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>ω</mi><mi>k</mi></msub><mo>⋅</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>i</mi><mo>=</mo><mn>2</mn><mi>k</mi></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>ω</mi><mi>k</mi></msub><mo>⋅</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>i</mi><mo>=</mo><mn>2</mn><mi>k</mi><mo>+</mo><mn>1</mn></mtd></mtr></mtable></mrow><mspace width="0.222em"></mspace></mrow><annotation encoding="application/x-tex">
\
f_i(t) = 
\begin{cases} 
\sin(\omega_k \cdot t), &amp; \text{if } i = 2k \\ 
\cos(\omega_k \cdot t), &amp; \text{if } i = 2k + 1 
\end{cases}
\
</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> is the number of rows in the column vector representation of the embedding vector, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> are simply ways to denote even and odd positions (i.e.&nbsp;the first row of the encoding vector is a sine, the second row is a cosine, the third row is a sine, and so on) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>=</mo><mfrac><mn>1</mn><msup><mn>10000</mn><mfrac><mrow><mn>2</mn><mi>k</mi></mrow><mi>d</mi></mfrac></msup></mfrac></mrow><annotation encoding="application/x-tex">w=\frac{1}{10000^{\frac{2k}{d}}}</annotation></semantics></math>. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math> has been chosen completely by guesswork. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> simply denotes the row number. Another good property is that since the encoding are periodic functions, you have also put in some information saying ‘the encoding of a word <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> places away from the current word is so-and-so’. Their periodicity implies that they can be represented as a linear combination of earlier encodings. I want to reiterate that this is a heuristic that works and theoretical justifications for this do not really exist. It works because you have differentiated between sentences such as ‘I just computed five times three plus two’ and ‘I just computed five plus three times two’ which have different underlying meanings.</p>
</section>
<section id="converting-embeddings-back-into-words" class="level4">
<h4 class="anchored" data-anchor-id="converting-embeddings-back-into-words">Converting embeddings back into words</h4>
<p>We finally discuss actually choosing the next word in the sequence. Suppose that have taken a sentence, tokenized it, converted to one-hot encoding, embedded these encodings, added position embeddings, and then trained a neural network to predict an output vector. The final step is to convert this output vector, <em>which is also an embedding</em>, back into a vector that represents the target vocabulary. We do not want to convert it back into a one-hot vector. How will you choose the next word?</p>
<p>Let’s do what is straightforward - multiply it with a matrix. This time, we are taking a smaller vector and making it a larger one. This comes with some caveats. We are making a column matrix bigger. If we want to make <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}1\\2\\3\end{pmatrix}</annotation></semantics></math> bigger, say to twice its size, how can we do it? Should we do <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}1\\0\\2\\0\\3\\0\end{pmatrix}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}0\\1\\2\\0\\3\\0\end{pmatrix}</annotation></semantics></math>, or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}0\\0\\1\\0\\2\\3\end{pmatrix}</annotation></semantics></math>? We can set up a matrix to do any one of these transformations, but we cannot set up a matrix that does this for all possible input vectors. This is because you will be solving an overdetermined system of equations. We have no choice but to accept this, so we have to assume that even if we find a matrix that makes the values as close to zero as possible, they will never all be 0 for any practical case. Going back to our initial task (English to Italian), we will end up with a vector that looks something like this</p>
$<br>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtable><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">Dios</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">mio</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">mangiato</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">una</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">and so on...</mtext></mtd></mtr></mtable><mspace width="1.0em"></mspace><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.00005</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.25</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">...</mtext></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{aligned}
&amp;\text{Dios}\\
&amp;\text{mio}\\
&amp;\text{mangiato}\\
&amp;\text{una}\\
&amp;\text{and so on...}
\end{aligned}
\quad
\begin{bmatrix}
0.1 \\ 0.00005 \\ 0.25 \\ 0 \\ \text{...}
\end{bmatrix}</annotation></semantics></math>
<p><br>
$</p>
<p>How do we select the next word? We can certainly pick the one with the largest value, but this is not so good. Fortunately, we have <em>already</em> looked at a way to emphasize the right word - softmaxing! Softmaxing and picking the highest probability allows us to enhance the probability of the right word (and it will be high because we train the neural network in this way - remember that matrix multiplications are just two-layer neural networks) being picked. An added bonus is that the softmax function is differentiable.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">#define words</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>words<span class="op">=</span>[<span class="st">'My'</span>,<span class="st">'rabbit'</span>,<span class="st">'likes'</span>,<span class="st">'bananas'</span>]  <span class="co">#ensure vocabulary is 4 words long</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tokenized sentence:"</span>, words)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">#ensure that the words are sufficiently apart for better visibility-say 30 degrees</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>angles<span class="op">=</span>np.sort(np.random.choice(np.linspace(<span class="dv">0</span>,<span class="dv">2</span><span class="op">*</span>np.pi,<span class="dv">360</span>,endpoint<span class="op">=</span><span class="va">False</span>),<span class="bu">len</span>(words),replace<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">#compute unit circle coordinates (2D embeddings)</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>embedding_dim<span class="op">=</span><span class="dv">2</span>  <span class="co">#set embedding dimension to 2</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>unit_circle_vectors<span class="op">=</span>np.array([[np.cos(angle),np.sin(angle)] <span class="cf">for</span> angle <span class="kw">in</span> angles])</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co">#define initial one-hot vectors</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>one_hot_vectors<span class="op">=</span>np.eye(<span class="bu">len</span>(words),dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co">#function for positional encoding as defined in "Attention is all you need"</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> positional_encoding(seq_len,d_model):</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">#initialize positional encoding matrix</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    pos_enc<span class="op">=</span>np.zeros((seq_len,d_model))</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> pos <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,d_model,<span class="dv">2</span>):</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>            pos_enc[pos,i]<span class="op">=</span>np.sin(pos<span class="op">/</span>(<span class="dv">10000</span><span class="op">**</span>(<span class="dv">2</span><span class="op">*</span>i<span class="op">/</span>d_model)))</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i<span class="op">+</span><span class="dv">1</span><span class="op">&lt;</span>d_model:  <span class="co">#check to prevent index out of range</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>                pos_enc[pos,i<span class="op">+</span><span class="dv">1</span>]<span class="op">=</span>np.cos(pos<span class="op">/</span>(<span class="dv">10000</span><span class="op">**</span>(<span class="dv">2</span><span class="op">*</span>i<span class="op">/</span>d_model)))</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pos_enc</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a><span class="co">#calculate positional encodings for the sentence</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>seq_len<span class="op">=</span><span class="bu">len</span>(words)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>positional_encodings<span class="op">=</span>positional_encoding(seq_len,embedding_dim)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a><span class="co">#calculate the sum of position encoding and embedding vectors</span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>combined_vectors<span class="op">=</span>unit_circle_vectors<span class="op">+</span>positional_encodings</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="co">#decoder matrix to map combined vectors back to one-hot-like representations</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>decoder_matrix<span class="op">=</span>np.linalg.pinv(unit_circle_vectors)  <span class="co">#pseudo-inverse to decode</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="co">#decode the combined vectors</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>decoded_vectors<span class="op">=</span>np.dot(combined_vectors,decoder_matrix)</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a><span class="co">#map decoded vectors to words by finding the closest match</span></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> decode_to_words(decoded_vectors,word_embeddings,word_list):</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>    result<span class="op">=</span>[]</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> vec <span class="kw">in</span> decoded_vectors:</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>        <span class="co">#project vec back into the original embedding space</span></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>        reconstructed_vec<span class="op">=</span>np.dot(vec,word_embeddings)</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>        <span class="co">#compute distances and find the closest match</span></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>        distances<span class="op">=</span>np.linalg.norm(word_embeddings<span class="op">-</span>reconstructed_vec,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>        closest_word_index<span class="op">=</span>np.argmin(distances)</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>        result.append(word_list[closest_word_index])</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>decoded_words<span class="op">=</span>decode_to_words(decoded_vectors,unit_circle_vectors,words)</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a><span class="co">#print initial one-hot vectors</span></span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Initial One-Hot Vectors:"</span>)</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>one_hot_vectors[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a><span class="co">#print embedded vectors on unit circle</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Embedded Vectors on Unit Circle:"</span>)</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>unit_circle_vectors[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a><span class="co">#print positional encodings</span></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Positional Encodings:"</span>)</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>positional_encodings[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a><span class="co">#print combined vectors</span></span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Combined Vectors (Embedding+Positional Encoding):"</span>)</span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>combined_vectors[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a><span class="co">#print decoded words</span></span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Decoded tokenized sentence from Combined Vectors:"</span>)</span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(decoded_words)</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>Tokenized sentence: ['My', 'rabbit', 'likes', 'bananas']
Initial One-Hot Vectors:
My:[1 0 0 0]
rabbit:[0 1 0 0]
likes:[0 0 1 0]
bananas:[0 0 0 1]

Embedded Vectors on Unit Circle:
My:[0.1391731  0.99026807]
rabbit:[-0.5591929  -0.82903757]
likes:[-0.34202014 -0.93969262]
bananas:[ 0.89100652 -0.4539905 ]

Positional Encodings:
My:[0. 1.]
rabbit:[0.84147098 0.54030231]
likes:[ 0.90929743 -0.41614684]
bananas:[ 0.14112001 -0.9899925 ]

Combined Vectors (Embedding+Positional Encoding):
My:[0.1391731  1.99026807]
rabbit:[ 0.28227808 -0.28873527]
likes:[ 0.56727728 -1.35583946]
bananas:[ 1.03212653 -1.443983  ]

Decoded tokenized sentence from Combined Vectors:
['My', 'bananas', 'bananas', 'bananas']</code></pre>
</section>
</section>
<section id="attention" class="level3">
<h3 class="anchored" data-anchor-id="attention">Attention</h3>
<p>We have technically made an encoder-decoder model at this point. As you can see in the code above, even without attention, it is somewhat difficult to get consistent sequence reconstruction. Let’s think about attention again. So far, we have simply made an attention matrix and used it to suppress unimportant parts of the code. Our attention matrix was created by stacking one-hot encoded vectors on top of each other. This doesn’t make much sense. Some parts of a sentence may be important - maybe not as important as other parts - but important nonetheless. Multiplication with an attention matrix should result in a vector with parts that are suppressed but not necessarily zero. We can give importance to different parts by making our attention matrix elements fractions instead of ones and zeros.</p>
<p>We have so far not defined precisely what attention does. We have so far said that given a matrix of attention masks, we can pull out specific rows given our one-hot encoded non-future-dependent vectors, and suppress things even more. Now we focus on the big question. How do we generate this attention matrix? We’re in a completely different domain now - our words are no longer one-hot encoded, they’re embeddings, we’re adding positional information to them, and now we want to suppress irrelevant information.</p>
<p>Let’s get an intuitive explanation of attention first. We can naturally ask whether the attention in machine learning is the same as attention in human beings. What does this mean? For example, our brain is flooded with many different sensory inputs every second. There’s the internal sensory information from the body (such as level of hunger, blood pressure, pain, our balance), and there’s external sensory information from the environment (such as me hearing the distant hum of cars outside the window while typing this, but choosing to ignore it). How do we not get overwhelmed? We ‘tune out’ (suppress) irrelevant information and only focus on the one that matters. Only a small subset of the sensory input data is considered <em>relevant</em> enough to be perceived - this is what we mean when we say we are paying attention to something.</p>
<p>Simply put, we are assigning importance to items by filtering out the irrelevant ones. We also have a finite amount of attention. For example, watching a group of ants move around is significantly easier than tracking the path of more than a few ants in that group. You can either have a general idea of how the group is moving, or a specific idea of how some finite amount of ants in that group are moving, <strong>but not both</strong>.</p>
<p>We can specify this mathematically: Given a set of input items <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>,</mo><msub><mi>i</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><msub><mi>i</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">i_1,i_2,...i_n</annotation></semantics></math>, we assign nonzero weights to them, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><msub><mi>w</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>w</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">w_1,w_2,...,w_n</annotation></semantics></math> such that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></msubsup><msub><mi>w</mi><mi>k</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sum_{k=0}^{n}w_k=1</annotation></semantics></math>. Interpreting the weights as importances, they satisfy the two properties of human attention. We can than make a judgment about the items based on this attention. Since our inputs to the attention matrix are a collection of items (embedded word vectors), we can then assign a weight to them.</p>
<p>Where do these weights come from? This is where the learning in machine learning happens. We want to <em>learn</em> a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> to <em>compute</em> these weights. This function is typically one that <strong>first computes</strong> some ‘relevance’ score for each word in the sequence <strong>and then</strong> softmaxes the weights in order to make the weights nonzero. We have already computed this relevance score. This is simply the sum of the embedding vector and positional encoding vector. What do we do now?</p>
<p>So far we have talked about second-order models. Throughout the tutorial, we have constructed our one-hot encoded vectors assuming second-order relationships in the past. We briefly said that we could extend the word-pair vector construction to word triplets, but as the discussion so far should show, this is very impractical. We now try to answer the question: <strong>How important is every word to every other word in the sentence, and it is possible to calculate this in one go?</strong> Positional encodings don’t really answer this question, as they show how ‘relevant’ each word is in the overall sentence, but not in relation to the actual other words. We have to figure out three main things:</p>
<ol type="1">
<li>How do you tell a word to ‘ask’ other words in the sentence the following question: ‘How important are you to me?’?</li>
<li>If this question is asked for every possible word pair, how do you calculate the response of every other word?</li>
<li>If you are able to answer questions 1 and 2, how do you actually construct the vector that will be fed into the neural network?</li>
</ol>
<p>Let explicitly define what this means. Each word has some sort of <strong>intrinsic value</strong>, which we have said is the sum of its embedding and positional encoding. For each word to ask each other word <em>how important they are to it</em>, we have to compare its intrinsic value to the other words’ values. But <em>what</em> values? In the compound sentence ‘I went out to buy fruit and my sister answered some emails.’, ‘fruit’ and ‘emails’ are the objects of the subjects ‘I’ and ‘my sister’ in the two coordinate clauses. Even though they are very important within their own clauses, they are more or less irrelevant for the other clause. If there was an earlier relation such as ‘my sister likes fruit’ much earlier (say 1000 words behind ‘buy fruit’) in the corpus, it would make sense to compare the intrinsic values of the embeddings. But since we practically do not want to look 1000 words behind for all the reasons outlined above, it makes sense to assign each word a <strong>response value</strong> i.e.&nbsp;if asked a question by another word, the word being asked will return a response value <em>that may be different from its intrinsic value</em>. Based on what the response value is, the initial word will decide what <strong>information value</strong> it ‘passes on’ to the neural network so that it can reconstruct this efficiently. Also, it does not make much sense for ‘I’ and ‘emails’ to be compared, so you also have to <strong>figure out how much ‘I’ will pay attention to the other words</strong>. This triplet of <em>intrinsic value</em>, <em>response value</em>, and <em>information value</em> are what defines the attention process.</p>
<p>How do we make a word get a vector response from every other word? Like we have been doing all along, we define an <strong>attention matrix</strong> that is <em>not</em> one-hot encoded this time around. The attention matrix <em>prepares each word</em> for asking questions. Multiplying a word vector by this attention matrix results in a vector that contains the information ‘how much attention should this word pay to other words?’. To make this process fast, you can <em>stack</em> each intrinsic value into a matrix and multiply it with the attention matrix. This results in a matrix (let’s call this matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>) that contains information about ‘how much should every word pay attention to every other word’? Note that this is not pair-specific; we are <em>not</em> saying that ‘I’ should pay lesser attention to ‘emails’ than to ‘went’. This is the behavior that is <em>learned</em> by the transformer.</p>
<p>Next, we multiply <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> with a matrix that contains a matrix of responses of the other words. Like before, we are not explicitly telling ‘I’ to provide a worse response if asked ‘How important are you to me?’ by ‘emails’, but we are <em>learning</em> this behavior. Let’s call the result of this matrix multiplication <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>. You can now <em>normalize</em> the weights and <em>softmax</em> them in order for the matrix to have stable values. We then finally multiply this matrix by the matrix of information values to get <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> <strong>the result of the attention step</strong>. <em>This</em> is the idea behind attention. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> contains information about <em>how much weight</em> each word should give to every other word, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> contains information that makes our two-layer neural network make a judgment. These matrices are <em>learned</em> by the transformer by backpropagation.</p>
<section id="multi-head-attention" class="level4">
<h4 class="anchored" data-anchor-id="multi-head-attention">Multi-head attention</h4>
<p>Since we are now no longer dealing with nonnegative zero-one matrices, the question of how to do this calculation efficiently for large corpora still remains. Unfortunately we cannot. We can only rely on specialized and accelerated hardware. We are also still not sure whether the attention process actually captures meanings in a way that is human-understandable. The solution is straightforward: instead of having <em>one</em> set of matrices to perform attention, have <em>more than one set</em>. Apply each matrix set’s attention independently, and for every newly returned information vector, combine them together in a specified way (usually, just concatenate, then use a neural network to predict outputs). The hope is that each set of matrices will learn something different about the text - sentence structure, word meanings, subject-object relationships. Each such set of matrices is called an <strong>attention head</strong>. To make this process somewhat computationally palatable, the matrices in multihead attention have smaller output dimensions (usually what the size would be for a single head for the whole corpus divided by the number of heads). <em>In practice</em>, this does work.</p>
<p>The final thing is dealing with practical languages. We still have to <strong>stack</strong> these attention blocks and two-layer neural networks many times in order to actually encode and decode things. This is the main reason it takes so long to train transformers on real text items. There is still of course the problem of getting correct datasets, but this is fine.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>tokenized_sentence<span class="op">=</span>[<span class="st">'My'</span>,<span class="st">'rabbit'</span>,<span class="st">'likes'</span>,<span class="st">'bananas'</span>]</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">#combined_vectors is a np.ndarray of shape (4,2) that has been initialized in a different cell</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>intrinsic_value_matrix<span class="op">=</span>combined_vectors.T</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The intrinsic value matrix is:</span><span class="ch">\n</span><span class="st">"</span>, intrinsic_value_matrix)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>l<span class="op">=</span><span class="bu">len</span>(tokenized_sentence) <span class="co">#this is completely arbitrary - i want a machine learning model whose final dimension is 4</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>attention_matrix<span class="op">=</span>np.random.rand(l,l)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The initial attention matrix is:</span><span class="ch">\n</span><span class="st">"</span>, attention_matrix)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>A<span class="op">=</span>np.matmul(intrinsic_value_matrix, attention_matrix)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Preparing the words for attention, we get:</span><span class="ch">\n</span><span class="st">"</span>, A)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>response_matrix<span class="op">=</span>np.random.rand(l,l)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The initial response matrix is:</span><span class="ch">\n</span><span class="st">"</span>, response_matrix)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span>np.matmul(A,response_matrix.T)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The responses given by each word to each other word is:</span><span class="ch">\n</span><span class="st">"</span>, B, <span class="st">"</span><span class="ch">\n</span><span class="st"> and after normalizing, we get:</span><span class="ch">\n</span><span class="st">"</span>, B<span class="op">/</span>np.sqrt(l))</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span>B<span class="op">/</span>np.sqrt(l) <span class="co">#explicitly rewrite the matrix</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"We will softmax the attention matrix in order to boost closer words. The result of doing this is:</span><span class="ch">\n</span><span class="st">"</span>, np.exp(B)<span class="op">/</span>np.<span class="bu">sum</span>(np.exp(B),axis<span class="op">=</span><span class="dv">1</span>,keepdims<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span>np.exp(B)<span class="op">/</span>np.<span class="bu">sum</span>(np.exp(B),axis<span class="op">=</span><span class="dv">1</span>,keepdims<span class="op">=</span><span class="va">True</span>) <span class="co">#store it again</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>information_matrix<span class="op">=</span>np.random.rand(l,l)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The information matrix is:</span><span class="ch">\n</span><span class="st">"</span>, information_matrix)</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>C<span class="op">=</span>np.matmul(B, information_matrix)</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The actual information passed on to the two-layer neural network for decoding is:</span><span class="ch">\n</span><span class="st">"</span>, C)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>The intrinsic value matrix is:
 [[ 0.1391731   0.28227808  0.56727728  1.03212653]
 [ 1.99026807 -0.28873527 -1.35583946 -1.443983  ]]
The initial attention matrix is:
 [[0.96775588 0.85687394 0.11764399 0.73973033]
 [0.31966683 0.98841599 0.48535898 0.1224339 ]
 [0.72518555 0.3230112  0.99707816 0.97978246]
 [0.65613402 0.12982645 0.686861   0.09303049]]
Preparing the words for attention, we get:
 [[ 1.31351514  0.71549621  1.42792633  0.78933853]
 [-0.09688701  0.79459977 -2.24969069 -0.02585148]]
The initial response matrix is:
 [[0.02936416 0.82295845 0.20975295 0.37902713]
 [0.97345739 0.3368405  0.29648978 0.918208  ]
 [0.11074845 0.75958241 0.98806557 0.90129601]
 [0.65166792 0.31406079 0.25432198 0.8192551 ]]
The responses given by each word to each other word is:
 [[ 1.22608639  2.66780165  2.81126061  2.09050766]
 [ 0.16939992 -0.51740933 -1.65330782 -0.40691028]] 
 and after normalizing, we get:
 [[ 0.61304319  1.33390082  1.4056303   1.04525383]
 [ 0.08469996 -0.25870467 -0.82665391 -0.20345514]]
We will softmax the attention matrix in order to boost closer words. The result of doing this is:
 [[0.14693005 0.30211697 0.32458379 0.22636919]
 [0.34953106 0.24794025 0.14050437 0.26202432]]
The information matrix is:
 [[0.87982191 0.52543491 0.24433994 0.11294321]
 [0.08717947 0.71907745 0.42011873 0.90535987]
 [0.7396713  0.19126042 0.61377742 0.43550545]
 [0.57724973 0.73806912 0.80619588 0.84373733]]
The actual information passed on to the two-layer neural network for decoding is:
 [[0.52636754 0.52360382 0.54454599 0.62247348]
 [0.5843209  0.58220905 0.48705008 0.54622243]]</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"This is a demonstration of multihead attention.</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Let's have two attention heads. The idea is to make our single-head attention model smaller."</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>intrinsic_value_matrix<span class="op">=</span>combined_vectors</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>l2<span class="op">=</span><span class="bu">int</span>(l<span class="op">/</span><span class="dv">2</span>) <span class="co">#two heads</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>matrix_sets<span class="op">=</span>[]</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>l3<span class="op">=</span><span class="bu">int</span>(l2<span class="op">/</span><span class="dv">2</span>) <span class="co">#two heads, so the underlying projection dimesion will be smaller</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(l2):</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    matrix_sets.append([np.random.rand(l2,l3),np.random.rand(l2,l3),np.random.rand(l2,l3)])</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The multihead attention matrices are now:"</span>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([i <span class="cf">for</span> i <span class="kw">in</span> matrix_sets])</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>passed_on<span class="op">=</span>[]</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The attention process is now applied to the intrinsic value matrix for each head."</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(l2):</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    A<span class="op">=</span>np.matmul(intrinsic_value_matrix,matrix_sets[i][<span class="dv">0</span>])</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    B<span class="op">=</span>np.matmul(A,matrix_sets[i][<span class="dv">1</span>].T)<span class="op">/</span>np.sqrt(l2)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    B<span class="op">=</span>np.exp(B)<span class="op">/</span>np.<span class="bu">sum</span>(np.exp(B),axis<span class="op">=</span><span class="dv">1</span>,keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    C<span class="op">=</span>np.matmul(B,matrix_sets[i][<span class="dv">2</span>])</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"For head"</span>,i<span class="op">+</span><span class="dv">1</span>,<span class="st">"the information passed on is:</span><span class="ch">\n</span><span class="st">"</span>, C)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    passed_on.append(C)</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Therefore, the total information passed on is:</span><span class="ch">\n</span><span class="st">"</span>,np.hstack((passed_on[<span class="dv">0</span>],passed_on[<span class="dv">1</span>])))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>This is a demonstration of multihead attention.

Let's have two attention heads. The idea is to make our single-head attention model smaller.
The multihead attention matrices are now:
[[array([[0.25397487],
       [0.0311156 ]]), array([[0.73929474],
       [0.10634373]]), array([[0.31491959],
       [0.00381713]])], [array([[0.68166948],
       [0.16713201]]), array([[0.65528241],
       [0.99174262]]), array([[0.07267087],
       [0.07707801]])]]
The attention process is now applied to the intrinsic value matrix for each head.
For head 1 the information passed on is:
 [[0.16275392]
 [0.16155103]
 [0.16291437]
 [0.16692318]]
For head 2 the information passed on is:
 [[0.07498641]
 [0.07491223]
 [0.0749164 ]
 [0.07499548]]
Therefore, the total information passed on is:
 [[0.16275392 0.07498641]
 [0.16155103 0.07491223]
 [0.16291437 0.0749164 ]
 [0.16692318 0.07499548]]</code></pre>
</section>
<section id="masked-attention" class="level4">
<h4 class="anchored" data-anchor-id="masked-attention">Masked attention</h4>
<p>We have not exactly implemented future-proofing here. Masked attention is simply a term that makes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> an upper triangular matrix, as we can easily now see why it is a future proofed matrix.</p>
</section>
<section id="skip-connections-layer-normalization-and-xavier-initializations" class="level4">
<h4 class="anchored" data-anchor-id="skip-connections-layer-normalization-and-xavier-initializations">Skip connections, Layer Normalization, and Xavier Initializations</h4>
<p>Sometimes the result of the total information being passed on is small, so you can always add a <strong>skip connection</strong> to make it so that your vector is a result of embedding+position encoding AND attention. This is all empirical stuff that ‘just works’. Skip connections also serve the dual purpose of being good for gradient descent. One thing we have not spoken about is gradient descent in practice. It is easy to see that gradient descent may be bad in practice while sound in theory, when the error function’s plot in parameter space is just too hilly. Skip connections, things like <em>layer normalization</em>, and <em>Xavier initialization</em> for all parameters make ‘the gradients flow smoother’ in backpropagation. In practice, this means that the gradient calculation actually updates the values fairly frequently, instead of the gradient being calculated as 0 (because of a loss of numerical precision in computers - they have finite precision!).</p>
<p>For backpropagation through layer normalization, this is the mathematics: $$ x_{}=f_{} \[5pt] f_{}=\[5pt]</p>
<p>= _{i=1}^N x_i \[5pt]</p>
<p>^2 = _{i=1}^N (x_i - )^2 \[5pt]</p>
<p>\[5pt]</p>
<p> d_{out}=\[5pt]</p>
<p> = <em>{i=1}^N d</em>{out_{i}} (x_i - ) (- (^2 + )^{-3/2})\[5pt]</p>
<p> = <em>{i=1}^N d</em>{out_{i}} (-) + _{i=1}^N (-2 (x_i - ) / N )\[5pt]</p>
<p> = + + $$</p>
</section>
<section id="cross-attention" class="level4">
<h4 class="anchored" data-anchor-id="cross-attention">Cross-attention</h4>
<p>So far we have described attention as a standalone procedure. The idea is now to encode text with attention (for example, our neural network converts to a lower dimensional space) and pass the result of that into a decoder matrix. What does that mean? It means that we have converted text into an arbitrary dimension space <em>after</em> performing all of these procedures. But wait. These can simply be interpreted as the results of a separate instance of attention results! While <em>generating</em> text, we can simply pull those results in and feed them into the decoder’s decoding stage, providing the attention we need.</p>
</section>
<section id="text-generation" class="level4">
<h4 class="anchored" data-anchor-id="text-generation">Text generation</h4>
<p>Now we do our actual task. How do we actually choose the next word? The idea is to predict the next word and generate a probability distribution with softmaxing, and greedily picking the one with the highest probability. Then, <em>take the text that has been generated so far along with the output</em> and use it as an <em>input</em> to the decoder, apply attention, then use multihead to combine things with the encoded transformer. Transformers generate text one word at a time. The way you put the text in as input is by inserting a special word that translates to ‘SEQUENCE STARTS HERE’. This allows the decoder to train and learn to predict the next word.</p>
</section>
</section>
<section id="a-transformer-from-scratch-in-pure-numpy" class="level3">
<h3 class="anchored" data-anchor-id="a-transformer-from-scratch-in-pure-numpy">A transformer from scratch in pure numpy</h3>
<p>After all of this, we can finally implement a transformer in numpy. Note that when you backpropagate errors through the model, the updates are applied to all matrices as single neurons as well. Let us lay out what the overall task is:</p>
<ol type="1">
<li>Give an input sentence, tokenize it, embed the words, add positional encoding. Keep a copy of the output.</li>
<li>Initialize <em>random</em> attention, response, and value matrices, and run the matrix of inputs through the multi-head attention step.</li>
<li>Add the output copy and attention step results, and use that as input to a 2 layer neural network (whose neurons are randomly initialized) to convert the data into some weird representation that only the machine understands.</li>
<li>Put this weird or <em>latent</em> representation into another neural network to decode it, then project it to the target space vocabulary size, then softmax. Select the word with the largest probability as the output and compare which word in the other vocabulary’s embeddings is the closest. Once the next word is found, use that as the input to the model (that is, put it through its own set of attention steps, but this time they’re masked) and use that to predict the next token. Once the entire sequence has been predicted, compute the cross-entropy loss and apply backpropagation, updating each matrix.</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Tuple, Optional, Dict</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Utilities</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> whitespace_tokenizer(sentence: <span class="bu">str</span>) <span class="op">-&gt;</span> List[<span class="bu">str</span>]:</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co">        Splits a latin-character sentence into individual words assuming that each word is separated by a whitespace, and there's no punctuation</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co">        Input:</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co">            sentence: a string </span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co">        Output:</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co">            List of strings containing each individual words</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">#we now start using python's inbuilt functions in order to make the code look more impressive</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sentence.strip().split()</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_vocab(list_of_tokenized_sentences: List[List[<span class="bu">str</span>]]) <span class="op">-&gt;</span> Tuple[<span class="bu">dict</span>, <span class="bu">dict</span>]:</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="co">        Builds a vocabulary of all words in the a given corpus of languages.</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a><span class="co">        This means that if you have more than one sentence, it creates a list of words.</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a><span class="co">        Input:</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a><span class="co">            list_of_tokenized_sentences: a list of list of strings, AKA a whitespace tokenized sentence</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a><span class="co">        Output:</span></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a><span class="co">            A tuple of Python dictionaries, containing all words and their index. This index is random, because of the use of set()</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">#the idea is to store the vocabulary in a Python set() object, which randomly stores elements for faster access (there's more to it but oh well)</span></span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>    vocabulary<span class="op">=</span><span class="bu">set</span>()</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tokenized_sentence <span class="kw">in</span> list_of_tokenized_sentences: </span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        vocabulary.update(tokenized_sentence) <span class="co">#when you update a python set with a list, the elements of the list are added to the vocabulary</span></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>    vocabulary<span class="op">=</span><span class="bu">list</span>(vocabulary) <span class="co">#convert the vocabulary set() into a list, so you can enumerate through it</span></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>    word_index_pairs<span class="op">=</span>{w:i <span class="cf">for</span> i,w <span class="kw">in</span> <span class="bu">enumerate</span>(vocabulary)} <span class="co">#create a word:index pairing dictionary</span></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>    index_word_pairs<span class="op">=</span>{i:w <span class="cf">for</span> i,w <span class="kw">in</span> <span class="bu">enumerate</span>(vocabulary)} <span class="co">#create an index:word pairing dictionary</span></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> word_index_pairs,index_word_pairs</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pad_sequences(sequences: List[List[<span class="bu">int</span>]], max_length: Optional[<span class="bu">int</span>]<span class="op">=</span><span class="va">None</span>, pad_value: <span class="bu">int</span><span class="op">=</span><span class="dv">0</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a><span class="co">        Pads sequences. This is done because it is easier to process sequences with the same length, so we artifically add numbers to smaller ones.</span></span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a><span class="co">            sequences: a list of list of integers</span></span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a><span class="co">            max_length: Optional. Allows you to set the maximum length for a sequence instead of computing it dynamically.</span></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a><span class="co">                     For example, in a corpus of sentences each under 20 words long, you can set the max length as 294</span></span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a><span class="co">            pad_value: What number you want to use to </span></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a><span class="co">        Output:</span></span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a><span class="co">            A numpy.ndarray of a padded sequence</span></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>    <span class="co">#dynamically figure out the max length, as you have to pad to this</span></span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> max_length <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span><span class="bu">max</span>(<span class="bu">len</span>(sequence) <span class="cf">for</span> sequence <span class="kw">in</span> sequences)</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>    padded<span class="op">=</span>[]</span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sequence <span class="kw">in</span> sequences:</span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>        padded_sequence<span class="op">=</span>sequence<span class="op">+</span>[pad_value]<span class="op">*</span>(max_length<span class="op">-</span><span class="bu">len</span>(sequence)) <span class="co">#simply append a list containing pad_value</span></span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>        padded.append(padded_sequence)</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(padded, dtype<span class="op">=</span>np.int32) <span class="co">#return an ndarray</span></span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_mask_for_removing_future_dependency(sequence: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a><span class="co">        Creates a non-future-dependent mask for the decoder, so that the decoder doesn't use the future for generation. This is also called autoregressive behavior.</span></span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a><span class="co">            sequence: An input ndarray whose shape is (batch_size, batch_size) - note that this assumes you are feeding it into the decoder at scale</span></span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a><span class="co">            An ndarray of the same shape as the input sequence which is upper triangular. The data type is Bool for faster processing. The upper triangular part has 'True'.</span></span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a>    sequence_length<span class="op">=</span>sequence.shape[<span class="dv">1</span>]</span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a>    autoregressive_mask<span class="op">=</span>np.triu(np.ones((sequence_length,sequence_length)),k<span class="op">=</span><span class="dv">1</span>).astype(<span class="bu">bool</span>) <span class="co">#again, using the inbuilt functions</span></span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> autoregressive_mask</span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_hot(indices: np.ndarray, vocabulary_size: <span class="bu">int</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb19-73"><a href="#cb19-73" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-74"><a href="#cb19-74" aria-hidden="true" tabindex="-1"></a><span class="co">        Converts input sequences into one-hot encoding in one go. </span></span>
<span id="cb19-75"><a href="#cb19-75" aria-hidden="true" tabindex="-1"></a><span class="co">        Since we are feeding sequences in one go to the encoder in batches, we need a 3D ndarray. </span></span>
<span id="cb19-76"><a href="#cb19-76" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-77"><a href="#cb19-77" aria-hidden="true" tabindex="-1"></a><span class="co">            indices: An input ndarray whose shape is (batch_size, sequence_length), where each element has an integer index. This is why we created indices above</span></span>
<span id="cb19-78"><a href="#cb19-78" aria-hidden="true" tabindex="-1"></a><span class="co">            vocabulary_size: The number of words in the vocabulary of the language</span></span>
<span id="cb19-79"><a href="#cb19-79" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-80"><a href="#cb19-80" aria-hidden="true" tabindex="-1"></a>    batched_tokens<span class="op">=</span>np.zeros((indices.shape[<span class="dv">0</span>], indices.shape[<span class="dv">1</span>], vocabulary_size)) <span class="co">#create your zero ndarray, and populate it</span></span>
<span id="cb19-81"><a href="#cb19-81" aria-hidden="true" tabindex="-1"></a>    <span class="co">#use a nested loop to set one-hot encodings</span></span>
<span id="cb19-82"><a href="#cb19-82" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_index <span class="kw">in</span> <span class="bu">range</span>(indices.shape[<span class="dv">0</span>]):</span>
<span id="cb19-83"><a href="#cb19-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> token_index <span class="kw">in</span> <span class="bu">range</span>(indices.shape[<span class="dv">1</span>]):</span>
<span id="cb19-84"><a href="#cb19-84" aria-hidden="true" tabindex="-1"></a>            batched_tokens[b,t,indices[batch_index,token_index]]<span class="op">=</span><span class="dv">1</span></span>
<span id="cb19-85"><a href="#cb19-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> batched_tokens</span>
<span id="cb19-86"><a href="#cb19-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-87"><a href="#cb19-87" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb19-88"><a href="#cb19-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalization, activation, and loss</span></span>
<span id="cb19-89"><a href="#cb19-89" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb19-90"><a href="#cb19-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-91"><a href="#cb19-91" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> layer_norm(x: np.ndarray, eps<span class="op">=</span><span class="fl">1e-6</span>) <span class="op">-&gt;</span> Tuple[np.ndarray,np.ndarray,np.ndarray]:</span>
<span id="cb19-92"><a href="#cb19-92" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-93"><a href="#cb19-93" aria-hidden="true" tabindex="-1"></a><span class="co">        Normalizes the input ndarray. This is done to make backpropagation not run into numerical errors.</span></span>
<span id="cb19-94"><a href="#cb19-94" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-95"><a href="#cb19-95" aria-hidden="true" tabindex="-1"></a><span class="co">            x: ndarray of inputs. usually just a vector</span></span>
<span id="cb19-96"><a href="#cb19-96" aria-hidden="true" tabindex="-1"></a><span class="co">            eps: this is done to prevent division by zero during normalization</span></span>
<span id="cb19-97"><a href="#cb19-97" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-98"><a href="#cb19-98" aria-hidden="true" tabindex="-1"></a><span class="co">            Tuple of ndarrays containing the normalized input ndarray, the mean, and the variance</span></span>
<span id="cb19-99"><a href="#cb19-99" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-100"><a href="#cb19-100" aria-hidden="true" tabindex="-1"></a>    mean<span class="op">=</span>np.mean(x, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-101"><a href="#cb19-101" aria-hidden="true" tabindex="-1"></a>    var<span class="op">=</span>np.var(x, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-102"><a href="#cb19-102" aria-hidden="true" tabindex="-1"></a>    x_norm<span class="op">=</span>(x<span class="op">-</span>mean)<span class="op">/</span>np.sqrt(var<span class="op">+</span>eps)</span>
<span id="cb19-103"><a href="#cb19-103" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_norm,mean,var</span>
<span id="cb19-104"><a href="#cb19-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-105"><a href="#cb19-105" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> layer_norm_backprop(dout: np.ndarray, x: np.ndarray, mean: np.ndarray, var: np.ndarray, eps: <span class="bu">float</span><span class="op">=</span><span class="fl">1e-6</span>) <span class="op">-&gt;</span>np.ndarray:</span>
<span id="cb19-106"><a href="#cb19-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-107"><a href="#cb19-107" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-108"><a href="#cb19-108" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the gradient of the loss with respect to the input x of a layer normalization operation, implementing backpropagation as defined in the tutorial</span></span>
<span id="cb19-109"><a href="#cb19-109" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs: </span></span>
<span id="cb19-110"><a href="#cb19-110" aria-hidden="true" tabindex="-1"></a><span class="co">            dout: an ndarray containing the gradient with respect to the normalized input</span></span>
<span id="cb19-111"><a href="#cb19-111" aria-hidden="true" tabindex="-1"></a><span class="co">            x: an ndarray containing the original input</span></span>
<span id="cb19-112"><a href="#cb19-112" aria-hidden="true" tabindex="-1"></a><span class="co">            mean: mean of the input ndarray x along the last axis, as computed above</span></span>
<span id="cb19-113"><a href="#cb19-113" aria-hidden="true" tabindex="-1"></a><span class="co">            var: exactly like the mean</span></span>
<span id="cb19-114"><a href="#cb19-114" aria-hidden="true" tabindex="-1"></a><span class="co">            eps: an optional value for numerical stabillity</span></span>
<span id="cb19-115"><a href="#cb19-115" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-116"><a href="#cb19-116" aria-hidden="true" tabindex="-1"></a><span class="co">            an ndarray containing the gradient with respect to the layer normalization procedure</span></span>
<span id="cb19-117"><a href="#cb19-117" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-118"><a href="#cb19-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-119"><a href="#cb19-119" aria-hidden="true" tabindex="-1"></a>    <span class="co">#numerically backpropagate by calculating the derivatives. unfortunately, this just requires knowing the formula as shown above</span></span>
<span id="cb19-120"><a href="#cb19-120" aria-hidden="true" tabindex="-1"></a>    N<span class="op">=</span>x.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb19-121"><a href="#cb19-121" aria-hidden="true" tabindex="-1"></a>    dx_norm<span class="op">=</span>dout<span class="op">/</span>np.sqrt(var<span class="op">+</span>eps)</span>
<span id="cb19-122"><a href="#cb19-122" aria-hidden="true" tabindex="-1"></a>    dvar<span class="op">=</span>np.<span class="bu">sum</span>(dout<span class="op">*</span>(x<span class="op">-</span>mean)<span class="op">*-</span><span class="fl">0.5</span><span class="op">*</span>(var<span class="op">+</span>eps)<span class="op">**</span>(<span class="op">-</span><span class="fl">1.5</span>), axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-123"><a href="#cb19-123" aria-hidden="true" tabindex="-1"></a>    dmean<span class="op">=</span>(np.<span class="bu">sum</span>(dout<span class="op">*-</span><span class="dv">1</span><span class="op">/</span>np.sqrt(var<span class="op">+</span>eps), axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)<span class="op">+</span>dvar<span class="op">*</span>np.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">2</span><span class="op">*</span>(x<span class="op">-</span>mean), axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)<span class="op">/</span>N)</span>
<span id="cb19-124"><a href="#cb19-124" aria-hidden="true" tabindex="-1"></a>    dx<span class="op">=</span>dx_norm<span class="op">+</span>dvar<span class="op">*</span><span class="dv">2</span><span class="op">*</span>(x<span class="op">-</span>mean)<span class="op">/</span>N<span class="op">+</span>dmean<span class="op">/</span>N</span>
<span id="cb19-125"><a href="#cb19-125" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dx</span>
<span id="cb19-126"><a href="#cb19-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-127"><a href="#cb19-127" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(x: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb19-128"><a href="#cb19-128" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-129"><a href="#cb19-129" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the softmax of an input ndarray and generate a probability distribution</span></span>
<span id="cb19-130"><a href="#cb19-130" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-131"><a href="#cb19-131" aria-hidden="true" tabindex="-1"></a><span class="co">            x: an ndarray that you want to softmax</span></span>
<span id="cb19-132"><a href="#cb19-132" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-133"><a href="#cb19-133" aria-hidden="true" tabindex="-1"></a><span class="co">            an ndarray containing the softmaxed version along the last axis</span></span>
<span id="cb19-134"><a href="#cb19-134" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-135"><a href="#cb19-135" aria-hidden="true" tabindex="-1"></a>    x_shifted<span class="op">=</span>x<span class="op">-</span>np.<span class="bu">max</span>(x, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>) <span class="co">#this is done to improve numerical stability. softmaxing is invariant to shifts by a constant value</span></span>
<span id="cb19-136"><a href="#cb19-136" aria-hidden="true" tabindex="-1"></a>    exp_x<span class="op">=</span>np.exp(x_shifted)</span>
<span id="cb19-137"><a href="#cb19-137" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp_x<span class="op">/</span>np.<span class="bu">sum</span>(exp_x, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-138"><a href="#cb19-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-139"><a href="#cb19-139" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_entropy_loss(predictions: np.ndarray, targets: np.ndarray) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb19-140"><a href="#cb19-140" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-141"><a href="#cb19-141" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the cross-entropy loss as defined above, measuring the difference between two probability predicted distributions</span></span>
<span id="cb19-142"><a href="#cb19-142" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-143"><a href="#cb19-143" aria-hidden="true" tabindex="-1"></a><span class="co">            predictions: an ndarray containing whatever you have predicted</span></span>
<span id="cb19-144"><a href="#cb19-144" aria-hidden="true" tabindex="-1"></a><span class="co">            targets: an ndarray containing whatever the real targets are</span></span>
<span id="cb19-145"><a href="#cb19-145" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-146"><a href="#cb19-146" aria-hidden="true" tabindex="-1"></a><span class="co">            a float of computed cross-entropy loss, averaged over all elements in the batch</span></span>
<span id="cb19-147"><a href="#cb19-147" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-148"><a href="#cb19-148" aria-hidden="true" tabindex="-1"></a>    epsilon<span class="op">=</span><span class="fl">1e-12</span> <span class="co">#more numerical precision constants</span></span>
<span id="cb19-149"><a href="#cb19-149" aria-hidden="true" tabindex="-1"></a>    predictions<span class="op">=</span>np.clip(predictions, epsilon, <span class="dv">1</span><span class="op">-</span>epsilon) <span class="co">#values smaller than epsilon become epsilon, values larger than 1-epsilon become 1-epsilon</span></span>
<span id="cb19-150"><a href="#cb19-150" aria-hidden="true" tabindex="-1"></a>    flat_targets<span class="op">=</span>targets.flatten() <span class="co">#maybe the arrays are not 1d, but the cross-entropy loss needs the 1d</span></span>
<span id="cb19-151"><a href="#cb19-151" aria-hidden="true" tabindex="-1"></a>    flat_preds<span class="op">=</span>predictions.reshape(<span class="op">-</span><span class="dv">1</span>, predictions.shape[<span class="op">-</span><span class="dv">1</span>]) <span class="co">#same reason here for flattening</span></span>
<span id="cb19-152"><a href="#cb19-152" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=-</span>np.mean(np.log(flat_preds[np.arange(flat_targets.shape[<span class="dv">0</span>]), flat_targets])) <span class="co">#compute the crossentropy loss</span></span>
<span id="cb19-153"><a href="#cb19-153" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb19-154"><a href="#cb19-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-155"><a href="#cb19-155" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_entropy_derivative(predictions: np.ndarray, targets: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb19-156"><a href="#cb19-156" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-157"><a href="#cb19-157" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the backpropagation for cross-entropy loss</span></span>
<span id="cb19-158"><a href="#cb19-158" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-159"><a href="#cb19-159" aria-hidden="true" tabindex="-1"></a><span class="co">            predictions: an ndarray containing whatever you have predicted</span></span>
<span id="cb19-160"><a href="#cb19-160" aria-hidden="true" tabindex="-1"></a><span class="co">            targets: an ndarray containing whatever the real targets are</span></span>
<span id="cb19-161"><a href="#cb19-161" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-162"><a href="#cb19-162" aria-hidden="true" tabindex="-1"></a><span class="co">            an ndarray of the gradient of the cross-entropy derivative required for backpropagation</span></span>
<span id="cb19-163"><a href="#cb19-163" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-164"><a href="#cb19-164" aria-hidden="true" tabindex="-1"></a>    <span class="co">#the gradient was also defined above so this is just a way to get batches of data in and publish it</span></span>
<span id="cb19-165"><a href="#cb19-165" aria-hidden="true" tabindex="-1"></a>    batch,length,vocab_size<span class="op">=</span>predictions.shape</span>
<span id="cb19-166"><a href="#cb19-166" aria-hidden="true" tabindex="-1"></a>    grad<span class="op">=</span>predictions.copy()</span>
<span id="cb19-167"><a href="#cb19-167" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch):</span>
<span id="cb19-168"><a href="#cb19-168" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(length):</span>
<span id="cb19-169"><a href="#cb19-169" aria-hidden="true" tabindex="-1"></a>            grad[b, t, targets[b,t]]<span class="op">-=</span><span class="dv">1</span></span>
<span id="cb19-170"><a href="#cb19-170" aria-hidden="true" tabindex="-1"></a>    grad<span class="op">/=</span>(batch<span class="op">*</span>length)</span>
<span id="cb19-171"><a href="#cb19-171" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> grad</span>
<span id="cb19-172"><a href="#cb19-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-173"><a href="#cb19-173" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb19-174"><a href="#cb19-174" aria-hidden="true" tabindex="-1"></a><span class="co"># Multi-Head Attention</span></span>
<span id="cb19-175"><a href="#cb19-175" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb19-176"><a href="#cb19-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-177"><a href="#cb19-177" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_heads(x: np.ndarray, num_heads: <span class="bu">int</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb19-178"><a href="#cb19-178" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-179"><a href="#cb19-179" aria-hidden="true" tabindex="-1"></a><span class="co">        Attention heads attend to different parts of the data, so this is a function to simply split the data </span></span>
<span id="cb19-180"><a href="#cb19-180" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-181"><a href="#cb19-181" aria-hidden="true" tabindex="-1"></a><span class="co">            x: an ndarray of input data</span></span>
<span id="cb19-182"><a href="#cb19-182" aria-hidden="true" tabindex="-1"></a><span class="co">            num_heads: the number of attention instances you want</span></span>
<span id="cb19-183"><a href="#cb19-183" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-184"><a href="#cb19-184" aria-hidden="true" tabindex="-1"></a><span class="co">            a reshaped x split into the number of heads</span></span>
<span id="cb19-185"><a href="#cb19-185" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-186"><a href="#cb19-186" aria-hidden="true" tabindex="-1"></a>    <span class="co">#at this point it should be familiar, split the data into batches and make it work</span></span>
<span id="cb19-187"><a href="#cb19-187" aria-hidden="true" tabindex="-1"></a>    batch,length,d_model<span class="op">=</span>x.shape</span>
<span id="cb19-188"><a href="#cb19-188" aria-hidden="true" tabindex="-1"></a>    head_dim<span class="op">=</span>d_model<span class="op">//</span>num_heads <span class="co">#the heads attend to different parts of the model</span></span>
<span id="cb19-189"><a href="#cb19-189" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x.reshape(batch, length, num_heads, head_dim).transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>) <span class="co">#reshape x so that every ndarray is for heads</span></span>
<span id="cb19-190"><a href="#cb19-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-191"><a href="#cb19-191" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> merge_heads(x: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb19-192"><a href="#cb19-192" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-193"><a href="#cb19-193" aria-hidden="true" tabindex="-1"></a><span class="co">        After the result of multihead attention, you need to recombine them, so this function does it</span></span>
<span id="cb19-194"><a href="#cb19-194" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-195"><a href="#cb19-195" aria-hidden="true" tabindex="-1"></a><span class="co">            x: an ndarray containing information of shape (batch, num_heads, length, head_dim)</span></span>
<span id="cb19-196"><a href="#cb19-196" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-197"><a href="#cb19-197" aria-hidden="true" tabindex="-1"></a><span class="co">            a combined x after (presumably) multihead attention has been done</span></span>
<span id="cb19-198"><a href="#cb19-198" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-199"><a href="#cb19-199" aria-hidden="true" tabindex="-1"></a>    batch,num_heads,length,head_dim<span class="op">=</span>x.shape</span>
<span id="cb19-200"><a href="#cb19-200" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x.transpose(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>).reshape(batch, length, num_heads<span class="op">*</span>head_dim)</span>
<span id="cb19-201"><a href="#cb19-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-202"><a href="#cb19-202" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> scaled_dot_product_attention(attention_matrix: np.ndarray, response_matrix: np.ndarray, information_matrix: np.ndarray, mask: Optional[np.ndarray]<span class="op">=</span><span class="va">None</span>) <span class="op">-&gt;</span> Tuple[np.ndarray, np.ndarray]:</span>
<span id="cb19-203"><a href="#cb19-203" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-204"><a href="#cb19-204" aria-hidden="true" tabindex="-1"></a><span class="co">        This is the official formula of the attention. As we have seen, we use an attention matrix to make each word ask each other word a question.</span></span>
<span id="cb19-205"><a href="#cb19-205" aria-hidden="true" tabindex="-1"></a><span class="co">        The response of each other word is the response matrix and the information passed on is the information matrix. </span></span>
<span id="cb19-206"><a href="#cb19-206" aria-hidden="true" tabindex="-1"></a><span class="co">        inputs:</span></span>
<span id="cb19-207"><a href="#cb19-207" aria-hidden="true" tabindex="-1"></a><span class="co">            attention_matrix: An ndarray of attention values</span></span>
<span id="cb19-208"><a href="#cb19-208" aria-hidden="true" tabindex="-1"></a><span class="co">            response_matrix: An ndarray of response values</span></span>
<span id="cb19-209"><a href="#cb19-209" aria-hidden="true" tabindex="-1"></a><span class="co">            information_matrix: An ndarray of information values that is passed on</span></span>
<span id="cb19-210"><a href="#cb19-210" aria-hidden="true" tabindex="-1"></a><span class="co">            mask: An ndarray of masks, but with Bool values</span></span>
<span id="cb19-211"><a href="#cb19-211" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-212"><a href="#cb19-212" aria-hidden="true" tabindex="-1"></a><span class="co">            a tuple of the information passed on and the attention weights</span></span>
<span id="cb19-213"><a href="#cb19-213" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span> </span>
<span id="cb19-214"><a href="#cb19-214" aria-hidden="true" tabindex="-1"></a>    normalization_factor<span class="op">=</span>attention_matrix.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb19-215"><a href="#cb19-215" aria-hidden="true" tabindex="-1"></a>    scores<span class="op">=</span>np.matmul(attention_matrix, response_matrix.transpose(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>))<span class="op">/</span>np.sqrt(normalization_factor) <span class="co">#we are now using 'official' terminology</span></span>
<span id="cb19-216"><a href="#cb19-216" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb19-217"><a href="#cb19-217" aria-hidden="true" tabindex="-1"></a>        scores<span class="op">=</span>np.where(mask[np.newaxis, np.newaxis,:,:], <span class="op">-</span><span class="fl">1e9</span>, scores) <span class="co">#mask if required</span></span>
<span id="cb19-218"><a href="#cb19-218" aria-hidden="true" tabindex="-1"></a>    attn_weights<span class="op">=</span>softmax(scores) <span class="co">#softmax the product</span></span>
<span id="cb19-219"><a href="#cb19-219" aria-hidden="true" tabindex="-1"></a>    output<span class="op">=</span>np.matmul(attn_weights, information_matrix) <span class="co">#send information on</span></span>
<span id="cb19-220"><a href="#cb19-220" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output, attn_weights</span>
<span id="cb19-221"><a href="#cb19-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-222"><a href="#cb19-222" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multi_head_attention(batched_input_intrinsic_value_matrix: np.ndarray, batched_input_response_matrix: np.ndarray, batched_input_information_matrix: np.ndarray, batched_input_intrinsic_value_projection_matrix: np.ndarray, batched_input_response_projection_matrix: np.ndarray, batched_input_information_projection_matrix: np.ndarray, final_reshaper_matrix: np.ndarray, num_heads: <span class="bu">int</span>, mask: Optional[np.ndarray]<span class="op">=</span><span class="va">None</span>) <span class="op">-&gt;</span> Tuple[np.ndarray, np.ndarray]:</span>
<span id="cb19-223"><a href="#cb19-223" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-224"><a href="#cb19-224" aria-hidden="true" tabindex="-1"></a><span class="co">        Implementation of multihead attention. Simply put, apply attention on smaller parts of the sequence, and then recombine them by concatenation.</span></span>
<span id="cb19-225"><a href="#cb19-225" aria-hidden="true" tabindex="-1"></a><span class="co">        Because we are splitting the sequence, we need to generate different attention, response, and information matrices for each attention instance.</span></span>
<span id="cb19-226"><a href="#cb19-226" aria-hidden="true" tabindex="-1"></a><span class="co">        This means we have to project the input into different matrices every time, and the matrix that does this projection is also learned.</span></span>
<span id="cb19-227"><a href="#cb19-227" aria-hidden="true" tabindex="-1"></a><span class="co">        This is the trick behind multihead attention.</span></span>
<span id="cb19-228"><a href="#cb19-228" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-229"><a href="#cb19-229" aria-hidden="true" tabindex="-1"></a><span class="co">            batched_input_intrinsic_value_matrix: an ndarray of your batched input</span></span>
<span id="cb19-230"><a href="#cb19-230" aria-hidden="true" tabindex="-1"></a><span class="co">            batched_input_response_matrix: an ndarray that is essentially the same as batched_input_intrinsic_value</span></span>
<span id="cb19-231"><a href="#cb19-231" aria-hidden="true" tabindex="-1"></a><span class="co">            batched_input_information_matrix: same as above. you have to declare that these matrices exist </span></span>
<span id="cb19-232"><a href="#cb19-232" aria-hidden="true" tabindex="-1"></a><span class="co">            batched_input_intrinsic_value_projection_matrix: an ndarray that projects the intrinsic value to the size for multihead attention</span></span>
<span id="cb19-233"><a href="#cb19-233" aria-hidden="true" tabindex="-1"></a><span class="co">            batched_input_response_projection_matrix: same thing for response_matrix</span></span>
<span id="cb19-234"><a href="#cb19-234" aria-hidden="true" tabindex="-1"></a><span class="co">            batched_input_information_projection_matrix: same thing as above</span></span>
<span id="cb19-235"><a href="#cb19-235" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb19-236"><a href="#cb19-236" aria-hidden="true" tabindex="-1"></a><span class="co">            A tuple of ndarrays that have the concatenated result of multihead attention and also the attention weights respectively. </span></span>
<span id="cb19-237"><a href="#cb19-237" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-238"><a href="#cb19-238" aria-hidden="true" tabindex="-1"></a>    attention_matrix<span class="op">=</span>batched_input_intrinsic_value_matrix <span class="op">@</span> batched_input_intrinsic_value_projection_matrix <span class="co">#we now use the inbuilt @ operator to do matrix multiplication quickly</span></span>
<span id="cb19-239"><a href="#cb19-239" aria-hidden="true" tabindex="-1"></a>    response_matrix<span class="op">=</span>batched_input_response_matrix <span class="op">@</span> batched_input_intrinsic_value_projection_matrix</span>
<span id="cb19-240"><a href="#cb19-240" aria-hidden="true" tabindex="-1"></a>    information_matrix<span class="op">=</span>batched_input_information_matrix <span class="op">@</span> batched_input_information_projection_matrix</span>
<span id="cb19-241"><a href="#cb19-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-242"><a href="#cb19-242" aria-hidden="true" tabindex="-1"></a>    <span class="co">#extract dimensions from the attention matrix (query tensor)</span></span>
<span id="cb19-243"><a href="#cb19-243" aria-hidden="true" tabindex="-1"></a>    <span class="co">#attention_matrix` is the query tensor with shape (batch_size, seq_len_q, d_model)</span></span>
<span id="cb19-244"><a href="#cb19-244" aria-hidden="true" tabindex="-1"></a>    batch, lq, d_model<span class="op">=</span>attention_matrix.shape</span>
<span id="cb19-245"><a href="#cb19-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-246"><a href="#cb19-246" aria-hidden="true" tabindex="-1"></a>    <span class="co">#extract the length of the key tensor from the response matrix</span></span>
<span id="cb19-247"><a href="#cb19-247" aria-hidden="true" tabindex="-1"></a>    <span class="co">#response_matrix is the key tensor with shape (batch_size, seq_len_k, d_model)</span></span>
<span id="cb19-248"><a href="#cb19-248" aria-hidden="true" tabindex="-1"></a>    <span class="co">#lk represents seq_len_k (sequence length of the response_matrix), which may differ from lq (seq_len_q)</span></span>
<span id="cb19-249"><a href="#cb19-249" aria-hidden="true" tabindex="-1"></a>    lk<span class="op">=</span>response_matrix.shape[<span class="dv">1</span>]</span>
<span id="cb19-250"><a href="#cb19-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-251"><a href="#cb19-251" aria-hidden="true" tabindex="-1"></a>    <span class="co">#we don't explicitly compute `lv` (seq_len_v, length of values) because information_matrix is expected to have the same sequence length as response_matrix</span></span>
<span id="cb19-252"><a href="#cb19-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-253"><a href="#cb19-253" aria-hidden="true" tabindex="-1"></a>    <span class="co">#reshape and transpose attention_matrix to prepare for multi-head attention</span></span>
<span id="cb19-254"><a href="#cb19-254" aria-hidden="true" tabindex="-1"></a>    <span class="co">#step 1: reshape attention_matrix from (batch_size, seq_len_q, d_model) to (batch_size, seq_len_q, num_heads, head_dim), where head_dim = d_model//num_heads</span></span>
<span id="cb19-255"><a href="#cb19-255" aria-hidden="true" tabindex="-1"></a>    <span class="co">#step 2: transpose to (batch_size, num_heads, seq_len_q, head_dim) for easier computation per head</span></span>
<span id="cb19-256"><a href="#cb19-256" aria-hidden="true" tabindex="-1"></a>    A<span class="op">=</span>attention_matrix.reshape(batch, lq, num_heads, d_model<span class="op">//</span>num_heads).transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb19-257"><a href="#cb19-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-258"><a href="#cb19-258" aria-hidden="true" tabindex="-1"></a>    <span class="co">#reshape and transpose response_matrix similarly</span></span>
<span id="cb19-259"><a href="#cb19-259" aria-hidden="true" tabindex="-1"></a>    B<span class="op">=</span>response_matrix.reshape(batch, lk, num_heads, d_model<span class="op">//</span>num_heads).transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb19-260"><a href="#cb19-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-261"><a href="#cb19-261" aria-hidden="true" tabindex="-1"></a>    <span class="co">#reshape and transpose information_matrix similarly</span></span>
<span id="cb19-262"><a href="#cb19-262" aria-hidden="true" tabindex="-1"></a>    C<span class="op">=</span>information_matrix.reshape(batch, lk, num_heads, d_model<span class="op">//</span>num_heads).transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb19-263"><a href="#cb19-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-264"><a href="#cb19-264" aria-hidden="true" tabindex="-1"></a>    <span class="co">#compute the attention each head</span></span>
<span id="cb19-265"><a href="#cb19-265" aria-hidden="true" tabindex="-1"></a>    <span class="co">#input ndarrays (A, B, B) are now split into individual heads for parallel processing</span></span>
<span id="cb19-266"><a href="#cb19-266" aria-hidden="true" tabindex="-1"></a>    <span class="co">#'mask` is optional and is used to block certain positions (e.g., future positions in autoregressive decoding)</span></span>
<span id="cb19-267"><a href="#cb19-267" aria-hidden="true" tabindex="-1"></a>    out_heads, attn_weights<span class="op">=</span>scaled_dot_product_attention(A, B, C, mask)</span>
<span id="cb19-268"><a href="#cb19-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-269"><a href="#cb19-269" aria-hidden="true" tabindex="-1"></a>    <span class="co">#concatenate the output</span></span>
<span id="cb19-270"><a href="#cb19-270" aria-hidden="true" tabindex="-1"></a>    out<span class="op">=</span>(merge_heads(out_heads))<span class="op">@</span>final_reshaper_matrix  <span class="co"># Apply a linear projection to combine the head outputs into `d_model` dimensions.</span></span>
<span id="cb19-271"><a href="#cb19-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-272"><a href="#cb19-272" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out, attn_weights</span>
<span id="cb19-273"><a href="#cb19-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-274"><a href="#cb19-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-275"><a href="#cb19-275" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mha_backprop(</span>
<span id="cb19-276"><a href="#cb19-276" aria-hidden="true" tabindex="-1"></a>        dout: np.ndarray,</span>
<span id="cb19-277"><a href="#cb19-277" aria-hidden="true" tabindex="-1"></a>        batched_input_intrinsic_value_matrix: np.ndarray,</span>
<span id="cb19-278"><a href="#cb19-278" aria-hidden="true" tabindex="-1"></a>        batched_input_response_matrix: np.ndarray,</span>
<span id="cb19-279"><a href="#cb19-279" aria-hidden="true" tabindex="-1"></a>        batched_input_information_matrix: np.ndarray, </span>
<span id="cb19-280"><a href="#cb19-280" aria-hidden="true" tabindex="-1"></a>        batched_input_intrinsic_value_projection_matrix: np.ndarray,</span>
<span id="cb19-281"><a href="#cb19-281" aria-hidden="true" tabindex="-1"></a>        batched_input_response_projection_matrix: np.ndarray, </span>
<span id="cb19-282"><a href="#cb19-282" aria-hidden="true" tabindex="-1"></a>        batched_input_information_projection_matrix: np.ndarray, </span>
<span id="cb19-283"><a href="#cb19-283" aria-hidden="true" tabindex="-1"></a>        final_projection_matrix: np.ndarray,</span>
<span id="cb19-284"><a href="#cb19-284" aria-hidden="true" tabindex="-1"></a>        attention_weights: np.ndarray, </span>
<span id="cb19-285"><a href="#cb19-285" aria-hidden="true" tabindex="-1"></a>        num_heads: <span class="bu">int</span>, </span>
<span id="cb19-286"><a href="#cb19-286" aria-hidden="true" tabindex="-1"></a>        mask: Optional[np.ndarray]<span class="op">=</span><span class="va">None</span></span>
<span id="cb19-287"><a href="#cb19-287" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:</span>
<span id="cb19-288"><a href="#cb19-288" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-289"><a href="#cb19-289" aria-hidden="true" tabindex="-1"></a><span class="co">        This is the big one. This is the function that backpropagates through multihead attention. </span></span>
<span id="cb19-290"><a href="#cb19-290" aria-hidden="true" tabindex="-1"></a><span class="co">        It computes the gradients of the loss with respect to the sequence matrices and the weight matrices used in the multi-head attention mechanism.</span></span>
<span id="cb19-291"><a href="#cb19-291" aria-hidden="true" tabindex="-1"></a><span class="co">        Like before, we are in fact feeding it the loss.</span></span>
<span id="cb19-292"><a href="#cb19-292" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-293"><a href="#cb19-293" aria-hidden="true" tabindex="-1"></a><span class="co">            dout: an ndarray of the gradient of the loss with respect to the output of multihead attention</span></span>
<span id="cb19-294"><a href="#cb19-294" aria-hidden="true" tabindex="-1"></a><span class="co">            batched_input_intrinsic_value_matrix: an ndarray as defined in the multihead attention function above,</span></span>
<span id="cb19-295"><a href="#cb19-295" aria-hidden="true" tabindex="-1"></a><span class="co">            batched_input_response_matrix: an ndarray as defined in the multihead attention function above ,</span></span>
<span id="cb19-296"><a href="#cb19-296" aria-hidden="true" tabindex="-1"></a><span class="co">            batched_input_information_matrix: an ndarray as defined in the multihead attention function above,   </span></span>
<span id="cb19-297"><a href="#cb19-297" aria-hidden="true" tabindex="-1"></a><span class="co">            batched_input_intrinsic_value_projection_matrix: an ndarray as defined in the multihead attention function above,</span></span>
<span id="cb19-298"><a href="#cb19-298" aria-hidden="true" tabindex="-1"></a><span class="co">            batched_input_response_projection_matrix: an ndarray as defined in the multihead attention function above, </span></span>
<span id="cb19-299"><a href="#cb19-299" aria-hidden="true" tabindex="-1"></a><span class="co">            batched_input_information_projection_matrix: an ndarray as defined in the multihead attention function above, </span></span>
<span id="cb19-300"><a href="#cb19-300" aria-hidden="true" tabindex="-1"></a><span class="co">            final_projection_matrix: an ndarray as defined in the multihead attention function above,</span></span>
<span id="cb19-301"><a href="#cb19-301" aria-hidden="true" tabindex="-1"></a><span class="co">            attention_weights: an ndarray that is the output of multihead attention function, required for backpropagation</span></span>
<span id="cb19-302"><a href="#cb19-302" aria-hidden="true" tabindex="-1"></a><span class="co">            mask: an optional ndarray of masks (Bool data type)</span></span>
<span id="cb19-303"><a href="#cb19-303" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-304"><a href="#cb19-304" aria-hidden="true" tabindex="-1"></a><span class="co">            the following tuple: Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:</span></span>
<span id="cb19-305"><a href="#cb19-305" aria-hidden="true" tabindex="-1"></a><span class="co">            - differential_intrinsic_value_matrix (np.ndarray): Gradient w.r.t. the query input `batched_input_intrinsic_value_matrix`, shape (batch_size, seq_len_q, d_model).</span></span>
<span id="cb19-306"><a href="#cb19-306" aria-hidden="true" tabindex="-1"></a><span class="co">            - differential_response_matrix (np.ndarray): Gradient w.r.t. the key input `batched_input_response_matrix`, shape (batch_size, seq_len_k, d_model).</span></span>
<span id="cb19-307"><a href="#cb19-307" aria-hidden="true" tabindex="-1"></a><span class="co">            - differential_information_matrix (np.ndarray): Gradient w.r.t. the value input `batched_input_information_matrix`, shape (batch_size, seq_len_k, d_model).</span></span>
<span id="cb19-308"><a href="#cb19-308" aria-hidden="true" tabindex="-1"></a><span class="co">            - original_shape_intrinsic_value_differential (np.ndarray): Gradient w.r.t. the query weight matrix `batched_input_intrinsic_value_projection_matrix`, shape (d_model, d_model).</span></span>
<span id="cb19-309"><a href="#cb19-309" aria-hidden="true" tabindex="-1"></a><span class="co">            - original_shape_response_differential (np.ndarray): Gradient w.r.t. the key weight matrix `batched_input_response_projection_matrix`, shape (d_model, d_model).</span></span>
<span id="cb19-310"><a href="#cb19-310" aria-hidden="true" tabindex="-1"></a><span class="co">            - original_shape_information_differential (np.ndarray): Gradient w.r.t. the value weight batched_input_information_projection_matrix `W_v`, shape (d_model, d_model).</span></span>
<span id="cb19-311"><a href="#cb19-311" aria-hidden="true" tabindex="-1"></a><span class="co">            - original_shape_attention_differential (np.ndarray): Gradient w.r.t. the output weight matrix `final_projection_matrix`, shape (d_model, d_model).</span></span>
<span id="cb19-312"><a href="#cb19-312" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-313"><a href="#cb19-313" aria-hidden="true" tabindex="-1"></a>    <span class="co">#throughout the code, a variable starting with 'd' denotes a derivative/gradient, aside from d_model, which is the model dimension (user-defined really)</span></span>
<span id="cb19-314"><a href="#cb19-314" aria-hidden="true" tabindex="-1"></a>    batch, attention_sequence_length, d_model<span class="op">=</span>batched_input_intrinsic_value_matrix.shape <span class="co">#compute the gradient matrix</span></span>
<span id="cb19-315"><a href="#cb19-315" aria-hidden="true" tabindex="-1"></a>    sequence_length<span class="op">=</span>batched_input_response_matrix.shape[<span class="dv">1</span>] <span class="co">#length of the sequence, obtained from really any matrix</span></span>
<span id="cb19-316"><a href="#cb19-316" aria-hidden="true" tabindex="-1"></a>    head_dim<span class="op">=</span>d_model<span class="op">//</span>num_heads <span class="co">#just the model head size</span></span>
<span id="cb19-317"><a href="#cb19-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-318"><a href="#cb19-318" aria-hidden="true" tabindex="-1"></a>    <span class="co">#recompute the forward step</span></span>
<span id="cb19-319"><a href="#cb19-319" aria-hidden="true" tabindex="-1"></a>    attention_matrix<span class="op">=</span>batched_input_intrinsic_value_matrix <span class="op">@</span> batched_input_intrinsic_value_projection_matrix <span class="co">#standard stuff</span></span>
<span id="cb19-320"><a href="#cb19-320" aria-hidden="true" tabindex="-1"></a>    response_matrix<span class="op">=</span>batched_input_response_matrix <span class="op">@</span> batched_input_response_projection_matrix</span>
<span id="cb19-321"><a href="#cb19-321" aria-hidden="true" tabindex="-1"></a>    information_matrix<span class="op">=</span>batched_input_information_matrix <span class="op">@</span> batched_input_information_projection_matrix</span>
<span id="cb19-322"><a href="#cb19-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-323"><a href="#cb19-323" aria-hidden="true" tabindex="-1"></a>    <span class="co">#similarly reshaping stuff as done before</span></span>
<span id="cb19-324"><a href="#cb19-324" aria-hidden="true" tabindex="-1"></a>    head_attention_matrix<span class="op">=</span>attention_matrix.reshape(batch, attention_sequence_length, num_heads, head_dim).transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb19-325"><a href="#cb19-325" aria-hidden="true" tabindex="-1"></a>    head_response_matrix<span class="op">=</span>response_matrix.reshape(batch, sequence_length, num_heads, head_dim).transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb19-326"><a href="#cb19-326" aria-hidden="true" tabindex="-1"></a>    head_information_matrix<span class="op">=</span>information_matrix.reshape(batch, sequence_length, num_heads, head_dim).transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb19-327"><a href="#cb19-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-328"><a href="#cb19-328" aria-hidden="true" tabindex="-1"></a>    d_merged_heads<span class="op">=</span>dout.reshape(batch, attention_sequence_length, d_model) <span class="co">#reshape dout into the correct shape</span></span>
<span id="cb19-329"><a href="#cb19-329" aria-hidden="true" tabindex="-1"></a>    d_out_heads<span class="op">=</span>d_merged_heads.reshape(batch, attention_sequence_length, num_heads, head_dim).transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb19-330"><a href="#cb19-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-331"><a href="#cb19-331" aria-hidden="true" tabindex="-1"></a>    <span class="co">#check shape consistency, because it makes for easier debugging</span></span>
<span id="cb19-332"><a href="#cb19-332" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> d_out_heads.shape<span class="op">==</span>(batch, num_heads, attention_sequence_length, head_dim), <span class="ss">f"Shape mismatch in d_out_heads: </span><span class="sc">{</span>d_out_heads<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span> <span class="co">#first time we've used 'assert'</span></span>
<span id="cb19-333"><a href="#cb19-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-334"><a href="#cb19-334" aria-hidden="true" tabindex="-1"></a>    <span class="co">#attention weights and value gradients</span></span>
<span id="cb19-335"><a href="#cb19-335" aria-hidden="true" tabindex="-1"></a>    d_attention_weights<span class="op">=</span>np.matmul(d_out_heads, head_information_matrix.transpose(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>)) <span class="co">#this is pretty standard fare, start getting the stuff out from backprop</span></span>
<span id="cb19-336"><a href="#cb19-336" aria-hidden="true" tabindex="-1"></a>    d_vh<span class="op">=</span>np.matmul(attention_weights.transpose(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>), d_out_heads) <span class="co">#this is also the same thing</span></span>
<span id="cb19-337"><a href="#cb19-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-338"><a href="#cb19-338" aria-hidden="true" tabindex="-1"></a>    <span class="co">#calculate the differential element of the attention scores, for backpropagation</span></span>
<span id="cb19-339"><a href="#cb19-339" aria-hidden="true" tabindex="-1"></a>    sum_over_j<span class="op">=</span>np.<span class="bu">sum</span>(attention_weights<span class="op">*</span>d_attention_weights, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-340"><a href="#cb19-340" aria-hidden="true" tabindex="-1"></a>    d_scores<span class="op">=</span>attention_weights<span class="op">*</span>(d_attention_weights<span class="op">-</span>sum_over_j)</span>
<span id="cb19-341"><a href="#cb19-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-342"><a href="#cb19-342" aria-hidden="true" tabindex="-1"></a>    <span class="co">#calculate the differential elements for the attention and response matrices, per head</span></span>
<span id="cb19-343"><a href="#cb19-343" aria-hidden="true" tabindex="-1"></a>    d_attention_head<span class="op">=</span>np.matmul(d_scores, head_response_matrix)<span class="op">/</span>np.sqrt(head_dim)</span>
<span id="cb19-344"><a href="#cb19-344" aria-hidden="true" tabindex="-1"></a>    d_response_head<span class="op">=</span>np.matmul(d_scores.transpose(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>), head_attention_matrix)<span class="op">/</span>np.sqrt(head_dim)</span>
<span id="cb19-345"><a href="#cb19-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-346"><a href="#cb19-346" aria-hidden="true" tabindex="-1"></a>    <span class="co">#combine the elements back for concatenation</span></span>
<span id="cb19-347"><a href="#cb19-347" aria-hidden="true" tabindex="-1"></a>    total_attention_differential<span class="op">=</span>d_attention_head.transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>).reshape(batch, attention_sequence_length, d_model)</span>
<span id="cb19-348"><a href="#cb19-348" aria-hidden="true" tabindex="-1"></a>    total_response_differential<span class="op">=</span>d_response_head.transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>).reshape(batch, sequence_length, d_model)</span>
<span id="cb19-349"><a href="#cb19-349" aria-hidden="true" tabindex="-1"></a>    total_information_differential<span class="op">=</span>d_vh.transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>).reshape(batch, sequence_length, d_model)</span>
<span id="cb19-350"><a href="#cb19-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-351"><a href="#cb19-351" aria-hidden="true" tabindex="-1"></a>    <span class="co">#now calculate the total gradients for all elements</span></span>
<span id="cb19-352"><a href="#cb19-352" aria-hidden="true" tabindex="-1"></a>    original_shape_intrinsic_value_differential<span class="op">=</span>np.matmul(batched_input_intrinsic_value_matrix.reshape(<span class="op">-</span><span class="dv">1</span>, d_model).T, total_attention_differential.reshape(<span class="op">-</span><span class="dv">1</span>, d_model))</span>
<span id="cb19-353"><a href="#cb19-353" aria-hidden="true" tabindex="-1"></a>    original_shape_response_differential<span class="op">=</span>np.matmul(batched_input_response_matrix.reshape(<span class="op">-</span><span class="dv">1</span>, d_model).T, total_response_differential.reshape(<span class="op">-</span><span class="dv">1</span>, d_model))</span>
<span id="cb19-354"><a href="#cb19-354" aria-hidden="true" tabindex="-1"></a>    original_shape_information_differential<span class="op">=</span>np.matmul(batched_input_information_matrix.reshape(<span class="op">-</span><span class="dv">1</span>, d_model).T, total_information_differential.reshape(<span class="op">-</span><span class="dv">1</span>, d_model))</span>
<span id="cb19-355"><a href="#cb19-355" aria-hidden="true" tabindex="-1"></a>    <span class="co">#original_shape_attention_differential=np.matmul(merge_heads(attention_weights @ information_matrix).reshape(batch*attention_sequence_length, d_model).T, dout.reshape(batch*attention_sequence_length, d_model))</span></span>
<span id="cb19-356"><a href="#cb19-356" aria-hidden="true" tabindex="-1"></a>    C <span class="op">=</span> information_matrix.reshape(batch, sequence_length, num_heads, head_dim).transpose(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb19-357"><a href="#cb19-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-358"><a href="#cb19-358" aria-hidden="true" tabindex="-1"></a>    original_shape_attention_differential <span class="op">=</span> np.matmul(</span>
<span id="cb19-359"><a href="#cb19-359" aria-hidden="true" tabindex="-1"></a>    merge_heads(np.matmul(attention_weights, C)).reshape(batch <span class="op">*</span> attention_sequence_length, d_model).T,</span>
<span id="cb19-360"><a href="#cb19-360" aria-hidden="true" tabindex="-1"></a>    dout.reshape(batch <span class="op">*</span> attention_sequence_length, d_model)</span>
<span id="cb19-361"><a href="#cb19-361" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb19-362"><a href="#cb19-362" aria-hidden="true" tabindex="-1"></a>    differential_intrinsic_value_matrix <span class="op">=</span> total_attention_differential <span class="op">@</span> batched_input_intrinsic_value_projection_matrix.T</span>
<span id="cb19-363"><a href="#cb19-363" aria-hidden="true" tabindex="-1"></a>    differential_response_matrix <span class="op">=</span> total_response_differential <span class="op">@</span> batched_input_response_projection_matrix.T</span>
<span id="cb19-364"><a href="#cb19-364" aria-hidden="true" tabindex="-1"></a>    differential_information_matrix <span class="op">=</span> total_information_differential <span class="op">@</span> batched_input_information_projection_matrix.T</span>
<span id="cb19-365"><a href="#cb19-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-366"><a href="#cb19-366" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> differential_intrinsic_value_matrix, differential_response_matrix, differential_information_matrix, original_shape_intrinsic_value_differential, original_shape_response_differential, original_shape_information_differential, original_shape_attention_differential</span>
<span id="cb19-367"><a href="#cb19-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-368"><a href="#cb19-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-369"><a href="#cb19-369" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb19-370"><a href="#cb19-370" aria-hidden="true" tabindex="-1"></a><span class="co"># Feed Forward Network</span></span>
<span id="cb19-371"><a href="#cb19-371" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb19-372"><a href="#cb19-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-373"><a href="#cb19-373" aria-hidden="true" tabindex="-1"></a><span class="co">#now we define the feedforward neural network (2 layers) with backprop. we use the ReLU activation function for easy differentials</span></span>
<span id="cb19-374"><a href="#cb19-374" aria-hidden="true" tabindex="-1"></a><span class="co">#note the different output in the definition of the feedforward network</span></span>
<span id="cb19-375"><a href="#cb19-375" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> feed_forward(x: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray) <span class="op">-&gt;</span> Tuple[np.ndarray, Tuple[np.ndarray, np.ndarray]]:</span>
<span id="cb19-376"><a href="#cb19-376" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" </span></span>
<span id="cb19-377"><a href="#cb19-377" aria-hidden="true" tabindex="-1"></a><span class="co">        This is a simple two layer feedforward neural network. The reason a cache is returned at all is because it is immensely helpful in backpropagation</span></span>
<span id="cb19-378"><a href="#cb19-378" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-379"><a href="#cb19-379" aria-hidden="true" tabindex="-1"></a><span class="co">            x: input ndarray of data</span></span>
<span id="cb19-380"><a href="#cb19-380" aria-hidden="true" tabindex="-1"></a><span class="co">            W1: weight matrix for the first linear transformation</span></span>
<span id="cb19-381"><a href="#cb19-381" aria-hidden="true" tabindex="-1"></a><span class="co">            b1: bias vector for first layer</span></span>
<span id="cb19-382"><a href="#cb19-382" aria-hidden="true" tabindex="-1"></a><span class="co">            W2, b2: same as above for second layer</span></span>
<span id="cb19-383"><a href="#cb19-383" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-384"><a href="#cb19-384" aria-hidden="true" tabindex="-1"></a><span class="co">            Tuple[np.ndarray, Tuple[np.ndarray, np.ndarray]]:</span></span>
<span id="cb19-385"><a href="#cb19-385" aria-hidden="true" tabindex="-1"></a><span class="co">            - z2 (np.ndarray): the output tensor after the feed-forward computation of shape (batch_size, seq_len, d_model).</span></span>
<span id="cb19-386"><a href="#cb19-386" aria-hidden="true" tabindex="-1"></a><span class="co">            - cache (Tuple[np.ndarray, np.ndarray]): a tuple containing:</span></span>
<span id="cb19-387"><a href="#cb19-387" aria-hidden="true" tabindex="-1"></a><span class="co">                - z1 (np.ndarray): the output of the first linear transformation before the ReLU activation, of shape (batch_size, seq_len, d_ff).</span></span>
<span id="cb19-388"><a href="#cb19-388" aria-hidden="true" tabindex="-1"></a><span class="co">                - relu (np.ndarray): The output of the ReLU activation, of shape (batch_size, seq_len, d_ff).</span></span>
<span id="cb19-389"><a href="#cb19-389" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-390"><a href="#cb19-390" aria-hidden="true" tabindex="-1"></a>    <span class="co">#this is just straighforward - two matrix multiplications with a ReLU in between</span></span>
<span id="cb19-391"><a href="#cb19-391" aria-hidden="true" tabindex="-1"></a>    z1<span class="op">=</span>x<span class="op">@</span>W1<span class="op">+</span>b1</span>
<span id="cb19-392"><a href="#cb19-392" aria-hidden="true" tabindex="-1"></a>    relu<span class="op">=</span>np.maximum(<span class="dv">0</span>, z1)</span>
<span id="cb19-393"><a href="#cb19-393" aria-hidden="true" tabindex="-1"></a>    z2<span class="op">=</span>relu<span class="op">@</span>W2<span class="op">+</span>b2</span>
<span id="cb19-394"><a href="#cb19-394" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z2, (z1, relu)</span>
<span id="cb19-395"><a href="#cb19-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-396"><a href="#cb19-396" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> feed_forward_backprop(dz2: np.ndarray, x: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray, cache: Tuple[np.ndarray, np.ndarray]) <span class="op">-&gt;</span> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:</span>
<span id="cb19-397"><a href="#cb19-397" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-398"><a href="#cb19-398" aria-hidden="true" tabindex="-1"></a><span class="co">        The backpropagation for a feedforward layer. Note that we are working backwards, so the input variables are defined that way</span></span>
<span id="cb19-399"><a href="#cb19-399" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-400"><a href="#cb19-400" aria-hidden="true" tabindex="-1"></a><span class="co">            dz2: ndarray containing gradient of the loss function with respect to output of the second layer</span></span>
<span id="cb19-401"><a href="#cb19-401" aria-hidden="true" tabindex="-1"></a><span class="co">            x: input ndarray of data</span></span>
<span id="cb19-402"><a href="#cb19-402" aria-hidden="true" tabindex="-1"></a><span class="co">            W1-cache: same as above</span></span>
<span id="cb19-403"><a href="#cb19-403" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-404"><a href="#cb19-404" aria-hidden="true" tabindex="-1"></a><span class="co">            Tuple of losses (ndarrays) with respect to x,W1,b1,W2,b2 respectively</span></span>
<span id="cb19-405"><a href="#cb19-405" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-406"><a href="#cb19-406" aria-hidden="true" tabindex="-1"></a>    <span class="co">#backpropagation, step by step, is the exact formula outlines in the tutorial</span></span>
<span id="cb19-407"><a href="#cb19-407" aria-hidden="true" tabindex="-1"></a>    (z1, relu)<span class="op">=</span>cache <span class="co">#now you see why we utilized the cache at all</span></span>
<span id="cb19-408"><a href="#cb19-408" aria-hidden="true" tabindex="-1"></a>    batch, length, d_model<span class="op">=</span>x.shape</span>
<span id="cb19-409"><a href="#cb19-409" aria-hidden="true" tabindex="-1"></a>    dW2<span class="op">=</span>np.matmul(relu.reshape(<span class="op">-</span><span class="dv">1</span>, d_model).T, dz2.reshape(<span class="op">-</span><span class="dv">1</span>, d_model))</span>
<span id="cb19-410"><a href="#cb19-410" aria-hidden="true" tabindex="-1"></a>    db2<span class="op">=</span>dz2.<span class="bu">sum</span>(axis<span class="op">=</span>(<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb19-411"><a href="#cb19-411" aria-hidden="true" tabindex="-1"></a>    d_relu<span class="op">=</span>dz2<span class="op">@</span>W2.T</span>
<span id="cb19-412"><a href="#cb19-412" aria-hidden="true" tabindex="-1"></a>    d_z1<span class="op">=</span>d_relu<span class="op">*</span>(z1<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb19-413"><a href="#cb19-413" aria-hidden="true" tabindex="-1"></a>    dW1<span class="op">=</span>np.matmul(x.reshape(<span class="op">-</span><span class="dv">1</span>, d_model).T, d_z1.reshape(<span class="op">-</span><span class="dv">1</span>, d_model))</span>
<span id="cb19-414"><a href="#cb19-414" aria-hidden="true" tabindex="-1"></a>    db1<span class="op">=</span>d_z1.<span class="bu">sum</span>(axis<span class="op">=</span>(<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb19-415"><a href="#cb19-415" aria-hidden="true" tabindex="-1"></a>    dX<span class="op">=</span>d_z1<span class="op">@</span>W1.T</span>
<span id="cb19-416"><a href="#cb19-416" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dX, dW1, db1, dW2, db2</span>
<span id="cb19-417"><a href="#cb19-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-418"><a href="#cb19-418" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb19-419"><a href="#cb19-419" aria-hidden="true" tabindex="-1"></a><span class="co"># Encoder and Decoder Blocks</span></span>
<span id="cb19-420"><a href="#cb19-420" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb19-421"><a href="#cb19-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-422"><a href="#cb19-422" aria-hidden="true" tabindex="-1"></a><span class="co">#we now build the actual encoder-decoder layer.</span></span>
<span id="cb19-423"><a href="#cb19-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-424"><a href="#cb19-424" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encoder_layer(x: np.ndarray, intrinsic_value_projector: np.ndarray, response_projector: np.ndarray, information_projector: np.ndarray, attention_projector: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray, num_heads: <span class="bu">int</span><span class="op">=</span><span class="dv">2</span>) <span class="op">-&gt;</span> Tuple[np.ndarray, Tuple]:</span>
<span id="cb19-425"><a href="#cb19-425" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-426"><a href="#cb19-426" aria-hidden="true" tabindex="-1"></a><span class="co">        Now we build the transformer in earnest from all of our classes. This implements the encoder layer using the structure outlined in the tutorial</span></span>
<span id="cb19-427"><a href="#cb19-427" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-428"><a href="#cb19-428" aria-hidden="true" tabindex="-1"></a><span class="co">            x: ndarray of input data</span></span>
<span id="cb19-429"><a href="#cb19-429" aria-hidden="true" tabindex="-1"></a><span class="co">            intrinsic_value_projector, response_projector, information_projector, attention_projector: ndarrays containing projection matrices for input data</span></span>
<span id="cb19-430"><a href="#cb19-430" aria-hidden="true" tabindex="-1"></a><span class="co">            W1, b1, W2, b2: ndarrays of the two-layer neural network</span></span>
<span id="cb19-431"><a href="#cb19-431" aria-hidden="true" tabindex="-1"></a><span class="co">            num_heads: number of attention heads</span></span>
<span id="cb19-432"><a href="#cb19-432" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-433"><a href="#cb19-433" aria-hidden="true" tabindex="-1"></a><span class="co">            out: output ndarray of the entire encoder</span></span>
<span id="cb19-434"><a href="#cb19-434" aria-hidden="true" tabindex="-1"></a><span class="co">            cache: a Tuple of all cached values for backpropagation</span></span>
<span id="cb19-435"><a href="#cb19-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-436"><a href="#cb19-436" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-437"><a href="#cb19-437" aria-hidden="true" tabindex="-1"></a>    <span class="co">#at this point i start using smaller variable names because the program becomes tedious to read. anyway, this code is self-explanatory </span></span>
<span id="cb19-438"><a href="#cb19-438" aria-hidden="true" tabindex="-1"></a>    x_norm, mean1, var1<span class="op">=</span>layer_norm(x)</span>
<span id="cb19-439"><a href="#cb19-439" aria-hidden="true" tabindex="-1"></a>    attn_out, attn_w<span class="op">=</span>multi_head_attention(x_norm, x_norm, x_norm, intrinsic_value_projector, response_projector, information_projector, attention_projector, num_heads<span class="op">=</span>num_heads)</span>
<span id="cb19-440"><a href="#cb19-440" aria-hidden="true" tabindex="-1"></a>    x2<span class="op">=</span>x<span class="op">+</span>attn_out</span>
<span id="cb19-441"><a href="#cb19-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-442"><a href="#cb19-442" aria-hidden="true" tabindex="-1"></a>    x2_norm, mean2, var2<span class="op">=</span>layer_norm(x2)</span>
<span id="cb19-443"><a href="#cb19-443" aria-hidden="true" tabindex="-1"></a>    ff_out, ff_cache<span class="op">=</span>feed_forward(x2_norm,W1,b1,W2,b2)</span>
<span id="cb19-444"><a href="#cb19-444" aria-hidden="true" tabindex="-1"></a>    out<span class="op">=</span>x2<span class="op">+</span>ff_out</span>
<span id="cb19-445"><a href="#cb19-445" aria-hidden="true" tabindex="-1"></a>    cache<span class="op">=</span>(x, x2, x_norm, x2_norm, mean1, var1, mean2, var2, attn_w, ff_cache)</span>
<span id="cb19-446"><a href="#cb19-446" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out, cache</span>
<span id="cb19-447"><a href="#cb19-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-448"><a href="#cb19-448" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> decoder_layer(x, encoder_out,</span>
<span id="cb19-449"><a href="#cb19-449" aria-hidden="true" tabindex="-1"></a>                  intrinsic_weight_masked, response_weight_masked, information_weight_masked, attention_weight_masked,</span>
<span id="cb19-450"><a href="#cb19-450" aria-hidden="true" tabindex="-1"></a>                  intrinsic_crossattention_weight, response_crossattention_weight, information_crossattention_weight, attention_crossattention_weight, W1, b1, W2, b2, mask, num_heads<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb19-451"><a href="#cb19-451" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-452"><a href="#cb19-452" aria-hidden="true" tabindex="-1"></a><span class="co">        We implement the decoder layer with masked attention.</span></span>
<span id="cb19-453"><a href="#cb19-453" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs: </span></span>
<span id="cb19-454"><a href="#cb19-454" aria-hidden="true" tabindex="-1"></a><span class="co">            x: ndarray of input data</span></span>
<span id="cb19-455"><a href="#cb19-455" aria-hidden="true" tabindex="-1"></a><span class="co">            intrinsic_weight_masked: projection matrix for masked self-attention for the intrinsic value weight</span></span>
<span id="cb19-456"><a href="#cb19-456" aria-hidden="true" tabindex="-1"></a><span class="co">            response_weight_masked: projection matrix for masked self-attention for the response weight</span></span>
<span id="cb19-457"><a href="#cb19-457" aria-hidden="true" tabindex="-1"></a><span class="co">            information_weight_masked: projection matrix for masked self-attention for the information weight</span></span>
<span id="cb19-458"><a href="#cb19-458" aria-hidden="true" tabindex="-1"></a><span class="co">            attention_weight_masked: projection matrix for masked self-attention for the attention weights</span></span>
<span id="cb19-459"><a href="#cb19-459" aria-hidden="true" tabindex="-1"></a><span class="co">            intrinsic_crossattention_weight: projection matrix for the intrinsic value weight during cross-attention</span></span>
<span id="cb19-460"><a href="#cb19-460" aria-hidden="true" tabindex="-1"></a><span class="co">            response_crossattention_weight: projection matrix for the response weight during cross-attention</span></span>
<span id="cb19-461"><a href="#cb19-461" aria-hidden="true" tabindex="-1"></a><span class="co">            information_crossattention_weight: projection matrix for the information weight during cross-attention</span></span>
<span id="cb19-462"><a href="#cb19-462" aria-hidden="true" tabindex="-1"></a><span class="co">            attention_crossattention_weight: projection matrix for the attention weights during cross-attention</span></span>
<span id="cb19-463"><a href="#cb19-463" aria-hidden="true" tabindex="-1"></a><span class="co">            W1,b1,W2,b2, mask: all defined above</span></span>
<span id="cb19-464"><a href="#cb19-464" aria-hidden="true" tabindex="-1"></a><span class="co">            num_heads: number of attention instances</span></span>
<span id="cb19-465"><a href="#cb19-465" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-466"><a href="#cb19-466" aria-hidden="true" tabindex="-1"></a><span class="co">            Tuple[np.ndarray, Tuple]:</span></span>
<span id="cb19-467"><a href="#cb19-467" aria-hidden="true" tabindex="-1"></a><span class="co">            - out: output ndarray of the decoder layer, shape (batch_size, seq_len, d_model)</span></span>
<span id="cb19-468"><a href="#cb19-468" aria-hidden="true" tabindex="-1"></a><span class="co">            - cache (Tuple): cached values for backpropagation, including:</span></span>
<span id="cb19-469"><a href="#cb19-469" aria-hidden="true" tabindex="-1"></a><span class="co">                - x, x2, x3 : intermediate ndarrays at various stages</span></span>
<span id="cb19-470"><a href="#cb19-470" aria-hidden="true" tabindex="-1"></a><span class="co">                - x_norm_dec1, x2_norm, x3_norm : layer-normalized ndarrays</span></span>
<span id="cb19-471"><a href="#cb19-471" aria-hidden="true" tabindex="-1"></a><span class="co">                - mean_dec1, var_dec1, mean_dec2, var_dec2, mean_dec3, var_dec3: Means and variances (in ndarrays) from layer normalization</span></span>
<span id="cb19-472"><a href="#cb19-472" aria-hidden="true" tabindex="-1"></a><span class="co">                - masked_attn_w, cross_attn_w: attention weight ndarrays from masked self-attention and cross-attention</span></span>
<span id="cb19-473"><a href="#cb19-473" aria-hidden="true" tabindex="-1"></a><span class="co">                - ff_cache_dec (Tuple): cached values from the feed-forward network</span></span>
<span id="cb19-474"><a href="#cb19-474" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-475"><a href="#cb19-475" aria-hidden="true" tabindex="-1"></a>    <span class="co">#this just implements the decoder layer</span></span>
<span id="cb19-476"><a href="#cb19-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-477"><a href="#cb19-477" aria-hidden="true" tabindex="-1"></a>    <span class="co">#first, apply layer normalization, then compute multihead attention</span></span>
<span id="cb19-478"><a href="#cb19-478" aria-hidden="true" tabindex="-1"></a>    x_norm_dec1,mean_dec1,var_dec1<span class="op">=</span>layer_norm(x)</span>
<span id="cb19-479"><a href="#cb19-479" aria-hidden="true" tabindex="-1"></a>    masked_attn_out, masked_attn_w<span class="op">=</span>multi_head_attention(x_norm_dec1, x_norm_dec1, x_norm_dec1,</span>
<span id="cb19-480"><a href="#cb19-480" aria-hidden="true" tabindex="-1"></a>                                                          intrinsic_weight_masked, response_weight_masked, information_weight_masked, attention_weight_masked, </span>
<span id="cb19-481"><a href="#cb19-481" aria-hidden="true" tabindex="-1"></a>                                                          num_heads<span class="op">=</span>num_heads, mask<span class="op">=</span>mask)</span>
<span id="cb19-482"><a href="#cb19-482" aria-hidden="true" tabindex="-1"></a>    x2<span class="op">=</span>x<span class="op">+</span>masked_attn_out</span>
<span id="cb19-483"><a href="#cb19-483" aria-hidden="true" tabindex="-1"></a>    <span class="co">#then layernorm again</span></span>
<span id="cb19-484"><a href="#cb19-484" aria-hidden="true" tabindex="-1"></a>    x2_norm, mean_dec2, var_dec2<span class="op">=</span>layer_norm(x2)</span>
<span id="cb19-485"><a href="#cb19-485" aria-hidden="true" tabindex="-1"></a>    cross_attn_out, cross_attn_w<span class="op">=</span>multi_head_attention(x2_norm, encoder_out, encoder_out,</span>
<span id="cb19-486"><a href="#cb19-486" aria-hidden="true" tabindex="-1"></a>                                                        intrinsic_crossattention_weight, response_crossattention_weight, information_crossattention_weight, attention_crossattention_weight,</span>
<span id="cb19-487"><a href="#cb19-487" aria-hidden="true" tabindex="-1"></a>                                                        num_heads<span class="op">=</span>num_heads, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb19-488"><a href="#cb19-488" aria-hidden="true" tabindex="-1"></a>    x3<span class="op">=</span>x2<span class="op">+</span>cross_attn_out</span>
<span id="cb19-489"><a href="#cb19-489" aria-hidden="true" tabindex="-1"></a>    <span class="co">#layer norm thrice</span></span>
<span id="cb19-490"><a href="#cb19-490" aria-hidden="true" tabindex="-1"></a>    x3_norm, mean_dec3, var_dec3<span class="op">=</span>layer_norm(x3)</span>
<span id="cb19-491"><a href="#cb19-491" aria-hidden="true" tabindex="-1"></a>    ff_out, ff_cache_dec<span class="op">=</span>feed_forward(x3_norm, W1, b1, W2, b2)</span>
<span id="cb19-492"><a href="#cb19-492" aria-hidden="true" tabindex="-1"></a>    out<span class="op">=</span>x3<span class="op">+</span>ff_out</span>
<span id="cb19-493"><a href="#cb19-493" aria-hidden="true" tabindex="-1"></a>    <span class="co">#prepare the cache for backprop</span></span>
<span id="cb19-494"><a href="#cb19-494" aria-hidden="true" tabindex="-1"></a>    cache<span class="op">=</span>(x, x2, x3, x_norm_dec1, x2_norm, x3_norm, mean_dec1, var_dec1, mean_dec2, var_dec2, mean_dec3, var_dec3, masked_attn_w, cross_attn_w, ff_cache_dec)</span>
<span id="cb19-495"><a href="#cb19-495" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out, cache</span>
<span id="cb19-496"><a href="#cb19-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-497"><a href="#cb19-497" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb19-498"><a href="#cb19-498" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward and Backprop Through Model (Single Layer Encoder-Decoder)</span></span>
<span id="cb19-499"><a href="#cb19-499" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb19-500"><a href="#cb19-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-501"><a href="#cb19-501" aria-hidden="true" tabindex="-1"></a><span class="co">#at this point </span></span>
<span id="cb19-502"><a href="#cb19-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-503"><a href="#cb19-503" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_transformer(enc_in: np.ndarray, dec_in: np.ndarray,</span>
<span id="cb19-504"><a href="#cb19-504" aria-hidden="true" tabindex="-1"></a>                        intrinsic_value_weight_enc: np.ndarray, response_weight_enc: np.ndarray, information_weight_enc: np.ndarray, attention_weight_enc: np.ndarray, W1_enc: np.ndarray, b1_enc: np.ndarray, W2_enc: np.ndarray, b2_enc: np.ndarray,</span>
<span id="cb19-505"><a href="#cb19-505" aria-hidden="true" tabindex="-1"></a>                        intrinsic_value_weight_dec_masked: np.ndarray, response_weight_dec_masked: np.ndarray, information_weight_dec_masked: np.ndarray, attention_weight_dec_masked: np.ndarray,</span>
<span id="cb19-506"><a href="#cb19-506" aria-hidden="true" tabindex="-1"></a>                        intrinsic_value_weight_dec_cross: np.ndarray, response_weight_dec_cross: np.ndarray, information_weight_dec_cross: np.ndarray, attention_weight_dec_cross: np.ndarray,</span>
<span id="cb19-507"><a href="#cb19-507" aria-hidden="true" tabindex="-1"></a>                        W1_dec: np.ndarray, b1_dec: np.ndarray, W2_dec: np.ndarray, b2_dec: np.ndarray,</span>
<span id="cb19-508"><a href="#cb19-508" aria-hidden="true" tabindex="-1"></a>                        W_embed_out: np.ndarray, b_embed_out: np.ndarray,</span>
<span id="cb19-509"><a href="#cb19-509" aria-hidden="true" tabindex="-1"></a>                        src_mask: np.ndarray, tgt_mask: np.ndarray) <span class="op">-&gt;</span> Tuple[np.ndarray, Tuple]:</span>
<span id="cb19-510"><a href="#cb19-510" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-511"><a href="#cb19-511" aria-hidden="true" tabindex="-1"></a><span class="co">        We implement the forward pass for the transformer model. This is fairly straightforward and how we've defined it</span></span>
<span id="cb19-512"><a href="#cb19-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-513"><a href="#cb19-513" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-514"><a href="#cb19-514" aria-hidden="true" tabindex="-1"></a><span class="co">            enc_in: ndarray of input data to the encoder, shape (batch_size, src_len, d_model)</span></span>
<span id="cb19-515"><a href="#cb19-515" aria-hidden="true" tabindex="-1"></a><span class="co">            dec_in: ndarray of input data to the decoder, shape (batch_size, tgt_len, d_model)</span></span>
<span id="cb19-516"><a href="#cb19-516" aria-hidden="true" tabindex="-1"></a><span class="co">            intrinsic_value_weight_enc: projection matrix for the query in the encoder's self-attention</span></span>
<span id="cb19-517"><a href="#cb19-517" aria-hidden="true" tabindex="-1"></a><span class="co">            response_value_weight_enc: projection matrix for the key in the encoder's self-attention</span></span>
<span id="cb19-518"><a href="#cb19-518" aria-hidden="true" tabindex="-1"></a><span class="co">            information_value_weight_enc: projection matrix for the value in the encoder's self-attention</span></span>
<span id="cb19-519"><a href="#cb19-519" aria-hidden="true" tabindex="-1"></a><span class="co">            attention_value_weight_enc: projection matrix for the output in the encoder's self-attention</span></span>
<span id="cb19-520"><a href="#cb19-520" aria-hidden="true" tabindex="-1"></a><span class="co">            W1_enc, b1_enc, W2_enc, b2_enc: feed-forward network weights and biases for the encoder</span></span>
<span id="cb19-521"><a href="#cb19-521" aria-hidden="true" tabindex="-1"></a><span class="co">            intrinsic_value_weight_dec_masked: projection matrix for the query in the decoder's masked self-attention</span></span>
<span id="cb19-522"><a href="#cb19-522" aria-hidden="true" tabindex="-1"></a><span class="co">            response_value_weight_dec_masked: projection matrix for the key in the decoder's masked self-attention</span></span>
<span id="cb19-523"><a href="#cb19-523" aria-hidden="true" tabindex="-1"></a><span class="co">            information_value_weight_dec_masked: projection matrix for the value in the decoder's masked self-attention</span></span>
<span id="cb19-524"><a href="#cb19-524" aria-hidden="true" tabindex="-1"></a><span class="co">            attention_value_weight_dec_masked: projection matrix for the output in the decoder's masked self-attention</span></span>
<span id="cb19-525"><a href="#cb19-525" aria-hidden="true" tabindex="-1"></a><span class="co">            intrinsic_value_weight_dec_cross: projection matrix for the query in the decoder's cross-attention</span></span>
<span id="cb19-526"><a href="#cb19-526" aria-hidden="true" tabindex="-1"></a><span class="co">            response_value_weight_dec_cross: projection matrix for the key in the decoder's cross-attention</span></span>
<span id="cb19-527"><a href="#cb19-527" aria-hidden="true" tabindex="-1"></a><span class="co">            information_value_weight_dec_cross: projection matrix for the value in the decoder's cross-attention</span></span>
<span id="cb19-528"><a href="#cb19-528" aria-hidden="true" tabindex="-1"></a><span class="co">            attention_value_weight_dec_cross: projection matrix for the output in the decoder's cross-attention</span></span>
<span id="cb19-529"><a href="#cb19-529" aria-hidden="true" tabindex="-1"></a><span class="co">            W1_dec, b1_dec, W2_dec, b2_dec: feed-forward network weights and biases for the decoder</span></span>
<span id="cb19-530"><a href="#cb19-530" aria-hidden="true" tabindex="-1"></a><span class="co">            W_embed_out: projection matrix for mapping decoder output to the vocabulary space, shape (d_model, vocab_size)</span></span>
<span id="cb19-531"><a href="#cb19-531" aria-hidden="true" tabindex="-1"></a><span class="co">            b_embed_out: bias vector for mapping decoder output to the vocabulary space, shape (vocab_size,)</span></span>
<span id="cb19-532"><a href="#cb19-532" aria-hidden="true" tabindex="-1"></a><span class="co">            src_mask: ndarray mask for the encoder, shape (src_len, src_len)</span></span>
<span id="cb19-533"><a href="#cb19-533" aria-hidden="true" tabindex="-1"></a><span class="co">            tgt_mask: ndarray mask for the decoder, shape (tgt_len, tgt_len)</span></span>
<span id="cb19-534"><a href="#cb19-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-535"><a href="#cb19-535" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-536"><a href="#cb19-536" aria-hidden="true" tabindex="-1"></a><span class="co">            Tuple[np.ndarray, Tuple]:</span></span>
<span id="cb19-537"><a href="#cb19-537" aria-hidden="true" tabindex="-1"></a><span class="co">            - probs: output probabilities over the vocabulary, shape (batch_size, tgt_len, vocab_size)</span></span>
<span id="cb19-538"><a href="#cb19-538" aria-hidden="true" tabindex="-1"></a><span class="co">            - cache (Tuple): cached values for backpropagation, including:</span></span>
<span id="cb19-539"><a href="#cb19-539" aria-hidden="true" tabindex="-1"></a><span class="co">                - enc_out: ndarray of the encoder output, shape (batch_size, src_len, d_model)</span></span>
<span id="cb19-540"><a href="#cb19-540" aria-hidden="true" tabindex="-1"></a><span class="co">                - enc_cache: cached intermediate values from the encoder</span></span>
<span id="cb19-541"><a href="#cb19-541" aria-hidden="true" tabindex="-1"></a><span class="co">                - dec_out: ndarray of the decoder output, shape (batch_size, tgt_len, d_model)</span></span>
<span id="cb19-542"><a href="#cb19-542" aria-hidden="true" tabindex="-1"></a><span class="co">                - dec_cache: cached intermediate values from the decoder</span></span>
<span id="cb19-543"><a href="#cb19-543" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-544"><a href="#cb19-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-545"><a href="#cb19-545" aria-hidden="true" tabindex="-1"></a>    <span class="co">#this should be fairly straighforward by now, how it's implemented. just feed your weights in and go through the entire layer</span></span>
<span id="cb19-546"><a href="#cb19-546" aria-hidden="true" tabindex="-1"></a>    enc_out, enc_cache<span class="op">=</span>encoder_layer(enc_in, intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc, W1_enc, b1_enc, W2_enc, b2_enc)</span>
<span id="cb19-547"><a href="#cb19-547" aria-hidden="true" tabindex="-1"></a>    dec_out, dec_cache<span class="op">=</span>decoder_layer(dec_in, enc_out, intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,</span>
<span id="cb19-548"><a href="#cb19-548" aria-hidden="true" tabindex="-1"></a>                                       intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,</span>
<span id="cb19-549"><a href="#cb19-549" aria-hidden="true" tabindex="-1"></a>                                       W1_dec, b1_dec, W2_dec, b2_dec,</span>
<span id="cb19-550"><a href="#cb19-550" aria-hidden="true" tabindex="-1"></a>                                       mask<span class="op">=</span>tgt_mask)</span>
<span id="cb19-551"><a href="#cb19-551" aria-hidden="true" tabindex="-1"></a>    logits<span class="op">=</span>dec_out<span class="op">@</span>W_embed_out<span class="op">+</span>b_embed_out</span>
<span id="cb19-552"><a href="#cb19-552" aria-hidden="true" tabindex="-1"></a>    probs<span class="op">=</span>softmax(logits)</span>
<span id="cb19-553"><a href="#cb19-553" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> probs, (enc_out, enc_cache, dec_out, dec_cache)</span>
<span id="cb19-554"><a href="#cb19-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-555"><a href="#cb19-555" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward_transformer(dprobs: np.ndarray,</span>
<span id="cb19-556"><a href="#cb19-556" aria-hidden="true" tabindex="-1"></a>    enc_in: np.ndarray,</span>
<span id="cb19-557"><a href="#cb19-557" aria-hidden="true" tabindex="-1"></a>    dec_in: np.ndarray,</span>
<span id="cb19-558"><a href="#cb19-558" aria-hidden="true" tabindex="-1"></a>    enc_out: np.ndarray,</span>
<span id="cb19-559"><a href="#cb19-559" aria-hidden="true" tabindex="-1"></a>    enc_cache: Tuple,</span>
<span id="cb19-560"><a href="#cb19-560" aria-hidden="true" tabindex="-1"></a>    dec_out: np.ndarray,</span>
<span id="cb19-561"><a href="#cb19-561" aria-hidden="true" tabindex="-1"></a>    dec_cache: Tuple,</span>
<span id="cb19-562"><a href="#cb19-562" aria-hidden="true" tabindex="-1"></a>    intrinsic_value_weight_enc: np.ndarray,</span>
<span id="cb19-563"><a href="#cb19-563" aria-hidden="true" tabindex="-1"></a>    response_weight_enc: np.ndarray,</span>
<span id="cb19-564"><a href="#cb19-564" aria-hidden="true" tabindex="-1"></a>    information_weight_enc: np.ndarray,</span>
<span id="cb19-565"><a href="#cb19-565" aria-hidden="true" tabindex="-1"></a>    attention_weight_enc: np.ndarray,</span>
<span id="cb19-566"><a href="#cb19-566" aria-hidden="true" tabindex="-1"></a>    W1_enc: np.ndarray,</span>
<span id="cb19-567"><a href="#cb19-567" aria-hidden="true" tabindex="-1"></a>    b1_enc: np.ndarray,</span>
<span id="cb19-568"><a href="#cb19-568" aria-hidden="true" tabindex="-1"></a>    W2_enc: np.ndarray,</span>
<span id="cb19-569"><a href="#cb19-569" aria-hidden="true" tabindex="-1"></a>    b2_enc: np.ndarray,</span>
<span id="cb19-570"><a href="#cb19-570" aria-hidden="true" tabindex="-1"></a>    intrinsic_value_weight_dec_masked: np.ndarray,</span>
<span id="cb19-571"><a href="#cb19-571" aria-hidden="true" tabindex="-1"></a>    response_weight_dec_masked: np.ndarray,</span>
<span id="cb19-572"><a href="#cb19-572" aria-hidden="true" tabindex="-1"></a>    information_weight_dec_masked: np.ndarray,</span>
<span id="cb19-573"><a href="#cb19-573" aria-hidden="true" tabindex="-1"></a>    attention_weight_dec_masked: np.ndarray,</span>
<span id="cb19-574"><a href="#cb19-574" aria-hidden="true" tabindex="-1"></a>    intrinsic_value_weight_dec_cross: np.ndarray,</span>
<span id="cb19-575"><a href="#cb19-575" aria-hidden="true" tabindex="-1"></a>    response_weight_dec_cross: np.ndarray,</span>
<span id="cb19-576"><a href="#cb19-576" aria-hidden="true" tabindex="-1"></a>    information_weight_dec_cross: np.ndarray,</span>
<span id="cb19-577"><a href="#cb19-577" aria-hidden="true" tabindex="-1"></a>    attention_weight_dec_cross: np.ndarray,</span>
<span id="cb19-578"><a href="#cb19-578" aria-hidden="true" tabindex="-1"></a>    W1_dec: np.ndarray,</span>
<span id="cb19-579"><a href="#cb19-579" aria-hidden="true" tabindex="-1"></a>    b1_dec: np.ndarray,</span>
<span id="cb19-580"><a href="#cb19-580" aria-hidden="true" tabindex="-1"></a>    W2_dec: np.ndarray,</span>
<span id="cb19-581"><a href="#cb19-581" aria-hidden="true" tabindex="-1"></a>    b2_dec: np.ndarray,</span>
<span id="cb19-582"><a href="#cb19-582" aria-hidden="true" tabindex="-1"></a>    W_embed_out: np.ndarray,</span>
<span id="cb19-583"><a href="#cb19-583" aria-hidden="true" tabindex="-1"></a>    b_embed_out: np.ndarray,</span>
<span id="cb19-584"><a href="#cb19-584" aria-hidden="true" tabindex="-1"></a>    src_mask: np.ndarray,</span>
<span id="cb19-585"><a href="#cb19-585" aria-hidden="true" tabindex="-1"></a>    tgt_mask: np.ndarray) <span class="op">-&gt;</span> Tuple[Dict[<span class="bu">str</span>, np.ndarray], np.ndarray]:</span>
<span id="cb19-586"><a href="#cb19-586" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-587"><a href="#cb19-587" aria-hidden="true" tabindex="-1"></a><span class="co">        This is possibly the hardest part in the code. This is complete backpropagation for the transformer, through all layers.</span></span>
<span id="cb19-588"><a href="#cb19-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-589"><a href="#cb19-589" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb19-590"><a href="#cb19-590" aria-hidden="true" tabindex="-1"></a><span class="co">            dprobs: ndarray of gradients with respect to output probabilities, shape (batch_size, tgt_len, vocab_size)</span></span>
<span id="cb19-591"><a href="#cb19-591" aria-hidden="true" tabindex="-1"></a><span class="co">            enc_in: ndarray of input data to the encoder, shape (batch_size, src_len, d_model)</span></span>
<span id="cb19-592"><a href="#cb19-592" aria-hidden="true" tabindex="-1"></a><span class="co">            dec_in: ndarray of input data to the decoder, shape (batch_size, tgt_len, d_model)</span></span>
<span id="cb19-593"><a href="#cb19-593" aria-hidden="true" tabindex="-1"></a><span class="co">            enc_out: ndarray of encoder outputs, shape (batch_size, src_len, d_model)</span></span>
<span id="cb19-594"><a href="#cb19-594" aria-hidden="true" tabindex="-1"></a><span class="co">            enc_cache: cached values from the encoder forward pass</span></span>
<span id="cb19-595"><a href="#cb19-595" aria-hidden="true" tabindex="-1"></a><span class="co">            dec_out: ndarray of decoder outputs, shape (batch_size, tgt_len, d_model)</span></span>
<span id="cb19-596"><a href="#cb19-596" aria-hidden="true" tabindex="-1"></a><span class="co">            dec_cache: cached values from the decoder forward pass</span></span>
<span id="cb19-597"><a href="#cb19-597" aria-hidden="true" tabindex="-1"></a><span class="co">            intrinsic_value_weight_enc: projection matrix for the query in the encoder's self-attention</span></span>
<span id="cb19-598"><a href="#cb19-598" aria-hidden="true" tabindex="-1"></a><span class="co">            response_weight_enc: projection matrix for the key in the encoder's self-attention</span></span>
<span id="cb19-599"><a href="#cb19-599" aria-hidden="true" tabindex="-1"></a><span class="co">            information_weight_enc: projection matrix for the value in the encoder's self-attention</span></span>
<span id="cb19-600"><a href="#cb19-600" aria-hidden="true" tabindex="-1"></a><span class="co">            attention_weight_enc: projection matrix for the output in the encoder's self-attention</span></span>
<span id="cb19-601"><a href="#cb19-601" aria-hidden="true" tabindex="-1"></a><span class="co">            W1_enc, b1_enc, W2_enc, b2_enc: feed-forward network weights and biases for the encoder</span></span>
<span id="cb19-602"><a href="#cb19-602" aria-hidden="true" tabindex="-1"></a><span class="co">            intrinsic_value_weight_dec_masked: projection matrix for the query in the decoder's masked self-attention</span></span>
<span id="cb19-603"><a href="#cb19-603" aria-hidden="true" tabindex="-1"></a><span class="co">            response_weight_dec_masked: projection matrix for the key in the decoder's masked self-attention</span></span>
<span id="cb19-604"><a href="#cb19-604" aria-hidden="true" tabindex="-1"></a><span class="co">            information_weight_dec_masked: projection matrix for the value in the decoder's masked self-attention</span></span>
<span id="cb19-605"><a href="#cb19-605" aria-hidden="true" tabindex="-1"></a><span class="co">            attention_weight_dec_masked: projection matrix for the output in the decoder's masked self-attention</span></span>
<span id="cb19-606"><a href="#cb19-606" aria-hidden="true" tabindex="-1"></a><span class="co">            intrinsic_value_weight_dec_cross: projection matrix for the query in the decoder's cross-attention</span></span>
<span id="cb19-607"><a href="#cb19-607" aria-hidden="true" tabindex="-1"></a><span class="co">            response_weight_dec_cross: projection matrix for the key in the decoder's cross-attention</span></span>
<span id="cb19-608"><a href="#cb19-608" aria-hidden="true" tabindex="-1"></a><span class="co">            information_weight_dec_cross: projection matrix for the value in the decoder's cross-attention</span></span>
<span id="cb19-609"><a href="#cb19-609" aria-hidden="true" tabindex="-1"></a><span class="co">            attention_weight_dec_cross: projection matrix for the output in the decoder's cross-attention</span></span>
<span id="cb19-610"><a href="#cb19-610" aria-hidden="true" tabindex="-1"></a><span class="co">            W1_dec, b1_dec, W2_dec, b2_dec: feed-forward network weights and biases for the decoder</span></span>
<span id="cb19-611"><a href="#cb19-611" aria-hidden="true" tabindex="-1"></a><span class="co">            W_embed_out: projection matrix for mapping decoder outputs to the vocabulary, shape (d_model, vocab_size)</span></span>
<span id="cb19-612"><a href="#cb19-612" aria-hidden="true" tabindex="-1"></a><span class="co">            b_embed_out: bias vector for mapping decoder outputs to the vocabulary, shape (vocab_size,)</span></span>
<span id="cb19-613"><a href="#cb19-613" aria-hidden="true" tabindex="-1"></a><span class="co">            src_mask: mask for the source sequence, shape (src_len, src_len)</span></span>
<span id="cb19-614"><a href="#cb19-614" aria-hidden="true" tabindex="-1"></a><span class="co">            tgt_mask: mask for the target sequence, shape (tgt_len, tgt_len)</span></span>
<span id="cb19-615"><a href="#cb19-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-616"><a href="#cb19-616" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb19-617"><a href="#cb19-617" aria-hidden="true" tabindex="-1"></a><span class="co">            Tuple[Dict[str, np.ndarray], np.ndarray]:</span></span>
<span id="cb19-618"><a href="#cb19-618" aria-hidden="true" tabindex="-1"></a><span class="co">            - grads: dictionary containing gradients for all trainable weights and biases, including:</span></span>
<span id="cb19-619"><a href="#cb19-619" aria-hidden="true" tabindex="-1"></a><span class="co">                - intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc: gradients for encoder self-attention weights</span></span>
<span id="cb19-620"><a href="#cb19-620" aria-hidden="true" tabindex="-1"></a><span class="co">                - W1_enc, b1_enc, W2_enc, b2_enc: gradients for encoder feed-forward network</span></span>
<span id="cb19-621"><a href="#cb19-621" aria-hidden="true" tabindex="-1"></a><span class="co">                - intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked: gradients for decoder masked self-attention weights</span></span>
<span id="cb19-622"><a href="#cb19-622" aria-hidden="true" tabindex="-1"></a><span class="co">                - intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross: gradients for decoder cross-attention weights</span></span>
<span id="cb19-623"><a href="#cb19-623" aria-hidden="true" tabindex="-1"></a><span class="co">                - W1_dec, b1_dec, W2_dec, b2_dec: gradients for decoder feed-forward network</span></span>
<span id="cb19-624"><a href="#cb19-624" aria-hidden="true" tabindex="-1"></a><span class="co">                - W_embed_out, b_embed_out: gradients for output projection layer</span></span>
<span id="cb19-625"><a href="#cb19-625" aria-hidden="true" tabindex="-1"></a><span class="co">            - dx_enc1: gradient with respect to the encoder input, shape (batch_size, src_len, d_model)</span></span>
<span id="cb19-626"><a href="#cb19-626" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-627"><a href="#cb19-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-628"><a href="#cb19-628" aria-hidden="true" tabindex="-1"></a>    <span class="co">#this is how backprop is implemented</span></span>
<span id="cb19-629"><a href="#cb19-629" aria-hidden="true" tabindex="-1"></a>    batch, length, d_model<span class="op">=</span>dec_out.shape  <span class="co">#first, extract the output of the decoder. what we are really interested in is the model dimension</span></span>
<span id="cb19-630"><a href="#cb19-630" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span>W_embed_out.shape[<span class="dv">1</span>]  <span class="co">#and get the vocabulary size</span></span>
<span id="cb19-631"><a href="#cb19-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-632"><a href="#cb19-632" aria-hidden="true" tabindex="-1"></a>    <span class="co">#start: backprop through the final layer</span></span>
<span id="cb19-633"><a href="#cb19-633" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-634"><a href="#cb19-634" aria-hidden="true" tabindex="-1"></a>    dW_embed_out<span class="op">=</span>dec_out.reshape(<span class="op">-</span><span class="dv">1</span>, d_model).T<span class="op">@</span>dprobs.reshape(<span class="op">-</span><span class="dv">1</span>, vocab_size)  <span class="co"># Shape: (d_model, vocab_size)</span></span>
<span id="cb19-635"><a href="#cb19-635" aria-hidden="true" tabindex="-1"></a>    db_embed_out<span class="op">=</span>dprobs.<span class="bu">sum</span>(axis<span class="op">=</span>(<span class="dv">0</span>,<span class="dv">1</span>))  <span class="co"># Shape: (vocab_size,)</span></span>
<span id="cb19-636"><a href="#cb19-636" aria-hidden="true" tabindex="-1"></a>    d_dec_out<span class="op">=</span>dprobs <span class="op">@</span> W_embed_out.T  <span class="co"># Gradient w.r.t decoder output, shape (batch, tgt_len, d_model)</span></span>
<span id="cb19-637"><a href="#cb19-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-638"><a href="#cb19-638" aria-hidden="true" tabindex="-1"></a>    <span class="co">#get all values from the decoder - this is called 'unpacking'</span></span>
<span id="cb19-639"><a href="#cb19-639" aria-hidden="true" tabindex="-1"></a>    (x, x2, x3,</span>
<span id="cb19-640"><a href="#cb19-640" aria-hidden="true" tabindex="-1"></a>     x_norm_dec1, x2_norm_dec, x3_norm_dec,</span>
<span id="cb19-641"><a href="#cb19-641" aria-hidden="true" tabindex="-1"></a>     mean_dec1, var_dec1, mean_dec2, var_dec2, mean_dec3, var_dec3,</span>
<span id="cb19-642"><a href="#cb19-642" aria-hidden="true" tabindex="-1"></a>     masked_attn_w, cross_attn_w, ff_cache_dec)<span class="op">=</span>dec_cache</span>
<span id="cb19-643"><a href="#cb19-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-644"><a href="#cb19-644" aria-hidden="true" tabindex="-1"></a>    <span class="co">#backprop through the feedforward neural network for the decoder</span></span>
<span id="cb19-645"><a href="#cb19-645" aria-hidden="true" tabindex="-1"></a>    d_x3_ff<span class="op">=</span>d_dec_out</span>
<span id="cb19-646"><a href="#cb19-646" aria-hidden="true" tabindex="-1"></a>    d_x3_ff, dW1_dec, db1_dec, dW2_dec, db2_dec<span class="op">=</span>feed_forward_backprop(d_x3_ff, x3_norm_dec, W1_dec, b1_dec, W2_dec, b2_dec, ff_cache_dec)</span>
<span id="cb19-647"><a href="#cb19-647" aria-hidden="true" tabindex="-1"></a>    dx3_norm<span class="op">=</span>d_x3_ff</span>
<span id="cb19-648"><a href="#cb19-648" aria-hidden="true" tabindex="-1"></a>    dx3<span class="op">=</span>layer_norm_backprop(dx3_norm, x3_norm_dec, mean_dec3, var_dec3)</span>
<span id="cb19-649"><a href="#cb19-649" aria-hidden="true" tabindex="-1"></a>    d_x3_skip<span class="op">=</span>dx3</span>
<span id="cb19-650"><a href="#cb19-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-651"><a href="#cb19-651" aria-hidden="true" tabindex="-1"></a>    <span class="co">#backprop through crossattention</span></span>
<span id="cb19-652"><a href="#cb19-652" aria-hidden="true" tabindex="-1"></a>    d_x2_cross<span class="op">=</span>d_x3_skip</span>
<span id="cb19-653"><a href="#cb19-653" aria-hidden="true" tabindex="-1"></a>    dx_cross_Q, dx_cross_K, dx_cross_V, dintrinsic_value_weight_dec_cross_, dresponse_weight_dec_cross_, dinformation_weight_dec_cross_, dattention_weight_dec_cross_ <span class="op">=</span> mha_backprop(</span>
<span id="cb19-654"><a href="#cb19-654" aria-hidden="true" tabindex="-1"></a>        d_x2_cross, x2_norm_dec, enc_out, enc_out,</span>
<span id="cb19-655"><a href="#cb19-655" aria-hidden="true" tabindex="-1"></a>        intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,</span>
<span id="cb19-656"><a href="#cb19-656" aria-hidden="true" tabindex="-1"></a>        cross_attn_w, num_heads<span class="op">=</span><span class="dv">2</span></span>
<span id="cb19-657"><a href="#cb19-657" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-658"><a href="#cb19-658" aria-hidden="true" tabindex="-1"></a>    d_x2_skip<span class="op">=</span>dx_cross_Q</span>
<span id="cb19-659"><a href="#cb19-659" aria-hidden="true" tabindex="-1"></a>    d_enc_out<span class="op">=</span>dx_cross_K<span class="op">+</span>dx_cross_V</span>
<span id="cb19-660"><a href="#cb19-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-661"><a href="#cb19-661" aria-hidden="true" tabindex="-1"></a>    <span class="co">#backprop through masked selfattention</span></span>
<span id="cb19-662"><a href="#cb19-662" aria-hidden="true" tabindex="-1"></a>    d_x2_masked<span class="op">=</span>d_x2_skip</span>
<span id="cb19-663"><a href="#cb19-663" aria-hidden="true" tabindex="-1"></a>    dx_masked_Q, dx_masked_K, dx_masked_V, dintrinsic_value_weight_dec_masked_, dresponse_weight_dec_masked_, dinformation_weight_dec_masked_, dattention_weight_dec_masked_ <span class="op">=</span> mha_backprop(</span>
<span id="cb19-664"><a href="#cb19-664" aria-hidden="true" tabindex="-1"></a>        d_x2_masked, x_norm_dec1, x_norm_dec1, x_norm_dec1,</span>
<span id="cb19-665"><a href="#cb19-665" aria-hidden="true" tabindex="-1"></a>        intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,</span>
<span id="cb19-666"><a href="#cb19-666" aria-hidden="true" tabindex="-1"></a>        masked_attn_w, num_heads<span class="op">=</span><span class="dv">2</span>, mask<span class="op">=</span>tgt_mask</span>
<span id="cb19-667"><a href="#cb19-667" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-668"><a href="#cb19-668" aria-hidden="true" tabindex="-1"></a>    dx_norm_dec1<span class="op">=</span>dx_masked_Q<span class="op">+</span>dx_masked_K<span class="op">+</span>dx_masked_V</span>
<span id="cb19-669"><a href="#cb19-669" aria-hidden="true" tabindex="-1"></a>    dx_dec1<span class="op">=</span>layer_norm_backprop(dx_norm_dec1, x_norm_dec1, mean_dec1, var_dec1)</span>
<span id="cb19-670"><a href="#cb19-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-671"><a href="#cb19-671" aria-hidden="true" tabindex="-1"></a>    dx2_norm<span class="op">=</span>d_x2_cross</span>
<span id="cb19-672"><a href="#cb19-672" aria-hidden="true" tabindex="-1"></a>    dx2<span class="op">=</span>layer_norm_backprop(dx2_norm, x2_norm_dec, mean_dec2, var_dec2)</span>
<span id="cb19-673"><a href="#cb19-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-674"><a href="#cb19-674" aria-hidden="true" tabindex="-1"></a>    <span class="co">#combine different layers' gradients</span></span>
<span id="cb19-675"><a href="#cb19-675" aria-hidden="true" tabindex="-1"></a>    dx<span class="op">=</span>dx_dec1<span class="op">+</span>dx2</span>
<span id="cb19-676"><a href="#cb19-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-677"><a href="#cb19-677" aria-hidden="true" tabindex="-1"></a>    <span class="co">#unpack encoder values</span></span>
<span id="cb19-678"><a href="#cb19-678" aria-hidden="true" tabindex="-1"></a>    (enc_x, enc_x2, enc_x_norm, enc_x2_norm, enc_mean1, enc_var1, enc_mean2, enc_var2, enc_attn_w, enc_ff_cache)<span class="op">=</span>enc_cache</span>
<span id="cb19-679"><a href="#cb19-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-680"><a href="#cb19-680" aria-hidden="true" tabindex="-1"></a>    <span class="co">#backprop through the encoder</span></span>
<span id="cb19-681"><a href="#cb19-681" aria-hidden="true" tabindex="-1"></a>    d_enc<span class="op">=</span>d_enc_out</span>
<span id="cb19-682"><a href="#cb19-682" aria-hidden="true" tabindex="-1"></a>    d_enc_ff, dW1_enc, db1_enc, dW2_enc, db2_enc<span class="op">=</span>feed_forward_backprop(d_enc, enc_x2_norm, W1_enc, b1_enc, W2_enc, b2_enc, enc_ff_cache)</span>
<span id="cb19-683"><a href="#cb19-683" aria-hidden="true" tabindex="-1"></a>    d_enc2_norm<span class="op">=</span>d_enc_ff</span>
<span id="cb19-684"><a href="#cb19-684" aria-hidden="true" tabindex="-1"></a>    d_enc2<span class="op">=</span>layer_norm_backprop(d_enc2_norm, enc_x2_norm, enc_mean2, enc_var2)</span>
<span id="cb19-685"><a href="#cb19-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-686"><a href="#cb19-686" aria-hidden="true" tabindex="-1"></a>    <span class="co">#backprop through encoder's attention</span></span>
<span id="cb19-687"><a href="#cb19-687" aria-hidden="true" tabindex="-1"></a>    dx_enc_Q, dx_enc_K, dx_enc_V, dintrinsic_value_weight_enc_, dresponse_weight_enc_, dinformation_weight_enc_, dattention_weight_enc_<span class="op">=</span>mha_backprop(</span>
<span id="cb19-688"><a href="#cb19-688" aria-hidden="true" tabindex="-1"></a>        d_enc2, enc_x_norm, enc_x_norm, enc_x_norm,</span>
<span id="cb19-689"><a href="#cb19-689" aria-hidden="true" tabindex="-1"></a>        intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc,</span>
<span id="cb19-690"><a href="#cb19-690" aria-hidden="true" tabindex="-1"></a>        enc_attn_w, num_heads<span class="op">=</span><span class="dv">2</span></span>
<span id="cb19-691"><a href="#cb19-691" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-692"><a href="#cb19-692" aria-hidden="true" tabindex="-1"></a>    d_enc_norm1<span class="op">=</span>dx_enc_Q<span class="op">+</span>dx_enc_K<span class="op">+</span>dx_enc_V</span>
<span id="cb19-693"><a href="#cb19-693" aria-hidden="true" tabindex="-1"></a>    dx_enc1<span class="op">=</span>layer_norm_backprop(d_enc_norm1, enc_x_norm, enc_mean1, enc_var1)</span>
<span id="cb19-694"><a href="#cb19-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-695"><a href="#cb19-695" aria-hidden="true" tabindex="-1"></a>    <span class="co">#combine all gradients in a dictionary </span></span>
<span id="cb19-696"><a href="#cb19-696" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> {</span>
<span id="cb19-697"><a href="#cb19-697" aria-hidden="true" tabindex="-1"></a>        <span class="st">'intrinsic_value_weight_enc'</span>: dintrinsic_value_weight_enc_, <span class="st">'response_weight_enc'</span>: dresponse_weight_enc_, <span class="st">'information_weight_enc'</span>: dinformation_weight_enc_, <span class="st">'attention_weight_enc'</span>: dattention_weight_enc_,</span>
<span id="cb19-698"><a href="#cb19-698" aria-hidden="true" tabindex="-1"></a>        <span class="st">'W1_enc'</span>: dW1_enc, <span class="st">'b1_enc'</span>: db1_enc, <span class="st">'W2_enc'</span>: dW2_enc, <span class="st">'b2_enc'</span>: db2_enc,</span>
<span id="cb19-699"><a href="#cb19-699" aria-hidden="true" tabindex="-1"></a>        <span class="st">'intrinsic_value_weight_dec_masked'</span>: dintrinsic_value_weight_dec_masked_, <span class="st">'response_weight_dec_masked'</span>: dresponse_weight_dec_masked_, <span class="st">'information_weight_dec_masked'</span>: dinformation_weight_dec_masked_, <span class="st">'attention_weight_dec_masked'</span>: dattention_weight_dec_masked_,</span>
<span id="cb19-700"><a href="#cb19-700" aria-hidden="true" tabindex="-1"></a>        <span class="st">'intrinsic_value_weight_dec_cross'</span>: dintrinsic_value_weight_dec_cross_, <span class="st">'response_weight_dec_cross'</span>: dresponse_weight_dec_cross_, <span class="st">'information_weight_dec_cross'</span>: dinformation_weight_dec_cross_, <span class="st">'attention_weight_dec_cross'</span>: dattention_weight_dec_cross_,</span>
<span id="cb19-701"><a href="#cb19-701" aria-hidden="true" tabindex="-1"></a>        <span class="st">'W1_dec'</span>: dW1_dec, <span class="st">'b1_dec'</span>: db1_dec, <span class="st">'W2_dec'</span>: dW2_dec, <span class="st">'b2_dec'</span>: db2_dec,</span>
<span id="cb19-702"><a href="#cb19-702" aria-hidden="true" tabindex="-1"></a>        <span class="st">'W_embed_out'</span>: dW_embed_out, <span class="st">'b_embed_out'</span>: db_embed_out</span>
<span id="cb19-703"><a href="#cb19-703" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb19-704"><a href="#cb19-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-705"><a href="#cb19-705" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> grads, dx_enc1</span>
<span id="cb19-706"><a href="#cb19-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-707"><a href="#cb19-707" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb19-708"><a href="#cb19-708" aria-hidden="true" tabindex="-1"></a><span class="co"># Main Training Setup</span></span>
<span id="cb19-709"><a href="#cb19-709" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb19-710"><a href="#cb19-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-711"><a href="#cb19-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-712"><a href="#cb19-712" aria-hidden="true" tabindex="-1"></a><span class="co">#we are finally done. let's now train the actual transformer</span></span>
<span id="cb19-713"><a href="#cb19-713" aria-hidden="true" tabindex="-1"></a>english_sentences<span class="op">=</span>[</span>
<span id="cb19-714"><a href="#cb19-714" aria-hidden="true" tabindex="-1"></a>    <span class="st">"My rabbit likes bananas"</span></span>
<span id="cb19-715"><a href="#cb19-715" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb19-716"><a href="#cb19-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-717"><a href="#cb19-717" aria-hidden="true" tabindex="-1"></a>italian_sentences<span class="op">=</span>[</span>
<span id="cb19-718"><a href="#cb19-718" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Al mio coniglio piacciono le banane"</span>,</span>
<span id="cb19-719"><a href="#cb19-719" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb19-720"><a href="#cb19-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-721"><a href="#cb19-721" aria-hidden="true" tabindex="-1"></a>eng_tokens<span class="op">=</span>[whitespace_tokenizer(s) <span class="cf">for</span> s <span class="kw">in</span> english_sentences]</span>
<span id="cb19-722"><a href="#cb19-722" aria-hidden="true" tabindex="-1"></a>for_tokens<span class="op">=</span>[whitespace_tokenizer(s) <span class="cf">for</span> s <span class="kw">in</span> italian_sentences]</span>
<span id="cb19-723"><a href="#cb19-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-724"><a href="#cb19-724" aria-hidden="true" tabindex="-1"></a>eng_word2idx, eng_idx2word<span class="op">=</span>build_vocab(eng_tokens)</span>
<span id="cb19-725"><a href="#cb19-725" aria-hidden="true" tabindex="-1"></a>for_word2idx, for_idx2word<span class="op">=</span>build_vocab(for_tokens)</span>
<span id="cb19-726"><a href="#cb19-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-727"><a href="#cb19-727" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode(tokens, w2i):</span>
<span id="cb19-728"><a href="#cb19-728" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [w2i[t] <span class="cf">for</span> t <span class="kw">in</span> tokens]</span>
<span id="cb19-729"><a href="#cb19-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-730"><a href="#cb19-730" aria-hidden="true" tabindex="-1"></a>eng_encoded<span class="op">=</span>[encode(t, eng_word2idx) <span class="cf">for</span> t <span class="kw">in</span> eng_tokens]</span>
<span id="cb19-731"><a href="#cb19-731" aria-hidden="true" tabindex="-1"></a>for_encoded<span class="op">=</span>[encode(t, for_word2idx) <span class="cf">for</span> t <span class="kw">in</span> for_tokens]</span>
<span id="cb19-732"><a href="#cb19-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-733"><a href="#cb19-733" aria-hidden="true" tabindex="-1"></a>max_eng_len<span class="op">=</span><span class="bu">max</span>(<span class="bu">len</span>(x) <span class="cf">for</span> x <span class="kw">in</span> eng_encoded)</span>
<span id="cb19-734"><a href="#cb19-734" aria-hidden="true" tabindex="-1"></a>max_for_len<span class="op">=</span><span class="bu">max</span>(<span class="bu">len</span>(x) <span class="cf">for</span> x <span class="kw">in</span> for_encoded)</span>
<span id="cb19-735"><a href="#cb19-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-736"><a href="#cb19-736" aria-hidden="true" tabindex="-1"></a><span class="co"># Add start/end tokens</span></span>
<span id="cb19-737"><a href="#cb19-737" aria-hidden="true" tabindex="-1"></a>start_token<span class="op">=</span><span class="bu">len</span>(for_word2idx)</span>
<span id="cb19-738"><a href="#cb19-738" aria-hidden="true" tabindex="-1"></a>end_token<span class="op">=</span><span class="bu">len</span>(for_word2idx)<span class="op">+</span><span class="dv">1</span></span>
<span id="cb19-739"><a href="#cb19-739" aria-hidden="true" tabindex="-1"></a>for_word2idx[<span class="st">"&lt;start&gt;"</span>]<span class="op">=</span>start_token</span>
<span id="cb19-740"><a href="#cb19-740" aria-hidden="true" tabindex="-1"></a>for_word2idx[<span class="st">"&lt;end&gt;"</span>]<span class="op">=</span>end_token</span>
<span id="cb19-741"><a href="#cb19-741" aria-hidden="true" tabindex="-1"></a>for_idx2word[start_token]<span class="op">=</span><span class="st">"&lt;start&gt;"</span></span>
<span id="cb19-742"><a href="#cb19-742" aria-hidden="true" tabindex="-1"></a>for_idx2word[end_token]<span class="op">=</span><span class="st">"&lt;end&gt;"</span></span>
<span id="cb19-743"><a href="#cb19-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-744"><a href="#cb19-744" aria-hidden="true" tabindex="-1"></a>for_idx2word<span class="op">=</span>{idx: token <span class="cf">for</span> token, idx <span class="kw">in</span> for_word2idx.items()}</span>
<span id="cb19-745"><a href="#cb19-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-746"><a href="#cb19-746" aria-hidden="true" tabindex="-1"></a>for_encoded_input<span class="op">=</span>[[start_token]<span class="op">+</span>seq <span class="cf">for</span> seq <span class="kw">in</span> for_encoded]</span>
<span id="cb19-747"><a href="#cb19-747" aria-hidden="true" tabindex="-1"></a>for_encoded_target<span class="op">=</span>[seq<span class="op">+</span>[end_token] <span class="cf">for</span> seq <span class="kw">in</span> for_encoded]</span>
<span id="cb19-748"><a href="#cb19-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-749"><a href="#cb19-749" aria-hidden="true" tabindex="-1"></a>max_for_len_inp<span class="op">=</span><span class="bu">max</span>(<span class="bu">len</span>(s) <span class="cf">for</span> s <span class="kw">in</span> for_encoded_input)</span>
<span id="cb19-750"><a href="#cb19-750" aria-hidden="true" tabindex="-1"></a>max_for_len_tgt<span class="op">=</span><span class="bu">max</span>(<span class="bu">len</span>(s) <span class="cf">for</span> s <span class="kw">in</span> for_encoded_target)</span>
<span id="cb19-751"><a href="#cb19-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-752"><a href="#cb19-752" aria-hidden="true" tabindex="-1"></a>eng_padded<span class="op">=</span>pad_sequences(eng_encoded, max_length<span class="op">=</span>max_eng_len, pad_value<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-753"><a href="#cb19-753" aria-hidden="true" tabindex="-1"></a>for_inp_padded<span class="op">=</span>pad_sequences(for_encoded_input, max_length<span class="op">=</span>max_for_len_inp, pad_value<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-754"><a href="#cb19-754" aria-hidden="true" tabindex="-1"></a>for_tgt_padded<span class="op">=</span>pad_sequences(for_encoded_target, max_length<span class="op">=</span>max_for_len_tgt, pad_value<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-755"><a href="#cb19-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-756"><a href="#cb19-756" aria-hidden="true" tabindex="-1"></a>batch_size<span class="op">=</span><span class="bu">len</span>(eng_padded)</span>
<span id="cb19-757"><a href="#cb19-757" aria-hidden="true" tabindex="-1"></a>src_len<span class="op">=</span>eng_padded.shape[<span class="dv">1</span>]</span>
<span id="cb19-758"><a href="#cb19-758" aria-hidden="true" tabindex="-1"></a>tgt_len<span class="op">=</span>for_inp_padded.shape[<span class="dv">1</span>]</span>
<span id="cb19-759"><a href="#cb19-759" aria-hidden="true" tabindex="-1"></a>vocab_size_src<span class="op">=</span><span class="bu">len</span>(eng_word2idx)</span>
<span id="cb19-760"><a href="#cb19-760" aria-hidden="true" tabindex="-1"></a>vocab_size_tgt<span class="op">=</span><span class="bu">len</span>(for_word2idx)</span>
<span id="cb19-761"><a href="#cb19-761" aria-hidden="true" tabindex="-1"></a>d_model<span class="op">=</span><span class="dv">16</span> <span class="co">#this is arbitrary. many AI companies sell access to their embeddings and dimensions</span></span>
<span id="cb19-762"><a href="#cb19-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-763"><a href="#cb19-763" aria-hidden="true" tabindex="-1"></a>src_embeddings<span class="op">=</span>np.random.randn(vocab_size_src, d_model)<span class="op">*</span><span class="fl">0.01</span> <span class="co">#i chose 0.01 for a balance between numerical stability and demonstrating the power of transformers</span></span>
<span id="cb19-764"><a href="#cb19-764" aria-hidden="true" tabindex="-1"></a>tgt_embeddings<span class="op">=</span>np.random.randn(vocab_size_tgt, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-765"><a href="#cb19-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-766"><a href="#cb19-766" aria-hidden="true" tabindex="-1"></a><span class="co">#we haven't actually defined the embedding function- let's embed our vocabulary</span></span>
<span id="cb19-767"><a href="#cb19-767" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> embed(x, emb):</span>
<span id="cb19-768"><a href="#cb19-768" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> emb[x] </span>
<span id="cb19-769"><a href="#cb19-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-770"><a href="#cb19-770" aria-hidden="true" tabindex="-1"></a><span class="co">#this next section is just defining random matrices</span></span>
<span id="cb19-771"><a href="#cb19-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-772"><a href="#cb19-772" aria-hidden="true" tabindex="-1"></a>W_embed_out<span class="op">=</span>np.random.randn(d_model, vocab_size_tgt)<span class="op">*</span><span class="fl">0.01</span> <span class="co">#define this random matrix for embedding</span></span>
<span id="cb19-773"><a href="#cb19-773" aria-hidden="true" tabindex="-1"></a>b_embed_out<span class="op">=</span>np.zeros(vocab_size_tgt) <span class="co">#and the bias of the weights as well</span></span>
<span id="cb19-774"><a href="#cb19-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-775"><a href="#cb19-775" aria-hidden="true" tabindex="-1"></a>intrinsic_value_weight_enc<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span> </span>
<span id="cb19-776"><a href="#cb19-776" aria-hidden="true" tabindex="-1"></a>response_weight_enc<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-777"><a href="#cb19-777" aria-hidden="true" tabindex="-1"></a>information_weight_enc<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-778"><a href="#cb19-778" aria-hidden="true" tabindex="-1"></a>attention_weight_enc<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-779"><a href="#cb19-779" aria-hidden="true" tabindex="-1"></a>W1_enc<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-780"><a href="#cb19-780" aria-hidden="true" tabindex="-1"></a>b1_enc<span class="op">=</span>np.zeros(d_model)</span>
<span id="cb19-781"><a href="#cb19-781" aria-hidden="true" tabindex="-1"></a>W2_enc<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-782"><a href="#cb19-782" aria-hidden="true" tabindex="-1"></a>b2_enc<span class="op">=</span>np.zeros(d_model)</span>
<span id="cb19-783"><a href="#cb19-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-784"><a href="#cb19-784" aria-hidden="true" tabindex="-1"></a>intrinsic_value_weight_dec_masked<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-785"><a href="#cb19-785" aria-hidden="true" tabindex="-1"></a>response_weight_dec_masked<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-786"><a href="#cb19-786" aria-hidden="true" tabindex="-1"></a>information_weight_dec_masked<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-787"><a href="#cb19-787" aria-hidden="true" tabindex="-1"></a>attention_weight_dec_masked<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-788"><a href="#cb19-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-789"><a href="#cb19-789" aria-hidden="true" tabindex="-1"></a>intrinsic_value_weight_dec_cross<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-790"><a href="#cb19-790" aria-hidden="true" tabindex="-1"></a>response_weight_dec_cross<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-791"><a href="#cb19-791" aria-hidden="true" tabindex="-1"></a>information_weight_dec_cross<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-792"><a href="#cb19-792" aria-hidden="true" tabindex="-1"></a>attention_weight_dec_cross<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-793"><a href="#cb19-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-794"><a href="#cb19-794" aria-hidden="true" tabindex="-1"></a>W1_dec<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-795"><a href="#cb19-795" aria-hidden="true" tabindex="-1"></a>b1_dec<span class="op">=</span>np.zeros(d_model)</span>
<span id="cb19-796"><a href="#cb19-796" aria-hidden="true" tabindex="-1"></a>W2_dec<span class="op">=</span>np.random.randn(d_model, d_model)<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb19-797"><a href="#cb19-797" aria-hidden="true" tabindex="-1"></a>b2_dec<span class="op">=</span>np.zeros(d_model)</span>
<span id="cb19-798"><a href="#cb19-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-799"><a href="#cb19-799" aria-hidden="true" tabindex="-1"></a><span class="co">#here we define the learning rate and epochs</span></span>
<span id="cb19-800"><a href="#cb19-800" aria-hidden="true" tabindex="-1"></a>learning_rate<span class="op">=</span><span class="fl">0.01</span></span>
<span id="cb19-801"><a href="#cb19-801" aria-hidden="true" tabindex="-1"></a>epochs<span class="op">=</span><span class="dv">20</span></span>
<span id="cb19-802"><a href="#cb19-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-803"><a href="#cb19-803" aria-hidden="true" tabindex="-1"></a>src_mask<span class="op">=</span><span class="va">None</span></span>
<span id="cb19-804"><a href="#cb19-804" aria-hidden="true" tabindex="-1"></a>tgt_mask<span class="op">=</span>create_mask_for_removing_future_dependency(for_inp_padded)</span>
<span id="cb19-805"><a href="#cb19-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-806"><a href="#cb19-806" aria-hidden="true" tabindex="-1"></a><span class="co">#and implement the generic neural network training</span></span>
<span id="cb19-807"><a href="#cb19-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-808"><a href="#cb19-808" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb19-809"><a href="#cb19-809" aria-hidden="true" tabindex="-1"></a>    enc_inp<span class="op">=</span>embed(eng_padded, src_embeddings)</span>
<span id="cb19-810"><a href="#cb19-810" aria-hidden="true" tabindex="-1"></a>    dec_inp<span class="op">=</span>embed(for_inp_padded, tgt_embeddings)</span>
<span id="cb19-811"><a href="#cb19-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-812"><a href="#cb19-812" aria-hidden="true" tabindex="-1"></a>    <span class="co">#this is the forward pass</span></span>
<span id="cb19-813"><a href="#cb19-813" aria-hidden="true" tabindex="-1"></a>    probs, cache<span class="op">=</span>forward_transformer(enc_inp, dec_inp,</span>
<span id="cb19-814"><a href="#cb19-814" aria-hidden="true" tabindex="-1"></a>                                       intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc, W1_enc, b1_enc, W2_enc, b2_enc,</span>
<span id="cb19-815"><a href="#cb19-815" aria-hidden="true" tabindex="-1"></a>                                       intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,</span>
<span id="cb19-816"><a href="#cb19-816" aria-hidden="true" tabindex="-1"></a>                                       intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,</span>
<span id="cb19-817"><a href="#cb19-817" aria-hidden="true" tabindex="-1"></a>                                       W1_dec, b1_dec, W2_dec, b2_dec,</span>
<span id="cb19-818"><a href="#cb19-818" aria-hidden="true" tabindex="-1"></a>                                       W_embed_out, b_embed_out,</span>
<span id="cb19-819"><a href="#cb19-819" aria-hidden="true" tabindex="-1"></a>                                       src_mask, tgt_mask)</span>
<span id="cb19-820"><a href="#cb19-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-821"><a href="#cb19-821" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span>cross_entropy_loss(probs, for_tgt_padded)</span>
<span id="cb19-822"><a href="#cb19-822" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-823"><a href="#cb19-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-824"><a href="#cb19-824" aria-hidden="true" tabindex="-1"></a>    pred_indices<span class="op">=</span>np.argmax(probs, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb19-825"><a href="#cb19-825" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb19-826"><a href="#cb19-826" aria-hidden="true" tabindex="-1"></a>        predicted_tokens<span class="op">=</span>[for_idx2word[idx] <span class="cf">for</span> idx <span class="kw">in</span> pred_indices[b]]</span>
<span id="cb19-827"><a href="#cb19-827" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Predicted:"</span>, <span class="st">" "</span>.join(predicted_tokens))</span>
<span id="cb19-828"><a href="#cb19-828" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-829"><a href="#cb19-829" aria-hidden="true" tabindex="-1"></a>    dprobs<span class="op">=</span>cross_entropy_derivative(probs, for_tgt_padded)</span>
<span id="cb19-830"><a href="#cb19-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-831"><a href="#cb19-831" aria-hidden="true" tabindex="-1"></a>    <span class="co">#this is the backward pass</span></span>
<span id="cb19-832"><a href="#cb19-832" aria-hidden="true" tabindex="-1"></a>    grads, dx_enc<span class="op">=</span>backward_transformer(dprobs, enc_inp, dec_inp, <span class="op">*</span>cache,</span>
<span id="cb19-833"><a href="#cb19-833" aria-hidden="true" tabindex="-1"></a>                                         intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc, W1_enc, b1_enc, W2_enc, b2_enc,</span>
<span id="cb19-834"><a href="#cb19-834" aria-hidden="true" tabindex="-1"></a>                                         intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,</span>
<span id="cb19-835"><a href="#cb19-835" aria-hidden="true" tabindex="-1"></a>                                         intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,</span>
<span id="cb19-836"><a href="#cb19-836" aria-hidden="true" tabindex="-1"></a>                                         W1_dec, b1_dec, W2_dec, b2_dec,</span>
<span id="cb19-837"><a href="#cb19-837" aria-hidden="true" tabindex="-1"></a>                                         W_embed_out, b_embed_out,</span>
<span id="cb19-838"><a href="#cb19-838" aria-hidden="true" tabindex="-1"></a>                                         src_mask, tgt_mask)</span>
<span id="cb19-839"><a href="#cb19-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-840"><a href="#cb19-840" aria-hidden="true" tabindex="-1"></a>    <span class="co">#update all parameters after backprop</span></span>
<span id="cb19-841"><a href="#cb19-841" aria-hidden="true" tabindex="-1"></a>    intrinsic_value_weight_enc<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'intrinsic_value_weight_enc'</span>]</span>
<span id="cb19-842"><a href="#cb19-842" aria-hidden="true" tabindex="-1"></a>    response_weight_enc<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'response_weight_enc'</span>]</span>
<span id="cb19-843"><a href="#cb19-843" aria-hidden="true" tabindex="-1"></a>    information_weight_enc<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'information_weight_enc'</span>]</span>
<span id="cb19-844"><a href="#cb19-844" aria-hidden="true" tabindex="-1"></a>    attention_weight_enc<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'attention_weight_enc'</span>]</span>
<span id="cb19-845"><a href="#cb19-845" aria-hidden="true" tabindex="-1"></a>    W1_enc<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'W1_enc'</span>]</span>
<span id="cb19-846"><a href="#cb19-846" aria-hidden="true" tabindex="-1"></a>    b1_enc<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'b1_enc'</span>]</span>
<span id="cb19-847"><a href="#cb19-847" aria-hidden="true" tabindex="-1"></a>    W2_enc<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'W2_enc'</span>]</span>
<span id="cb19-848"><a href="#cb19-848" aria-hidden="true" tabindex="-1"></a>    b2_enc<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'b2_enc'</span>]</span>
<span id="cb19-849"><a href="#cb19-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-850"><a href="#cb19-850" aria-hidden="true" tabindex="-1"></a>    intrinsic_value_weight_dec_masked<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'intrinsic_value_weight_dec_masked'</span>]</span>
<span id="cb19-851"><a href="#cb19-851" aria-hidden="true" tabindex="-1"></a>    response_weight_dec_masked<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'response_weight_dec_masked'</span>]</span>
<span id="cb19-852"><a href="#cb19-852" aria-hidden="true" tabindex="-1"></a>    information_weight_dec_masked<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'information_weight_dec_masked'</span>]</span>
<span id="cb19-853"><a href="#cb19-853" aria-hidden="true" tabindex="-1"></a>    attention_weight_dec_masked<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'attention_weight_dec_masked'</span>]</span>
<span id="cb19-854"><a href="#cb19-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-855"><a href="#cb19-855" aria-hidden="true" tabindex="-1"></a>    intrinsic_value_weight_dec_cross<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'intrinsic_value_weight_dec_cross'</span>]</span>
<span id="cb19-856"><a href="#cb19-856" aria-hidden="true" tabindex="-1"></a>    response_weight_dec_cross<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'response_weight_dec_cross'</span>]</span>
<span id="cb19-857"><a href="#cb19-857" aria-hidden="true" tabindex="-1"></a>    information_weight_dec_cross<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'information_weight_dec_cross'</span>]</span>
<span id="cb19-858"><a href="#cb19-858" aria-hidden="true" tabindex="-1"></a>    attention_weight_dec_cross<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'attention_weight_dec_cross'</span>]</span>
<span id="cb19-859"><a href="#cb19-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-860"><a href="#cb19-860" aria-hidden="true" tabindex="-1"></a>    W1_dec<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'W1_dec'</span>]</span>
<span id="cb19-861"><a href="#cb19-861" aria-hidden="true" tabindex="-1"></a>    b1_dec<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'b1_dec'</span>]</span>
<span id="cb19-862"><a href="#cb19-862" aria-hidden="true" tabindex="-1"></a>    W2_dec<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'W2_dec'</span>]</span>
<span id="cb19-863"><a href="#cb19-863" aria-hidden="true" tabindex="-1"></a>    b2_dec<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'b2_dec'</span>]</span>
<span id="cb19-864"><a href="#cb19-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-865"><a href="#cb19-865" aria-hidden="true" tabindex="-1"></a>    W_embed_out<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'W_embed_out'</span>]</span>
<span id="cb19-866"><a href="#cb19-866" aria-hidden="true" tabindex="-1"></a>    b_embed_out<span class="op">-=</span>learning_rate<span class="op">*</span>grads[<span class="st">'b_embed_out'</span>]</span>
<span id="cb19-867"><a href="#cb19-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-868"><a href="#cb19-868" aria-hidden="true" tabindex="-1"></a><span class="co"># After training, you can use the decoder in inference mode by feeding</span></span>
<span id="cb19-869"><a href="#cb19-869" aria-hidden="true" tabindex="-1"></a><span class="co"># previously generated tokens (shifted) as input to the decoder and applying</span></span>
<span id="cb19-870"><a href="#cb19-870" aria-hidden="true" tabindex="-1"></a><span class="co"># the masked multi-head attention to predict the next token</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>Epoch 1, Loss: 2.0795187664172397
Predicted: banane coniglio banane banane &lt;start&gt; piacciono &lt;end&gt;
Epoch 2, Loss: 2.0793373726502367
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 3, Loss: 2.0791564355830636
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 4, Loss: 2.078974718625546
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 5, Loss: 2.0787933552602227
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 6, Loss: 2.0786123118403346
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 7, Loss: 2.078431551323341
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 8, Loss: 2.078251032354273
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 9, Loss: 2.0780707083355354
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 10, Loss: 2.0778905181441116
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 11, Loss: 2.077710390742451
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 12, Loss: 2.077530273264538
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 13, Loss: 2.0773500940542484
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 14, Loss: 2.0771697955319044
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 15, Loss: 2.0769892772510743
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 16, Loss: 2.07680845041442
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 17, Loss: 2.0766272061537157
Predicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;
Epoch 18, Loss: 2.0764454984062057
Predicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;
Epoch 19, Loss: 2.076263321722837
Predicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;
Epoch 20, Loss: 2.0760807185682095
Predicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;</code></pre>
</section>
<section id="observations" class="level3">
<h3 class="anchored" data-anchor-id="observations">Observations</h3>
<p>Well, after all of that, this poor of a performance is what we got? This is true. We were translating only one sequence, after all. Where transformers really excel is at scale, because of the differentiable attention matrix which can be parallelized. But you can see how it gets stuck depending on different learning rates and initialization. One of the difficulties is initialization to make gradient descent helpful. Xavier initialization could have worked, but it is much better to do it with numpy to get real understanding.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>This aims to be an implementation of transformers that works, end-to-end, in numpy. No pointing to paper that blew up - Attention is All You Need (no, not it isn’t). No awkward discussions of ‘keys’, ‘values’, ‘queries’ (replaced with <em>fundamental</em> names instead). No list of mathematical formulas for different attention mechanisms with ‘choose what you want’. No describing the transformer architecture in a way that is essentially ‘just look at the figure’. No code snippets without explanation for the basics. For the last point, the final implementation is, in fact, annoying. The assumption is that you read this point from start to end. However there is something I have not mentioned at all. That something is <strong>automatic differentiation</strong>.</p>
<section id="automatic-differentiation" class="level4">
<h4 class="anchored" data-anchor-id="automatic-differentiation">Automatic Differentiation</h4>
<p>You may have noticed just how obnoxious implementing backpropagation was. Every single function had to be done manually. Was there a way we could have done it recursively? The answer is yes. This is called <em>automatic differentiation</em>. Automatic differentiation relies on the technique that all mathematical operations done by a computer are at the end pulled from basic arithmetic functions, since that’s what’s implemented on their circuit boards. Therefore, all computations of functions are inherently limited by this. Even though multiplication is not repeated addition, to a computer it is. Libraries exist entirely to implement backpropagation and automatic differentiation for user-defined functions. One such library is <strong>PyTorch</strong>. It implements automatic differentiation and abstracts it away, allowing users to define new functions and not worry about how backpropagation deals with that object. How would the transformer from scratch look like in PyTorch? <a href="https://github.com/IParraMartin/An-Explanation-Is-All-You-Need/blob/main/model.py">:)</a></p>


</section>
</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>If you angrily clicked on the footnote after reading this sentence, are you an NLP enthusiast?<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This is almost certainly me being lazy/not having any formal training in deep learning, but there <em>is</em> an awful lot of garbage out there, and most of it from full-time machine learning engineers! If I read another variation on ‘Transformers have revolutionized the field of natural language processing by leveraging the attention mechanism to embed data and make better predictions’….<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The better commented implementations of transformers from scratch that I found were created in 2024. Does this say anything about the field of machine learning?<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Slides are often combined with lectures, so I am willing to give this one a pass. They are also often meant for explaining new things to people who already have experience in the field, so it makes sense that I wouldn’t understand them. Unsurprisingly: https://sites.astro.caltech.edu/%7Egeorge/ay141/mermin.pdf<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>As of 2024, anyone claiming that transformers/LLMs can understand and reason like humans is pontificating.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Obviously, this goes wrong the instant you add punctuation, have strings like ‘abcdefghijklabcabcdabcdeab’ as input, have more words than your input stream can handle, and so on. However, implementing byte-pair encoding would go beyond the scope of the tutorial.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>I have transposed the column vector to make it a row vector.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Foreshadowing<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>In the sense that the next word is ‘meaningful’.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>I am deliberately including punctuation here, as the same technique can be used when punctuations are treated as unique words.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>This is also partially the reason the appeal to etymology is incorrect.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>Compare ‘hot’, ‘dog’, and ‘hot dog’.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>See <a href="https://en.wikipedia.org/wiki/Stirling%27s_approximation">Stirling’s approximation</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>Foreshadowing<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>Technically, this is what we have been attempting to do this entire time.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>More on this later<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>For this reason, layers tend to have the same activation function, as it is easy to parallelize the computation<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>Making automatic differentiation easier led to the invention of many fundamental inventions in empirical learning<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>https://sacred-texts.com/hin/m01/m01002.htm, ‘I am (continued Sauti)…’<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ksd3\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>