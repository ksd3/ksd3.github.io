<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Kshitij Duraphe</title>
<link>https://ksd3.github.io/blog/</link>
<atom:link href="https://ksd3.github.io/blog/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Sun, 16 Feb 2025 05:00:00 GMT</lastBuildDate>
<item>
  <title>PINOs in space physics</title>
  <link>https://ksd3.github.io/blog/2025-02-16-pino/</link>
  <description><![CDATA[ 





<p>When I was just starting at the <a href="https://heaviside.bu.edu">Semeter Lab</a> in October 2022, one of the first tasks assigned to me (which eventually became the first section of my thesis) was to do what is known to computer graphics scientists as <a href="https://www.youtube.com/watch?v=yYKqNjIMhek">novel view synthesis</a> using different 2D spectra viewing the ionosphere.</p>
<p>As with most deep learning tasks, the trick to this challenge often lies in <a href="https://en.wikipedia.org/wiki/Data_preprocessing">data preprocessing</a>, choosing the right <a href="https://en.wikipedia.org/wiki/Neural_architecture_search">architecture</a>, <a href="https://en.wikipedia.org/wiki/Activation_function">activation function</a>, designing the right type of layer for the data you are processing, and designing the <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a>. When dealing with physical simulations, a natural choise is to start with a <a href="https://i-systems.github.io/tutorial/KSNVE/220525/01_PINN.html">Physics-Informed Neural Network</a>. There is no difference between an ANN and a PINN. ANNs are <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal function approximators</a> which are trained by <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagating</a> errors through the entire network and updating the weight of each node. Many fields in deep learning are built on defining this loss function itself. The famous <a href="https://en.wikipedia.org/wiki/You_Only_Look_Once">YOLO</a> algorithm is entirely a result of loss function engineering. While the idea itself predates machine learning, modifying the loss function and training for long enough with gradient descent gets you fairly accurate results in many scenarios.</p>
<p>The idea behind a PINN is to map a set of <em>input</em> points to their <em>outputs</em>, where the output is decided as the result of passing each point through a solution of the physical system being modeled. The most common way to get the output is to map input points to the solution of a particular differential equation that the physical system is being modeled as. Since ANNs can fit any function if trained for long enough, PINNs are designed as a neural network that are trained for shorter time periods and generalize to solutions of the differential equation that are not seen in the training data. The way they do this is by adding the differential equation itself to the loss function.</p>
<p>This idea can be taken further. A <a href="https://arxiv.org/abs/2111.03794">physics-informed neural <em>operator</em></a> tries to generalize to the entire domain. Specifically, given an <a href="https://en.wikipedia.org/wiki/Operator_(mathematics)">operator</a> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>:</mo><mi>X</mi><mo>→</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">F:X \rightarrow Y</annotation></semantics></math>, the idea is to take some points from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> and their corresponding transformations from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math> and learn <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>. That is: given a neural network represented as a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>x</mi><mo>∈</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">N(x), x \in X</annotation></semantics></math>, we want to make <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> arbitrarily close to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>. The resulting matrix multiplication that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> performs should be close to the transformation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>, which may not be a matrix multiplication. The way you feed points from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> into <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> is often modalities such as an image. In simple terms: If you have a video of waves in the ocean, the PINO should be able to predict where the wave is going (that is, where it is as time <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t+1</annotation></semantics></math>) given only the picture of the wave (the current frame) at the current time <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>.</p>
<p>The way this is done is also very simple. The idea is to take a <a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier transform</a> of the input data, multiply it by a randomly-initialized weight matrix, and take the inverse Fourier transform and compare the results. Your backpropagation will update the weight matrix itself. In practice, noise in your input data is high frequency, so the weight matrix is often initialized to favour lower frequencies in the Fourier-transformed-data. The loss function is often modified as well to include the physics-informed loss.</p>
<p>The natural generalization is the <a href="https://github.com/vsitzmann/awesome-implicit-representations">implicit neural representation</a>. This means training a neural network that more or less overfits to the specific function at all points. I have used INRs for modeling <a href="https://en.wikipedia.org/wiki/Equatorial_plasma_bubble">plasma bubbles</a> in the ionosphere.</p>
<p>This is a very simple overview of an interesting topic. So far PINOs have found applications in CFD simulations. I, however, used them for <a href="https://en.wikipedia.org/wiki/Magnetohydrodynamics">MHD</a> simulations. I modeled <a href="https://en.wikipedia.org/wiki/STEVE">STEVE</a>-like phenomena in the ionosphere for novel-view generation and 3D reconstruction. This was done to answer some outstanding questions in ionospheric plasma physics; namely, what can we infer about the physical processes going on in the atmosphere if the phenomenon is so-and-so large?</p>
<section id="plasma-bubbles" class="level2">
<h2 class="anchored" data-anchor-id="plasma-bubbles">Plasma Bubbles</h2>
<p>The graphs below show simulations of plasma bubbles at low resolution using a PINO with the SIREN architecture. The reconstruction is pretty good, even if the SIREN architecture is not designed for diffuse boundaries. This works because SIREN architectures work well at multiple scales.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://ksd3.github.io/images/Plasma_bubble_1.png" class="img-fluid figure-img"></p>
<figcaption>Plasma Bubble 1</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://ksd3.github.io/images/Plasma_bubble_2.png" class="img-fluid figure-img"></p>
<figcaption>Plasma Bubble 2</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://ksd3.github.io/images/Plasma_bubble_reconstruction.png" class="img-fluid figure-img"></p>
<figcaption>Plasma Bubble Reconstruction</figcaption>
</figure>
</div>
</section>
<section id="steve-mhd" class="level2">
<h2 class="anchored" data-anchor-id="steve-mhd">STEVE MHD</h2>
<p>A full STEVE-like simulation can be done with <a href="https://gemini3d.github.io/gemini3d/">Gemini3D</a>. I modeled the basic underlying phenomena and simulated them over lower resolutions and larger timescales. You can still see the underlying structure in the video.</p>
<video controls="" width="100%">
<source src="../../videos/mhd_pino.mp4" type="video/mp4">
</video>
<p>For this, I used 2D spectral convolutions. The PINO didn’t quite reconstruct the phenomenon because of the diffuse boundaries of ionospheric phenomena, which is what SIREN activations are designed for. I eventually ended up going with classical algorithms that helped me do a 3D reconstruction of STEVE.</p>
<p>Here’s a single frame (frame 39) from that video. I only used 16% of the simulation as training data, and it still reconstructed everything further on pretty well!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://ksd3.github.io/images/mhd_pino_frame.png" class="img-fluid figure-img"></p>
<figcaption>MHD PINO Frame</figcaption>
</figure>
</div>


</section>

 ]]></description>
  <category>physics</category>
  <category>machine-learning</category>
  <category>research</category>
  <guid>https://ksd3.github.io/blog/2025-02-16-pino/</guid>
  <pubDate>Sun, 16 Feb 2025 05:00:00 GMT</pubDate>
</item>
<item>
  <title>How to get into AI research as an outsider</title>
  <link>https://ksd3.github.io/blog/2025-02-10-ml-research/</link>
  <description><![CDATA[ 





<p>This post offers my perspective on how to break into AI research. Although it is divided into several chapters, I will primarily discuss two things. The first is <em>what</em> AI research is really like, and the second is what to do in AI research depending on your aims.</p>
<section id="preliminaries" class="level2">
<h2 class="anchored" data-anchor-id="preliminaries">Preliminaries</h2>
<p>Let’s not beat around the bush. The reason a lot of people want to get into AI research is because they want to be rich, respected, ‘in the know’, or be able to say ‘I did it first’; primarily the first reason. It’s the same with any fad. It was the same with cryptocurrency. Bitcoin <a href="https://finance.yahoo.com/quote/BTC-USD/history/?period1=1410912000&amp;period2=1739128862">opened</a> at $10,077.40 on November 29, 2017 and nearly doubled in price three weeks later, before correcting to below $10,000 on February 2 of the following year. It felt like practically every financial technology company launched an initial coin offering (think initial public offering, but crypto) in the summer of 2018 based on nothing more than a whitepaper proving how their cryptocurrency would beat all the other cryptocurrencies on the market. Nearly all of the coins ended up being fraudulent schemes designed to extract money from unsuspecting customers. After regulations curtailed private investments that enabled these launches the volume of coins being traded plummeted.</p>
<p>Why did this happen? The main reason was that every financial company wanted to get in on this shiny new money-making opportunity because of fear of missing out. What did they do? They hired en masse, trying to set up teams that had domain knowledge. If they didn’t hire, no one else would. Venture capitalists and private equity offered money freely to people who presented murky plans detailing expected return on investment (often 100x or more) and promised the moon. What ended up happening? Bitcoin didn’t replace paper currency. The world didn’t move to public-ledger-only transactions. Governments introduced new regulations to tax cryptocurrency earnings. Large companies rolled out public projects for facilitating transactions with a verifiable token.</p>
<p>It’s the same with AI. The only major difference is that cryptocurrency was still locked behind a door that only people with the right knowledge could open. AI is accessible and usable by everyone, even children. Let me take a step back and define what exactly ‘AI’ means, and what people think it means. A person employed as an AI engineer, ML engineer, Data Scientist, or something similar primarily works on one of the following things. I personally categorize them in the following way:</p>
<ul>
<li>A <em>data scientist</em> is primarily concerned with <em>statistical inference</em> and hypothesis testing.</li>
<li>A <em>machine learning engineer</em> is primarily concerned with writing code that builds models to solve a particular task.</li>
<li>An <em>AI engineer</em> is primarily someone that takes together open-source tools people have built and stacks them on top of each other like LEGO blocks.</li>
</ul>
<p>These are not mutually exclusive. I know many ML engineers that only write backend software and use models available online. I know a few data scientists that primarily take models others have built and use them on tasks. I myself was an AI engineer that worked on model finetuning and data analytics.</p>
</section>
<section id="what-ai-research-is-like" class="level2">
<h2 class="anchored" data-anchor-id="what-ai-research-is-like">What AI research is like</h2>
<p>In <a href="https://arxiv.org/stats/monthly_submissions">2024</a> there were just under an average of 20000 submissions per month on arXiv. A <a href="https://arxiv.org/pdf/2210.00881">paper</a> published two years earlier showed the number of papers submitted per month in four ‘AI’ categories doubled every two years or so. By my count, there are about <a href="https://arxiv.org/">155 separate categories</a> on arXiv’s front page. This means that in 2024, 20000 papers were submitted per month to 155 categories. Under the fairly biased and weak assumption that the abovementioned trends hold, we can say that there were 8000 papers on AI submitted per month in 2024 to arXiv, or about 40% of all submissions were in these four categories.</p>
<p>We need to make a distinction between different types of AI research. A straightforward one can be made by classifying research as <em>applied machine learning</em> and <em>non-applied machine learning</em>.</p>
<p><strong>Applied machine learning</strong> is the application of pre-researched methods to a task. Papers such as <a href="https://www.sciencedirect.com/science/article/pii/S0306261924003544">Day-ahead regional solar power forecasting with hierarchical temporal convolutional neural networks</a> are a prime example. The authors construct a model to do something, then apply it to the dataset and evaluate the results.</p>
<p><strong>Non-applied machine learning</strong> creates general methods and applies them to a variety of tasks. <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a> is an example. They create residual connections and show empirically that they are a huge improvement over all pre-existing models.</p>
<p>When people say they want to get into AI research, they often mean <em>non-applied machine learning</em> research.</p>
</section>
<section id="the-reality-of-research" class="level2">
<h2 class="anchored" data-anchor-id="the-reality-of-research">The reality of research</h2>
<p>Before you think about AI research, you should probably know what research, the field, is like. A lot of people are drawn to research because they see it as a calling. But research as a field is populated by humans, and humans are biased. The big research jobs available at big companies (Applied Scientists at Amazon, Research Scientists at NVIDIA) are only available to people with PhDs. Because they are the biggest and highest-paying research jobs, they tend to take PhDs only from high-ranking universities.</p>
<p>There is no difference between AI research and other types of research, at least on the ‘research’ side. It still involves learning your field from first principles, reproducing other peoples’ work, coming up with hypotheses about what can be improved in the field and how to improve it, and finally rigorously testing everything before publishing.</p>
</section>
<section id="practicalities" class="level2">
<h2 class="anchored" data-anchor-id="practicalities">Practicalities</h2>
<section id="tooling" class="level3">
<h3 class="anchored" data-anchor-id="tooling">Tooling</h3>
<p>One of the big things when being introduced to a new field is learning the tooling. One of the good things about machine learning is that it is easily accessible. With tools like Google Colab, Paperspace’s Gradient, and Kaggle Notebooks, it is very easy for people to start working with models and datasets. The common theme among these is Python. Python has become synonymous with machine learning, with <a href="https://pytorch.org/">PyTorch</a> being the library of choice for implementing most general-purpose models.</p>
<p>If you want to work more on the data science side of things, <a href="https://www.r-project.org/">R</a> is better than Python because of the sheer variety of algorithms implemented in it. If you want to work in scientific computing, <a href="https://docs.jax.dev/en/latest/quickstart.html">Jax</a> and <a href="https://julialang.org/">Julia</a> are better options.</p>
<p>The next part is the mathematics. People need to know the basics - undergraduate-level multivariate calculus, statistics, optimization theory, and possibly some formal mathematical proving before working in machine learning. A popular book is <a href="https://mml-book.github.io/">Mathematics of Machine Learning</a>. The two holy books of the field are <a href="https://hastie.su.domains/ElemStatLearn/">ESL</a> and <a href="https://www.statlearning.com/">ISL</a>. To get into <em>deep learning</em>, read <a href="https://www.deeplearningbook.org/">Deep Learning</a>.</p>
</section>
<section id="educational-requirements" class="level3">
<h3 class="anchored" data-anchor-id="educational-requirements">Educational requirements</h3>
<p>Do you need to be in higher education to start working in AI research? The answer without any caveats is <strong>no</strong>. AI is one of the few fields that allows people unaffiliated with an institution to submit research works to a conference. But you have a much higher chance of your research getting results if you are in higher education.</p>
</section>
<section id="if-youre-looking-to-start" class="level3">
<h3 class="anchored" data-anchor-id="if-youre-looking-to-start">If you’re looking to start</h3>
<p>If you know someone who needs a problem solved with machine learning, start working. Identifying which problems need to be solved with machine learning is an art in itself.</p>
<p>If you’re an undergraduate student then you should actively reach out to professors. Many professors are happy to take undergraduate students and assign them to a project. If you’re a Master’s student then it’s significantly more difficult: you have to prove to professors that 1. you have the technical skill and 2. enough domain knowledge to immediately start making a contribution.</p>
<p>A good technique is to cold email a lot of people. More than 99% of the time you will not get a reply. Be honest about your skills and shortcomings and hope for the best.</p>
</section>
<section id="mindset" class="level3">
<h3 class="anchored" data-anchor-id="mindset">Mindset</h3>
<p>I can throw around buzzwords like resilience, mental fortitude, determination, discipline, but that’s not quite what’s required. Like anything, you do need to have some amount of natural talent in thinking through a problem before tackling it. The second thing is that you need to have <a href="https://en.wiktionary.org/wiki/Sitzfleisch">Sitzfleisch</a>, the ability to carry on under any circumstances. You only get this with practice and liking the work you do. Hard work, when properly applied, is the only thing that makes you successful.</p>


</section>
</section>

 ]]></description>
  <category>machine-learning</category>
  <category>career</category>
  <category>research</category>
  <guid>https://ksd3.github.io/blog/2025-02-10-ml-research/</guid>
  <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Transformers from scratch in numpy</title>
  <link>https://ksd3.github.io/blog/2024-12-07-transformers/</link>
  <description><![CDATA[ 





<section id="welcome" class="level1">
<h1>Welcome</h1>
<p>Transformers are a type of <em>sequence to sequence</em> model, i.e., given a sequence of characters, which may be split into words, transformers are able to convert that sequence to another sequence in a way that preserves the original sequence’s ‘meaning’ <em>without using any predefined rules</em>. An example of a sequence-to-sequence task is text translation, such as converting the English sentence “I ate an apple” into the Italian equivalent: “Ho mangiato una mela.”</p>
<p>Transformers are an example of an <em>encoder-decoder</em> architecture. Encoder-decoder architectures take input data, squeeze it into a kind of secret code (often called ‘creating a latent representation’), and sometimes decode the squeezed data to perform useful tasks. An example of an encoder-decoder architecture and a task it may be used on would be using an <a href="https://en.wikipedia.org/wiki/Autoencoder">autoencoder</a> to denoise images.</p>
<p>This page contains an implementation and demonstration of the transformer architecture <strong>from scratch</strong> using as few predefined libraries as possible in order to give the reader an understanding of what really goes on in each step of the process. We avoid using predefined models or implementations of algorithms as much as possible.</p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>We address the following questions:</p>
<ol type="1">
<li>What is the fundamental principle of a transformer?</li>
<li>In very general terms, what does a transformer do?</li>
<li>What specific tasks do we need to do in order to implement a transformer?</li>
</ol>
<section id="what-is-the-fundamental-principle-of-a-transformer" class="level3">
<h3 class="anchored" data-anchor-id="what-is-the-fundamental-principle-of-a-transformer">1. What is the fundamental principle of a transformer?</h3>
<p>The fundamental principle of a transformer is that one-hot vectors can be used to look up particular rows of a matrix, and you can exploit this to selectively extract, combine, and mask information from your input to produce better outputs<sup>1</sup>. A lot of readers, especially NLP enthusiasts, may immediately have a problem with this statement. After all, we have not used the most famous term associated with a transformer (attention) while stating this fundamental principle. We also did not say anything about feature embeddings, long-range dependencies, contextual relationships, and encodings - all terms that are used when talking/reading about transformers. This is done for two reasons. First, I believe that it is extremely important to understand what exactly is going on in terms of as many elementary operations as possible. I believe that this necessarily precludes using domain-specific jargon. Second, I am tired of reading innumerable blogs<sup>2</sup>, code comments on GitHub<sup>3</sup>, and slides that fail to give you understanding<sup>4</sup>.</p>
</section>
<section id="in-very-general-terms-what-does-a-transformer-do" class="level3">
<h3 class="anchored" data-anchor-id="in-very-general-terms-what-does-a-transformer-do">2. In very general terms, what does a transformer do?</h3>
<p>A transformer takes in a <em>sequence</em> of <em>elements</em> (this sequence is often long), figures out how different elements in that sequence are related, then squeezes that sequence into a list of numbers that capture any inherent ‘meaning’<sup>5</sup>. It then takes that list of numbers and unsqueezes it into a different sequence that tries to preserve the ‘meaning’ of the original sequence. As shown in the welcome section, transformers can be used for language translation.</p>
</section>
<section id="what-specific-tasks-do-we-need-to-do-in-order-to-implement-a-transformer" class="level3">
<h3 class="anchored" data-anchor-id="what-specific-tasks-do-we-need-to-do-in-order-to-implement-a-transformer">3. What specific tasks do we need to do in order to implement a transformer?</h3>
<ol type="1">
<li>Figure out a way to split individual elements in a sequence (hereafter ‘words and punctuation in a sentence’) and find a way to feed them to the computer.</li>
<li>Make the computer squeeze these words into a list of numbers it can understand.</li>
<li>Make the computer unsqueeze the list of numbers into words and form sentences in another language.</li>
</ol>
</section>
</section>
<section id="preliminaries" class="level2">
<h2 class="anchored" data-anchor-id="preliminaries">Preliminaries</h2>
<p>Suppose our input language only has four words (‘My’, ‘rabbit’, ‘likes’, ‘bananas’), and no punctuation at all. Sentences from our language could be ‘rabbit likes bananas’ or ‘My rabbit likes bananas’ or ‘likes My bananas’ or ‘bananas rabbit My’. For the sake of sanity, let’s assume that our language only has the sentence ‘My rabbit likes bananas’. We want to translate this into Italian: ‘Al mio coniglio piacciono le banane’. How do we feed our initial sentence to a computer?</p>
<section id="tokenization" class="level3">
<h3 class="anchored" data-anchor-id="tokenization">Tokenization</h3>
<p>We first need to split the sentence into individual words. Because our language does not have any sort of punctuation, we can do what is called whitespace tokenization. This is the most natural way of splitting an English sentence - assume that individual words are separated by a blank space, read through the sentence, and store all characters between two whitespaces as a single word <sup>6</sup>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> List <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#for clean function annotation</span></span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> whitespace_tokenizer(sentence: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> List[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>]:</span>
<span id="cb1-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Function that reads a string/sentence and outputs a list of strings, where each output string is a word in that sentence. Each word is considered to be delimited by whitespaces.</span></span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Input:</span></span>
<span id="cb1-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        sentence: str - assumed nonempty for explanation purposes</span></span>
<span id="cb1-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Output:</span></span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        list of strings</span></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb1-12">    tokenized_sentence<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[] <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#final output, a list of words</span></span>
<span id="cb1-13">    current_word<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[] <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#list to store the current word</span></span>
<span id="cb1-14"></span>
<span id="cb1-15">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#The technique to whitespace tokenize the sentence is to iterate through it, and store each non-whitespace character in current_word. </span></span>
<span id="cb1-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Once a whitespace is encountered, append the contents of current_word to tokenized_sentence and clear current_word</span></span>
<span id="cb1-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sentence:</span>
<span id="cb1-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>:</span>
<span id="cb1-19">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> current_word:</span>
<span id="cb1-20">                tokenized_sentence.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">''</span>.join(current_word)) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#append to the list of tokens</span></span>
<span id="cb1-21">                current_word<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[]<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#reset current_word</span></span>
<span id="cb1-22">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-23">            current_word.append(i)</span>
<span id="cb1-24">    </span>
<span id="cb1-25">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#this still leaves the final word in, so add it last</span></span>
<span id="cb1-26">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> current_word:</span>
<span id="cb1-27">        tokenized_sentence.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">''</span>.join(current_word))</span>
<span id="cb1-28">    </span>
<span id="cb1-29">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#delete the current word from memory explicitly (not required)</span></span>
<span id="cb1-30">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">del</span> current_word</span>
<span id="cb1-31"></span>
<span id="cb1-32">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> tokenized_sentence</span>
<span id="cb1-33"></span>
<span id="cb1-34">english_sentence<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"My rabbit likes bananas"</span></span>
<span id="cb1-35"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Your list of tokens for the English sentence is:"</span>, whitespace_tokenizer(english_sentence))</span>
<span id="cb1-36">english_tokenized_sentence<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>whitespace_tokenizer(english_sentence) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#save the tokenization in a list</span></span>
<span id="cb1-37"></span>
<span id="cb1-38">italian_sentence<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Al mio coniglio piacciono le banane"</span></span>
<span id="cb1-39"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Your list of tokens for the Italian sentence is:"</span>, whitespace_tokenizer(italian_sentence))</span>
<span id="cb1-40">italian_tokenized_sentence<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>whitespace_tokenizer(italian_sentence)</span></code></pre></div></div>
<pre><code>Your list of tokens for the English sentence is: ['My', 'rabbit', 'likes', 'bananas']
Your list of tokens for the Italian sentence is: ['Al', 'mio', 'coniglio', 'piacciono', 'le', 'banane']</code></pre>
</section>
<section id="feeding-these-words-to-a-computer" class="level3">
<h3 class="anchored" data-anchor-id="feeding-these-words-to-a-computer">Feeding these words to a computer</h3>
<p>How do we feed these words into a computer? One way of doing it would be by assigning each individual word to a real number: [‘My’, ‘rabbit’, ‘likes’, ‘bananas’] -&gt; [‘935.88’, ‘-28124.4483957’, ‘3’, ‘-2’]. This is inefficient, as the amount of precision you would need to implement would increase computatational costs and storage requirements. A better way of storing a word would be to store it in a vector. Here is how we can do this. Given a vector, stored as a column matrix with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> rows, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> is the number of words in your vocabulary, replace one of the zeros with 1 such that the position of the 1 is unique for that particular word. Then, stack those vectors side by side to form a matrix where each row and column has only one 1 and all other elements are zero. Such vectors are called ‘one-hot’ vectors and this is a type of encoding called <strong>one-hot encoding</strong>.</p>
<p>This is illustrated below:</p>
‘My’=$
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
0\\
0\\
1\\
0
\end{pmatrix}</annotation></semantics></math>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>,</mo><mi>′</mi><mi>r</mi><mi>a</mi><mi>b</mi><mi>b</mi><mi>i</mi><mi>t</mi><mi>′</mi><mo>=</mo></mrow><annotation encoding="application/x-tex">, 'rabbit'=</annotation></semantics></math>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
0\\
1\\
0\\
0
\end{pmatrix}</annotation></semantics></math>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>,</mo><mi>′</mi><mi>l</mi><mi>i</mi><mi>k</mi><mi>e</mi><mi>s</mi><mi>′</mi><mo>=</mo></mrow><annotation encoding="application/x-tex">, 'likes'=</annotation></semantics></math>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
1\\
0\\
0\\
0
\end{pmatrix}</annotation></semantics></math>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>,</mo><mi>′</mi><mi>b</mi><mi>a</mi><mi>n</mi><mi>a</mi><mi>n</mi><mi>a</mi><mi>s</mi><mi>′</mi><mo>=</mo></mrow><annotation encoding="application/x-tex">, 'bananas'=</annotation></semantics></math>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
0\\
0\\
0\\
1
\end{pmatrix}</annotation></semantics></math>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>,</mo><mi>′</mi><mi>M</mi><mi>y</mi><mi>r</mi><mi>a</mi><mi>b</mi><mi>b</mi><mi>i</mi><mi>t</mi><mi>l</mi><mi>i</mi><mi>k</mi><mi>e</mi><mi>s</mi><mi>b</mi><mi>a</mi><mi>n</mi><mi>a</mi><mi>n</mi><mi>a</mi><mi>s</mi><mi>′</mi><mo>=</mo></mrow><annotation encoding="application/x-tex">,
'My rabbit likes bananas'=</annotation></semantics></math>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}</annotation></semantics></math>
<p>$</p>
<p>Let’s call the last matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math>, for ‘word matrix’. Observe that when any one-hot encoded is multiplied with another matrix, by <a href="https://www.dummies.com/article/academics-the-arts/math/pre-calculus/how-to-multiply-matrices-by-each-other-167710/">the rules of matrix multiplication</a>, the column of the second matrix corresponding to the position of the 1 in the column vector is ‘pulled out’<sup>7</sup>. This is illustrated below:</p>
$
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
0 &amp; 0 &amp; 1 &amp; 0
\end{pmatrix}</annotation></semantics></math>
$ $
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.4</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mn>9</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn><mo>+</mo><mi>i</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>29</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>12</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>45.328539</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn><mi>i</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>32</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>32</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>12</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>43</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.482</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.212</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>241</mn></mtd><mtd columnalign="center" style="text-align: center"><msup><mi>e</mi><mi>π</mi></msup></mtd><mtd columnalign="center" style="text-align: center"><mi>T</mi><mi>R</mi><mi>E</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mi>T</mi><mi>R</mi><mi>E</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
0.2 &amp; 0.4 &amp; -9 &amp; 1+i \\
29 &amp; 12 &amp; 45.328539 &amp; 0 \\
2i &amp; 32 &amp; 2 &amp; 32 \\
12 &amp; 43 &amp; 0.482 &amp; 0.212 \\
241 &amp; e^{\pi} &amp; TREE(3) &amp; -TREE(3)
\end{pmatrix}</annotation></semantics></math>
<p>$ <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>−</mi><mn>9</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>45.328539</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.482</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>T</mi><mi>R</mi><mi>E</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">=\begin{pmatrix}
-9 \\
45.328539 \\
2 \\
0.482\\
TREE(3)
\end{pmatrix}</annotation></semantics></math></p>
<p>If you’re able to construct this second matrix, then it can potentially lead to something interesting<sup>8</sup>. There are of course other ways to encode words, and for practical language tasks you take someone else’s encoding and use it, but it is important to understand the core principle.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> List, Dict, Tuple</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> random</span>
<span id="cb3-4"></span>
<span id="cb3-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> create_concatenated_matrix_from_tokens(tokens: List[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb3-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb3-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Function that creates a concatenated one-hot encoded matrix from a tokenized sentence.</span></span>
<span id="cb3-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    </span></span>
<span id="cb3-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Input:</span></span>
<span id="cb3-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        tokens: List containing tokens</span></span>
<span id="cb3-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Output:</span></span>
<span id="cb3-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        tokenized_matrix: A 2-D one-hot encoded np.ndarray of tokens. Each row and column contains only one 1. Always square.</span></span>
<span id="cb3-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb3-14"></span>
<span id="cb3-15">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#The idea is to simply generate a diagonal matrix which will be one-hot encoded by definition</span></span>
<span id="cb3-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Create a dictionary to map each token to a unique index</span></span>
<span id="cb3-17">    token_to_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{token: idx <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> idx, token <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(tokens)} <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#get index-token pair from the input list</span></span>
<span id="cb3-18">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Initialize the matrix with zeros</span></span>
<span id="cb3-19">    tokenized_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.zeros((<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(tokens), <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(tokens)), dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>)</span>
<span id="cb3-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Populate the one-hot encoded matrix</span></span>
<span id="cb3-21">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> token <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> tokens:</span>
<span id="cb3-22">        index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>token_to_index[token]</span>
<span id="cb3-23">        tokenized_matrix[index][index]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Set the diagonal element to 1</span></span>
<span id="cb3-24"></span>
<span id="cb3-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> tokenized_matrix, token_to_index <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#this second variable is returned to pull out a random token and corresponding vector later</span></span>
<span id="cb3-26"></span>
<span id="cb3-27">english_onehot_matrix, english_token_to_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>create_concatenated_matrix_from_tokens(english_tokenized_sentence)</span>
<span id="cb3-28"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The English one-hot encoded matrix is:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, english_onehot_matrix)</span>
<span id="cb3-29"></span>
<span id="cb3-30"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#let's pull out a random token and a one-hot encoded vector to see how it pulls out specific features</span></span>
<span id="cb3-31"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_random_token_and_vector(token_to_index: Dict[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>], one_hot_matrix: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tuple[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, np.ndarray]:</span>
<span id="cb3-32">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb3-33"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Function that pulls a random token and its corresponding one-hot vector.</span></span>
<span id="cb3-34"></span>
<span id="cb3-35"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Input:</span></span>
<span id="cb3-36"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        token_to_index: Dictionary mapping tokens to their indices</span></span>
<span id="cb3-37"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        one_hot_matrix: 2D NumPy array containing one-hot vectors</span></span>
<span id="cb3-38"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Output:</span></span>
<span id="cb3-39"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        A tuple containing the random token and its corresponding one-hot vector</span></span>
<span id="cb3-40"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb3-41">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Randomly select a token</span></span>
<span id="cb3-42">    random_token<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>random.choice(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(token_to_index.keys()))</span>
<span id="cb3-43">    </span>
<span id="cb3-44">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Get the corresponding one-hot vector</span></span>
<span id="cb3-45">    one_hot_vector<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>one_hot_matrix[token_to_index[random_token]]</span>
<span id="cb3-46">    </span>
<span id="cb3-47">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> random_token, one_hot_vector</span>
<span id="cb3-48"></span>
<span id="cb3-49">random_token, corresponding_one_hot_vector<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>get_random_token_and_vector(english_token_to_index, english_onehot_matrix)</span>
<span id="cb3-50"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Let's pick a random token:"</span>, random_token, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">The corresponding one-hot vector is:"</span>, corresponding_one_hot_vector)</span>
<span id="cb3-51"></span>
<span id="cb3-52"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#generate a random matrix to demonstrate pulling out certain columns/rows</span></span>
<span id="cb3-53">random_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.rand(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(english_tokenized_sentence), <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(english_tokenized_sentence))</span>
<span id="cb3-54"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Multiplying an example random matrix</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, random_matrix, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">by"</span>, random_token<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"'s one-hot vector"</span>, corresponding_one_hot_vector, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">pulls out the row"</span>, np.matmul(corresponding_one_hot_vector, random_matrix), <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">and multiplying by the tranpose of that vector pulls out the column:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, np.matmul(random_matrix, corresponding_one_hot_vector[:, np.newaxis]))</span></code></pre></div></div>
<pre><code>The English one-hot encoded matrix is:
 [[1 0 0 0]
 [0 1 0 0]
 [0 0 1 0]
 [0 0 0 1]]
Let's pick a random token: bananas 
The corresponding one-hot vector is: [0 0 0 1]
Multiplying an example random matrix
 [[0.48871169 0.52574402 0.78833029 0.7045616 ]
 [0.76188795 0.13720883 0.39406852 0.02774654]
 [0.27090269 0.35964049 0.20715361 0.3064574 ]
 [0.23701048 0.67718606 0.87441259 0.05116356]] 
by bananas's one-hot vector [0 0 0 1] 
pulls out the row [0.23701048 0.67718606 0.87441259 0.05116356] 
and multiplying by the tranpose of that vector pulls out the column:
 [[0.7045616 ]
 [0.02774654]
 [0.3064574 ]
 [0.05116356]]</code></pre>
</section>
<section id="sequence-prediction" class="level3">
<h3 class="anchored" data-anchor-id="sequence-prediction">Sequence prediction</h3>
<p>An immediate application of this specific kind of matrix multiplication is as follows. Suppose we have the following sentence in our four-word language: ‘My rabbit’. Our task is to predict the next word that comes after it<sup>9</sup>. One easy way of doing this is by observing that we only have four options. We can construct the following four sentences: | Next Word | Potential next (possible incomplete) sentence | |—-|—-| |My|My rabbit My| |rabbit|My rabbit rabbit| |likes|My rabbit likes| |bananas|My rabbit bananas|</p>
<p>How do we decide what word comes next? Well, we can’t decide on our own. Perhaps, to an alien whose language consists of only four words that sound exactly like English words, the sentence ‘My rabbit My’ would translate to English as ‘I am in need of two oranges and a deck of playing cards.’ The sentence ‘My rabbit rabbit’ would translate to ‘I am on fire’. The sentence ‘My rabbit rabbit rabbit rabbit bananas bananas rabbit bananas My likes likes bananas’ would translate to ‘Yes’ (remember, I have not put any limits on the length of the sentences!). The point of these examples is to show you that there is no way for us to predict the next word unless we have some idea of what it is going to be. One way to solve this problem is for a third party (say a talking dog) to step in and say, “I’ve been around these aliens, and I’ve observed that whenever they begin a sentence with ‘My rabbit’, the next word is ‘bananas’ 10% of the time, ‘likes’ 85% of the time, ‘rabbit’ 5% of the time, but ‘My’ never comes after ‘rabbit’. Is there some way for you to use this information? Also, whatever I say is always true.”</p>
<p>Since we have no better option, let’s trust the talking dog. We can in fact use its information in the following way. We can construct the following vector that shows the probability of predicting the next word after ‘rabbit’, if spoken by an alien.</p>
$<br>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtable><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">bananas</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">likes</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">rabbit</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">My</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd></mtr></mtable><mspace width="1.0em"></mspace><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.85</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.05</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{aligned}
&amp;\text{bananas}\\
&amp;\text{likes}\\
&amp;\text{rabbit}\\
&amp;\text{My}\\
\end{aligned}
\quad
\begin{bmatrix}
0.1 \\ 0.85 \\ 0.05 \\ 0
\end{bmatrix}</annotation></semantics></math>
<p><br>
$</p>
<p>This is somewhat useful. We know that there is a high chance that the next word in the sentence will be ‘likes’, so the possible incomplete sentence will now probably be ‘My rabbit likes’. But wait a minute. This vector of probabilities is like the column we pulled out of the matrix above. Is it possible to reconstruct this matrix? We can certainly do so - just assume that the dog is always true and start interrogating the dog about the probabilities of the next word <em>after</em> each word in the language, <em>regardless of the context</em>. Let’s assume the dog is happy to tell us this, so we now have the following matrix:</p>
$<br>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal"></mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">rabbit</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">My</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.9</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.07</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.03</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">rabbit</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.85</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.05</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">My</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.01</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.79</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{array}{r|cccc}
\text{} &amp; \text{bananas} &amp; \text{likes} &amp; \text{rabbit} &amp; \text{My} \\
\hline
\text{bananas} &amp; 0.1 &amp; 0 &amp; 0 &amp; 0 \\
\text{likes}   &amp; 0.9 &amp; 0 &amp; 0.07 &amp; 0.03  \\
\text{rabbit}  &amp; 0.1 &amp; 0.85 &amp; 0.05 &amp; 0 \\
\text{My}      &amp; 0.2 &amp; 0.01 &amp; 0.79 &amp; 0\\
\end{array}</annotation></semantics></math>
<p><br>
$</p>
<p>The matrix is read row-first, column-second i.e.&nbsp;the probability that the word ‘likes’ occurs after ‘rabbit’ is 0.85.</p>
<p>This tells us something about how the language is constructed. We know that if we hear an alien say ‘rabbit’, there is a very high chance that it will say ‘likes’ next. If we hear it say ‘likes’, there is a very high chance it will say ‘bananas’ next. There is also a very small chance it will say ‘My’ after ‘likes’, but it will never say ‘likes’ after ‘likes’. Therefore, <strong>to a first order</strong>, we can construct this matrix of probabilities that tells us what the next word in the language is going to be. In more formal terms, this is the <strong>stochastic matrix</strong> of a <strong>first-order Markov chain</strong>. It is first-order because the next word in the language only depends on the current word of the language.</p>
<p>But wait. Languages tend to have meaning when several words are used together. For example, in English, the word ‘cold’ refers to something whose molecules have a lower average kinetic energy than a reference object. However, the phrase ‘cold call’ means unsolicited phone calls typically made for business purposes. If you only know that the current word in the sentence is ‘cold’ and your probability matrix says that the word ‘call’ appears after ‘cold’ 70% of the time, you may say that the sentence ‘The water is cold.’ is incomplete and would complete it by saying ‘The water is cold call.’, which makes no sense<sup>10</sup>. What do we do?</p>
<p>The natural approach is to say, “I know combinations of words tend to change the meaning of a phrase<sup>11</sup>, but I don’t have any idea what constitutes a phrase in my unknown language, nor do I know if the ‘meaning’ of the sentence itself changes if a two words are present in adjacent positions<sup>12</sup>. Let me do the same thing I did for my first-order Markov chain. Instead of asking the talking dog the probabilities of the next word after my current word, I will look at the probabilities of the next word after my current word <strong>if another word is present in the sentence</strong>.”</p>
<p>Specifically, you can ask the talking dog the questions “If ‘rabbit likes’ is present in the sentence, what is the probability that the next word is ‘bananas’? What about ‘rabbit’, ‘My’, and ‘likes’? If ‘My rabbit’ is present, what is are the probabilities for the next word?” and construct the same matrix as we did above. Since we are looking at <em>every</em> pair of words, the number of rows of the matrix quickly grows in size. If there are 5 words in the language, the number of two-word pairs is 20 (obtained from $ 5 $) since the order matters. If there are 100 words, there are 4950 pairs. If there are 260,000 words (a quick Google search tells me that this is roughly the number of words in Italian) then there are 33799870000 pairs. And this is just for consecutive word pairings! If we attempt to look even further back i.e.&nbsp;three-word pairs, there will be even more. It is easy to see that the amount of space required to store this prediction matrix grows exponentially<sup>13</sup>.</p>
$<br>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal"></mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">rabbit</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">My</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">likes bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>T</mi><mi>R</mi><mi>E</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>4</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>T</mi><mi>R</mi><mi>E</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>5</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.02</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">My rabbit</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.826</mn></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>T</mi><mi>R</mi><mi>E</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>T</mi><mi>R</mi><mi>E</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>4</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mn>0.03</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">rabbit likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.024682</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.5</mn></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mi>π</mi><mrow><mn>10</mn><mi>e</mi></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">My bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.004</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.12</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.0018256151</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">and so on..</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{array}{r|cccc}
\text{} &amp; \text{bananas} &amp; \text{likes} &amp; \text{rabbit} &amp; \text{My} \\
\hline
\text{likes bananas} &amp; 0 &amp; \frac{TREE(4)}{TREE(5)} &amp; 0 &amp; 0.02 \\
\text{My rabbit}   &amp; 0 &amp; 0.826 &amp; \frac{TREE(3)}{TREE(4)} &amp; 0.03  \\
\text{rabbit likes}  &amp; 0.024682 &amp; 0.5 &amp; \frac{\pi}{10e} &amp; 0 \\
\text{My bananas}      &amp; 0.004 &amp; 0 &amp; 0.12 &amp; 0.0018256151\\
\text{and so on..}
\end{array}</annotation></semantics></math>
<p><br>
$</p>
<p>Given a sufficiently large prediction matrix containing all possible words and combinations of all possible lengths, we are able to predict the next word. Note that we have not said anything about <em>actually choosing the next word in this situation</em>, as this leads to problems. One problem is that we still do not know how to deal with cases where there is an equal chance of two words appearing after our current word. Let’s ignore this for now and focus on the biggest one: We want to avoid actually constructing any such matrix. Let’s try another trick. Let’s say, “The next word in a sentence is easier to predict if another word appears before the current word, <strong>but not necessarily directly before it</strong>. It may happen sometimes, but there is no reason why it should be like this. Here is my hypothesis. I think that it is easier to predict the next word in the sentence given a probability matrix containing all possible combinations of words <strong>where the second word is the current word</strong>.” This would look something like:</p>
$<br>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal"></mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">rabbit</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">My</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">likes bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.01</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">My bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.03</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">rabbit bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.00000023</mn></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mi>e</mi><mi>π</mi></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">bananas bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>0.004</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.046826</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.12</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.5151</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">and so on...</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{array}{r|cccc}
\text{} &amp; \text{bananas} &amp; \text{likes} &amp; \text{rabbit} &amp; \text{My} \\
\hline
\text{likes bananas} &amp; 0 &amp; 0 &amp; 0 &amp; 0.01 \\
\text{My bananas}   &amp; 0 &amp; 0 &amp; 0 &amp; 0.03  \\
\text{rabbit bananas}  &amp; 0.1 &amp; 0.00000023 &amp; \frac{e}{\pi} &amp; 0 \\
\text{bananas bananas}      &amp; 0.004 &amp; 0.046826 &amp; 0.12 &amp; 0.5151\\
\text{and so on...}
\end{array}</annotation></semantics></math>
<p><br>
$</p>
<p>This is much better to work with. Note that this is no longer a representation of a Markov chain, as we cannot simply look at the row corresponding to the current word and predict the next one. What can we do instead? We can say: “Okay, let’s say that these probabilities represent how much these pairs contribute to the next word in the sequence. We call these probabilities as <strong>votes</strong> and to predict the next word, we can <strong>sum over each column</strong> and compare these sums to determine the next word.” This is good, because now we are capturing <strong>long-range/skip dependencies</strong> in the language/sequence. Each row now represents one of many <strong>features</strong> that can describe our sequence at a particular point.</p>
<p>This is more clearly illustrated when you have, say, only two possible sentences in the language, but the main takeaway from actually doing this task for a set vocabulary and finite amount of sentences in the language is the observation that <strong>many elements in this probability matrix do not matter</strong>. They can either be so small that they are practically zero, or something like 0.5, which means that the next word is equally likely to appear regardless of the sentence, so it may not matter too much. What we are really interested in are elements we can distinguish. For example, suppose that the two sentences that were possible in our language were ‘My rabbit likes bananas’ and ‘My bananas likes rabbit’. If we had the incomplete sentence ‘My rabbit likes’, then we could ask the talking dog to give us this matrix, and what we would see is that the matrix has a large number of zeros but a 1 for ‘bananas’, enabling us to do this sum-over-columns technique to accurately predict the next word, even with a deep dependency. To be fair, this example is a bit contrived and longer sentences would illustrate the point much more easily.</p>
<p>This is still pretty bad. Real languages have a large number of words. Our talking dog could have only been around aliens who lived on a certain continent of the alien planet, which led to them developing their own dialect. If you think about it for just a little bit, it is easy to see that this sum-over-columns approach can end up telling us that the next word in the incomplete sentence ‘Japan is east of’ can be ‘China’, with a vote total of 2339, and ‘Mongolia’, with a vote total of 2340. Sure, we can still pick ‘Mongolia’ as the next word, but such a small difference can naturally be induced by statistical noise, unknowingly biased probability matrices, and other factors (such as us messing up the addition!). Are there ways to overcome this?</p>
<p>One approach is to modify the values in the columns before you sum them up, in a way that allows us to differentiate between them even more. One way to do this is to simply sum all the values and divide each value by the sum, to get a fractional representation. This is not very helpful - it preserves the same relation between the numbers in terms of scaling. Converting a column of [1,2,3] to [0.1666, 0.3334, 0.5] preserves the scaling. To overcome this, we utilize the <a href="https://en.wikipedia.org/wiki/Independence_of_irrelevant_alternatives">independence from irrelevant alternatives</a> axiom of decision theory, which states that irrelevant choices should not affect the relative probability of choosing between the things you really want to choose between. In mathematical terms, this means that if you have a set of numbers <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">x \in X</annotation></semantics></math> and you want to decide between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_1</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>2</mn></msub><annotation encoding="application/x-tex">x_2</annotation></semantics></math> but <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>3</mn></msub><mi>.</mi><mi>.</mi><mi>.</mi><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_3...x_n</annotation></semantics></math> are small values that are affecting your confidence, you can suppress <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>3</mn></msub><mi>.</mi><mi>.</mi><mi>.</mi><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_3...x_n</annotation></semantics></math> by replacing each variable in the following way:</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>→</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>=</mo><mi>n</mi></mrow></msubsup><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">x_i\rightarrow\frac{e^{x_i}}{\sum_{i=1}^{i=n}e^{x_i}}</annotation></semantics></math></p>
<p>This is the famous <strong>softmax</strong> function which is more or less used to convert a probability distribution to another probability distribution<sup>14</sup>. The important thing is that the softmax function suppressed irrelevant values (as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mi>k</mi></msup><mo>→</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">e^k \rightarrow 1</annotation></semantics></math> as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">k \rightarrow 0</annotation></semantics></math>).</p>
<p>However, the softmax function is also not applicable to our scenario. Suppose we did actually end up converting the votes to a probability distribution and summing them. What would it actually look like? Let’s do an example below:</p>
$
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.5</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>→</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.15706</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.258948</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.426933</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.15706</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{pmatrix}
0 \\ 0.5 \\ 1 \\ 0
\end{pmatrix}
\rightarrow
\begin{pmatrix}
0.15706 \\ 0.258948 \\ 0.426933 \\ 0.15706
\end{pmatrix}</annotation></semantics></math>
<p>$</p>
<p>The sum of the softmaxed vector elements is 1. This is correct, because we did just convert it to a probability distribution. So this approach, while it did ‘suppress’ the smaller values, does not actually help us with voting. What can we do?</p>
<p>“Okay,” we say. “Let’s do something else. Instead of attempting to modify every value, let’s just discard the values that aren’t important<sup>15</sup>. First, let’s look at how to extract specific features from our matrix. We know one-hot encoded vectors pull relevant rows/columns out of the matrix, so let’s make a one-hot encoded vector to pull out the relevant features in the matrix in the following way. We construct a vector initially filled with zeros featuring all possible pairs in sentence where the second word is the current word, and the first word has all other words (possibly including the current word, depending on the dimensions of our matrix). Then, if the first word appears before the current word in the sentence, set that element to 1. This vector allows us to pull out the features of our probability matrix that are ‘active’ until that current point.”</p>
<p>This would look like:</p>
$<br>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtable><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">My likes</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">rabbit likes</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">bananas likes</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd></mtr></mtable><mspace width="1.0em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{aligned}
&amp;\text{My likes}\\
&amp;\text{rabbit likes}\\
&amp;\text{bananas likes}\\
\end{aligned}
\quad
\begin{pmatrix}
1 \\ 1 \\ 0
\end{pmatrix}</annotation></semantics></math>
^<br>
$ $<br>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal"></mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">bananas</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">rabbit</mtext></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">My</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">My likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">rabbit likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.03</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">bananas likes</mtext></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.00000023</mn></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mi>e</mi><mi>π</mi></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">and so on...</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{array}{r|cccc}
\text{} &amp; \text{bananas} &amp; \text{likes} &amp; \text{rabbit} &amp; \text{My} \\
\hline
\text{My likes} &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
\text{rabbit likes}   &amp; 1 &amp; 0 &amp; 0 &amp; 0.03  \\
\text{bananas likes}  &amp; 1 &amp; 0.00000023 &amp; \frac{e}{\pi} &amp; 0 \\
\text{and so on...}
\end{array}</annotation></semantics></math>
<p><br>
$</p>
<p>Note the transpose sign. We can see that the matrix multiplication will <strong>suppress</strong> those elements in the pulled out feature vectors where pairs taking into account words appearing after the current word in the sequence will be suppressed i.e.&nbsp;we cannot use knowledge of the entire sentence to predict the next sentence. We have now ‘suppressed the future’, but we still need to figure out what feature elements in our sequence are important. This is still an unknown, but what we can do is use <em>another</em> one-hot encoded vector to multiply this suppressed vector, to suppress even more. That is: we can compute the <strong>pairwise product</strong> to return a vector after multiplying our two vectors. Where can we get this second one-hot encoded vector? Let’s assume that the talking dog gave this to us. The point is that if we manage to suppress information then our voting becomes much stronger, as a lot of elements will be 0. The trick is now to find out how to create this second vector so that we suppress irrelevant information.</p>
<p>Incidentally, the second form of suppression is the idea behind <strong>attention</strong>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb5-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> List, Dict, Tuple</span>
<span id="cb5-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> random</span>
<span id="cb5-4"></span>
<span id="cb5-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#remember that the english sentence is "My rabbit likes bananas"</span></span>
<span id="cb5-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> generate_biased_probability_matrix(size: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb5-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb5-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Function to generate a square probability matrix where each row and column </span></span>
<span id="cb5-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    has one value significantly higher than the others.</span></span>
<span id="cb5-10"></span>
<span id="cb5-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Input:</span></span>
<span id="cb5-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        size: The number of rows and columns in the square matrix</span></span>
<span id="cb5-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Output:</span></span>
<span id="cb5-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        biased_matrix: A 2-D np.ndarray where each row and column has one high-probability value</span></span>
<span id="cb5-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb5-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#the technique is to generate a uniform matrix and randomly assign biased high probability values in each row and column</span></span>
<span id="cb5-17">    biased_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.uniform(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, (size, size))</span>
<span id="cb5-18">    </span>
<span id="cb5-19">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#generate high probability values for each row and column</span></span>
<span id="cb5-20">    high_probabilities<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.uniform(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>size)</span>
<span id="cb5-21">    </span>
<span id="cb5-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#ghuffle indices to randomly distribute the high probabilities across columns</span></span>
<span id="cb5-23">    indices<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.arange(size)</span>
<span id="cb5-24">    np.random.shuffle(indices)</span>
<span id="cb5-25">    </span>
<span id="cb5-26">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#assign one high probability per row and column</span></span>
<span id="cb5-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(size):</span>
<span id="cb5-28">        biased_matrix[i, indices[i]]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>high_probabilities[i]</span>
<span id="cb5-29">    </span>
<span id="cb5-30">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#normalize each row to sum to 1</span></span>
<span id="cb5-31">    biased_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>biased_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>biased_matrix.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb5-32">    </span>
<span id="cb5-33">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> biased_matrix</span>
<span id="cb5-34"></span>
<span id="cb5-35">size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb5-36">example_probability_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>generate_biased_probability_matrix(size)</span>
<span id="cb5-37"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"As an example, a second-order probability matrix with skip dependencies given to us by the talking dog can be this:"</span>)</span>
<span id="cb5-38"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(example_probability_matrix)</span>
<span id="cb5-39"></span>
<span id="cb5-40"></span>
<span id="cb5-41"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> softmax(numbers: List[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span>List[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>]:</span>
<span id="cb5-42">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb5-43"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Function to softmax a set of numbers</span></span>
<span id="cb5-44"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Input:</span></span>
<span id="cb5-45"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        numbers: a list of integers</span></span>
<span id="cb5-46"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Output:</span></span>
<span id="cb5-47"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        The list, softmaxed</span></span>
<span id="cb5-48"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb5-49"></span>
<span id="cb5-50">    exponential_list<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.exp(numbers)</span>
<span id="cb5-51">    softmaxed_numbers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[np.exp(number)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(exponential_list) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> number <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> numbers]</span>
<span id="cb5-52">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> softmaxed_numbers</span>
<span id="cb5-53"></span>
<span id="cb5-54"></span>
<span id="cb5-55"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> digram_one_hot_encoding(sentence: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, tokens: List[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>], index_of_word: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tuple[np.ndarray, np.ndarray]:</span>
<span id="cb5-56">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb5-57"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Generate one-hot encoding vectors for digrams based on a user-defined index.</span></span>
<span id="cb5-58"></span>
<span id="cb5-59"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Input:</span></span>
<span id="cb5-60"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        sentence: Original sentence (used for context if needed).</span></span>
<span id="cb5-61"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        tokens: List of words (tokens) in the sentence.</span></span>
<span id="cb5-62"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        index_of_word: Index of the target word in the tokens list. Zero-indexed</span></span>
<span id="cb5-63"></span>
<span id="cb5-64"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Output:</span></span>
<span id="cb5-65"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        A tuple containing:</span></span>
<span id="cb5-66"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - A NumPy array of digrams (other words paired with the target word).</span></span>
<span id="cb5-67"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - A 1D NumPy array where each element is 1 if the other word appears before the target word, 0 otherwise.</span></span>
<span id="cb5-68"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb5-69">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> index_of_word<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> index_of_word<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(tokens):</span>
<span id="cb5-70">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">raise</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">ValueError</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Invalid user-defined index. Must be within the range of the tokens list."</span>)</span>
<span id="cb5-71"></span>
<span id="cb5-72">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#extract the target word. This of course assumes that the tokenization is sequential, but for illustrative purposes, it is fine</span></span>
<span id="cb5-73">    target_word <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokens[index_of_word]</span>
<span id="cb5-74"></span>
<span id="cb5-75">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#generate digrams and the one-hot vector. The idea is that if the word appears before our word then set the index to 1, else 0</span></span>
<span id="cb5-76">    digrams<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>token<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">,</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>target_word<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> idx,token <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(tokens) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> idx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span>index_of_word]</span>
<span id="cb5-77">    one_hot_vector<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> idx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span>index_of_word <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> idx <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(tokens)) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> idx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span>index_of_word],dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>)</span>
<span id="cb5-78"></span>
<span id="cb5-79">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.array(digrams), one_hot_vector</span>
<span id="cb5-80"></span>
<span id="cb5-81"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Our sentence is:"</span>, english_sentence)</span>
<span id="cb5-82"></span>
<span id="cb5-83"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#lets take index 2 ('likes' in "My rabbit likes bananas")</span></span>
<span id="cb5-84">index_of_word<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb5-85">incomplete_sentence<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>.join([word <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> index,word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(english_tokenized_sentence) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span>index_of_word<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb5-86"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"We want an incomplete sentence. Our generation task is to predict the next word in:"</span>, incomplete_sentence)</span>
<span id="cb5-87"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#generate the digrams (word pairs)</span></span>
<span id="cb5-88">digrams,digram_onehot_vector<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>digram_one_hot_encoding(english_sentence,english_tokenized_sentence,index_of_word)</span>
<span id="cb5-89"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Digrams:"</span>)</span>
<span id="cb5-90"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(digrams)</span>
<span id="cb5-91"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"One-hot vector (without any future dependency):"</span>)</span>
<span id="cb5-92"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(digram_onehot_vector)</span>
<span id="cb5-93"></span>
<span id="cb5-94"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#next, we create the attention mask by hand. specifically, we generate a ones vector equal to the size of the number of words in our sentence</span></span>
<span id="cb5-95"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#then we randomly pick 2</span></span>
<span id="cb5-96">attention_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.ones((<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(whitespace_tokenizer(incomplete_sentence))), dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>)</span>
<span id="cb5-97">zero_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.choice(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(whitespace_tokenizer(incomplete_sentence)), size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>random.randrange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(whitespace_tokenizer(incomplete_sentence))), replace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb5-98">attention_mask[zero_indices]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb5-99"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Example attention mask: "</span>, attention_mask.T)</span>
<span id="cb5-100"></span>
<span id="cb5-101"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Attention applied to the non-future dependency capturing one-hot vector:"</span>, attention_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>digram_onehot_vector)</span></code></pre></div></div>
<pre><code>As an example, a second-order probability matrix with skip dependencies given to us by the talking dog can be this:
[[0.03888508 0.03133871 0.88147493 0.04830127]
 [0.89819378 0.03108927 0.03605569 0.03466125]
 [0.02927141 0.88815681 0.02795252 0.05461926]
 [0.01763851 0.04167578 0.05967272 0.881013  ]]
Our sentence is: My rabbit likes bananas
We want an incomplete sentence. Our generation task is to predict the next word in: My rabbit likes
Digrams:
['My,likes' 'rabbit,likes' 'bananas,likes']
One-hot vector (without any future dependency):
[1 1 0]
Example attention mask:  [0 0 1]
Attention applied to the non-future dependency capturing one-hot vector: [0 0 0]</code></pre>
</section>
<section id="what-attention-does" class="level3">
<h3 class="anchored" data-anchor-id="what-attention-does">What attention does</h3>
<p>We have so far our non-future dependent feature vector. We have used it so far in conjunction with the probability matrix to predict the next step. If we want to suppress the feature vector with attention, does it make sense to use <em>another matrix</em> in the same way? Let’s assume that we have a bunch of attention masks/vectors. We can stack them either vertically or horizontally (depending on how exactly we want to implement our lookup) and generate a <em>matrix of attention masks</em>. We can then send our feature vector through the attention matrix and then send the result of that product into our probability matrix to predict the next word.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> send_vector_through_two_matrices(vector: np.ndarray, probability_matrix: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb7-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb7-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Function to send an input vector through an attention matrix and then a probability matrix</span></span>
<span id="cb7-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Inputs:</span></span>
<span id="cb7-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        vector: a 1D NumPy ndarray</span></span>
<span id="cb7-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Output:</span></span>
<span id="cb7-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        a 1D NumPy ndarray of the same length as the input after being sent through two matrices</span></span>
<span id="cb7-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb7-9"></span>
<span id="cb7-10">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#the idea is to generate a 1d array of ones, replace half of the elements with 0, and shuffle and reshape it</span></span>
<span id="cb7-11"></span>
<span id="cb7-12">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Your input vector is:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, vector)</span>
<span id="cb7-13"></span>
<span id="cb7-14">    total_elements<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>vector.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb7-15">    half<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>total_elements<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb7-16">    flat<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.ones(total_elements, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>)</span>
<span id="cb7-17">    flat[:half]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb7-18">    np.random.shuffle(flat)</span>
<span id="cb7-19">    example_attention_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>flat.reshape((vector.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>],vector.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]))</span>
<span id="cb7-20"></span>
<span id="cb7-21">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"An example attention matrix is:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, example_attention_matrix)</span>
<span id="cb7-22"></span>
<span id="cb7-23">    result_1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(vector, example_attention_matrix)</span>
<span id="cb7-24">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"After multiplying, you get:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>,result_1)</span>
<span id="cb7-25"></span>
<span id="cb7-26">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"After multiplying the result with the probability matrix, you get"</span>, np.matmul(result_1, probability_matrix))</span>
<span id="cb7-27"></span>
<span id="cb7-28">example_probability_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.rand(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(digram_onehot_vector), <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(digram_onehot_vector))</span>
<span id="cb7-29"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"An example probability matrix is:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, example_probability_matrix)</span>
<span id="cb7-30"></span>
<span id="cb7-31">send_vector_through_two_matrices(digram_onehot_vector, example_probability_matrix)</span></code></pre></div></div>
<pre><code>An example probability matrix is:
 [[0.6600921  0.31488209 0.01236613]
 [0.51302175 0.04638934 0.92027961]
 [0.38042472 0.01756476 0.29279993]]
Your input vector is:
 [1 1 0]
An example attention matrix is:
 [[1 1 0]
 [1 0 1]
 [0 0 1]]
After multiplying, you get:
 [2 1 1]
After multiplying the result with the probability matrix, you get [2.21363067 0.69371827 1.2378118 ]</code></pre>
</section>
<section id="reconstructing-word-pairs-from-encoded-vectors" class="level3">
<h3 class="anchored" data-anchor-id="reconstructing-word-pairs-from-encoded-vectors">Reconstructing word pairs from encoded vectors</h3>
<p>What do we do with the result of the attention step? Sure, we have a vector that has encoded word pairs (a second-order model), but we don’t yet have a way to deconstruct that vector back into a word pair. How do we do this? So far, matrix multiplication has enabled us to encode sentences into vectors and selectively mask the irrelevant word pairs. Can we apply matrix multiplication to <em>decode</em> a word pair? The answer is yes. Matrix multiplications are in fact what <strong>neural networks</strong> do.</p>
<section id="neural-networks" class="level4">
<h4 class="anchored" data-anchor-id="neural-networks">Neural Networks</h4>
<p>Neural networks are a deep learning architecture based on the neuron-synapse structure of the human brain. Neural networks consist of a series of blocks called artificial neurons (hereafter just ‘neuron’) stacked vertically in layers. Each neuron has the possibility to receive an input and pass along an output to another neuron. To decide whether it passes along an output, a neuron sums up all of its inputs (which are weighted by the value of the connection along which the output travels) and applies a function, called an <em>activation function</em>, to that sum. Depending on the result of the activation function, the neuron sends an output to one or more neurons depending on how many it connects to. Mathematically, passing data through a neuron is equivalent to applying the mathematical function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>i</mi><mo>=</mo><mi>n</mi></mrow></msubsup><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\sum_{i=0}^{i=n} w_i x_i)</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> is the activation function, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_i</annotation></semantics></math> is the weight along an input path, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics></math> the actual value being sent along that input path.</p>
<p>Neural networks are equivalent to matrix multiplication. Why is this so? Suppose there are two layers in our neural network. The first layer has 3 neurons, and the second layer has two neurons. Let’s name the latter two <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding="application/x-tex">n_1</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding="application/x-tex">n_2</annotation></semantics></math>. Also, let’s assume that each neuron in the first layer sends is connected to each neuron in the second layer. Therefore, the outputs of the first layer are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>x</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">x_1,x_2,x_3</annotation></semantics></math> and the weights of the paths along which they are sent are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>11</mn></msub><mo>,</mo><msub><mi>w</mi><mn>12</mn></msub><mo>,</mo><msub><mi>w</mi><mn>21</mn></msub><mo>,</mo><msub><mi>w</mi><mn>22</mn></msub><mo>,</mo><msub><mi>w</mi><mn>31</mn></msub><mo>,</mo><msub><mi>w</mi><mn>32</mn></msub></mrow><annotation encoding="application/x-tex">w_{11}, w_{12}, w_{21}, w_{22}, w_{31}, w_{32}</annotation></semantics></math> for each neuron-neuron path.</p>
<p>The input to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding="application/x-tex">n_1</annotation></semantics></math> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>11</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>21</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><msub><mi>w</mi><mn>31</mn></msub><msub><mi>x</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">w_{11}x_1+w_{21}x_2+w_{31}x_3</annotation></semantics></math>, and the input to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding="application/x-tex">n_2</annotation></semantics></math> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>12</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>22</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>32</mn></msub><msub><mi>x</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">w_{12}x_1+w_{22}x_1+w_{32}x_3</annotation></semantics></math>. Writing these out in the form of a system of linear expressions:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>w</mi><mn>11</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>21</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><msub><mi>w</mi><mn>31</mn></msub><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>w</mi><mn>12</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>22</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>32</mn></msub><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
w_{11}x_1+w_{21}x_2+w_{31}x_3 \\
w_{12}x_1+w_{22}x_1+w_{32}x_3
\end{aligned}</annotation></semantics></math></p>
<p>it is easy to see that this is in fact a matrix multiplication:<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">WX</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>w</mi><mn>11</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>w</mi><mn>21</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>w</mi><mn>31</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>w</mi><mn>21</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>w</mi><mn>22</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>w</mi><mn>32</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>X</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">W=\begin{pmatrix}w_{11}&amp;w_{21}&amp;w_{31}\\w_{21}&amp;w_{22}&amp;w_{32}\end{pmatrix}, X=\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}</annotation></semantics></math>. Each layer also tends to have a <strong>bias</strong> term, so the input to a layer can be represented as the equation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mi>X</mi><mo>+</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">WX+B</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> is the bias matrix (usually a column vector containing the same value). The activation function is applied to this resultant vector. This means that <strong>the output of a layer of a neural network can be represented by a vector</strong>.</p>
<p>#### Properties of neural networks</p>
<ol type="1">
<li>Neural networks are <strong>universal function approximators</strong>. This means that given a large-enough network with nonlinear activation functions, neural networks can model <strong>any</strong> mapping between elements of a domain <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> and a codomain <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>. However, this does NOT tell us how many neurons and layers we need or what the activation function is.</li>
<li>Neural networks can model nonlinear relationships between elements. While the discussion of linear decision boundaries is beyond the scope of this tutorial, it is enough to know that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, the activation function, is usually chosen to be something like <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> or the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a>.</li>
<li>Since neural networks are just matrix multiplication, they are extremely fast to train on computers<sup>16</sup></li>
</ol>
<p>#### Activation functions</p>
<p>If the activation function is linear (such as a simple multiplier <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>2</mn><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x)=2x</annotation></semantics></math>), the neural network cannot learn linear relationships, no matter how big you make it and how long you train it for. Nonlinear activation functions are necessary to learn nonlinear relationships i.e.&nbsp;relationships between two variables that cannot be explained by a matrix multiplication (attention is linear!). Activation functions like ReLU and the sigmoid function are chosen not only because they are easy to compute, but because of a certain requirement explained below.</p>
</section>
<section id="training-a-neural-network" class="level4">
<h4 class="anchored" data-anchor-id="training-a-neural-network">Training a neural network</h4>
<p>Since each layer of a neural network can be expressed as a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, a neural network can be thought of as a large composite function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>W</mi><mn>1</mn></msub><msub><mi>f</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>.</mi><mi>.</mi><mi>.</mi><msub><mi>f</mi><mi>n</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>W</mi><mi>n</mi></msub><msub><mi>X</mi><mi>n</mi></msub><mo>+</mo><msub><mi>B</mi><mi>N</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mi>.</mi><mi>.</mi><mo>+</mo><msub><mi>B</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mi>B</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_1(W_1f_2(...f_n(W_nX_n+B_N)...+B_1)+B_0)</annotation></semantics></math>. Given a training set of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">{(x_i,y_i)}</annotation></semantics></math> pairs, the loss of the model is a <em>cost function</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">C(y_i,g(x_i))</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(x_i)</annotation></semantics></math> is the prediction of the neural network (the large composite function defined above) for the input variable <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics></math>. We want to minimize this cost function, as it means our neural network has learned the relationship between the input and output variables. This is done with an algorithm called <strong>backpropagation</strong>.</p>
<p>Backpropagation is an algorithm that utilizes the technique of gradient descent - given a cost function, we calculate its gradient with respect to the weights and biases of the neural network. According to the learning rate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>, gradient descent updates the weights and biases of the neural network according to the rule <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mi>/</mi><msub><mi>B</mi><mtext mathvariant="normal">next</mtext></msub><mo>=</mo><mi>W</mi><mi>/</mi><msub><mi>B</mi><mtext mathvariant="normal">current</mtext></msub><mo>−</mo><mi>α</mi><mfrac><mrow><mi>∂</mi><mi>C</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>,</mo><mi>W</mi><mi>/</mi><msub><mi>B</mi><mtext mathvariant="normal">current</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>∂</mi><mi>W</mi><mi>/</mi><mi>B</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">
W/B_{\text{next}}=W/B_{\text{current}}-\alpha\frac{\partial C(X,W/B_{\text{current}})}{\partial W/B}
</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>/</mi><annotation encoding="application/x-tex">/</annotation></semantics></math> is read as ‘OR’. We <em>subtract</em> the gradient because the gradient denotes the direction of maximum increase, so the direction of maximum decrease would be the direction opposite to it. We apply this many times (this is therefore a <strong>greedy</strong> algorithm) to find the local minimum of the function i.e.&nbsp;the values of all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math> and all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> where the cost function is minimized. After each step of updating the gradients, we have to compute the prediction of the network again, in order to prepare for the next step. This is called the <strong>forward pass</strong> or <strong>forward step</strong> through the network, and must be computed repeatedly, making the process a back-and-forth.</p>
</section>
<section id="calculating-backpropagation" class="level4">
<h4 class="anchored" data-anchor-id="calculating-backpropagation">Calculating backpropagation</h4>
<p>Let us consider a neural network set up in the following way. We have <strong>3</strong> input neurons (that is, 3 input variables) and <strong>1</strong> hidden layer with 4 neurons, and <strong>one</strong> output neuron. For the sake of this example, assume that every neuron in one layer is connected to every neuron in the next layer and every neuron in the previous layer. Such a neural network is called a <strong>fully-connected neural network</strong>.</p>
<p>We have already seen how matrices can represent the input to a layer. Let’s represent the output of a layer by a vector after an activation function is applied to each neuron<sup>17</sup>. We will now define several variables that mathematically represent each layer. For each layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>, we have:</p>
<p>$$ n_l \</p>
<p>w_l n_{l} n{l-1} \</p>
<p>b_l n_l \</p>
<p>a_l \</p>
<p>z_l a_l b_l \</p>
<p>g_l $$</p>
<p>We can write down some straightforward formluae after these definitions.</p>
<p>$$ z_l=w_la_l+b_l\</p>
<p>a_l=g_l(z_l)\ $$</p>
<p>The next question is choosing an appropriate cost function for our task. Let us think about our task for a moment. Since we have been using probabilities all along to predict the next word in our sequence, it is appropriate to use a cost function that tells us how good our probability prediction is. The classical cost function that is used to explain backpropagation is the <strong>squared error function</strong>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">real value</mtext><mo>−</mo><mtext mathvariant="normal">predicted value</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><annotation encoding="application/x-tex">(\text{real value}-\text{predicted value})^2</annotation></semantics></math>. This function is natural because it is simply the difference between what we predict and what the truth is, and it is squared for many reasons such as being the variance of the unbiased estimator (if used in its mean-square form) and also being easily differentiable<sup>18</sup>. But we effectively want to measure the difference between a <em>predicted probability distribution</em> and the <em>real probability distribution</em>, as we are predicting the next word in a sentence. This requires having a maximum likelihood estimate of the parameters, and when working with Bernoulli-distributed variables (such as one-hot encoded vectors) the <strong>cross-entropy loss</strong> function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>−</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mi>l</mi><mi>n</mi><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>l</mi><mi>n</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">-(yln\hat{y}+(1-y)ln(1-\hat{y}))</annotation></semantics></math> minimizes the maximum likelihood estimate<sup>19</sup>.</p>
<p>Let’s see how the derivative is calculated.</p>
<p>$$ = \[5pt]</p>
<p>= $$</p>
<p>by a simple application of the <a href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a>. This can easily be extended by observing that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mn>3</mn></msub><annotation encoding="application/x-tex">z_3</annotation></semantics></math> is a function of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding="application/x-tex">a_2</annotation></semantics></math>, which is a function of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mn>2</mn></msub><annotation encoding="application/x-tex">z_2</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mn>2</mn></msub><annotation encoding="application/x-tex">z_2</annotation></semantics></math> is a function of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>2</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">w_2, b_2, a_1</annotation></semantics></math>.</p>
<p>$$ = \[5pt]</p>
<p>= $$</p>
<p>and similarly for the first (input) layer, named layer 0.</p>
<p>The next task is setting up a way to recursively calculate the derivative of the cost function for any arbitrary layer’s weight and bias. The general equation for this is</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mfrac><mrow><mi>∂</mi><mi>C</mi></mrow><mrow><mi>∂</mi><msub><mi>w</mi><mi>l</mi></msub></mrow></mfrac></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>C</mi></mrow><mrow><mi>∂</mi><msub><mi>z</mi><mi>l</mi></msub></mrow></mfrac><mfrac><mrow><mi>∂</mi><msub><mi>z</mi><mi>l</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>w</mi><mi>l</mi></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mfrac><mrow><mi>∂</mi><mi>C</mi></mrow><mrow><mi>∂</mi><msub><mi>b</mi><mi>l</mi></msub></mrow></mfrac></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>C</mi></mrow><mrow><mi>∂</mi><msub><mi>z</mi><mi>l</mi></msub></mrow></mfrac><mfrac><mrow><mi>∂</mi><msub><mi>z</mi><mi>l</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>b</mi><mi>l</mi></msub></mrow></mfrac></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\frac{\partial C}{\partial w_l}&amp;=\frac{\partial C}{\partial z_l}\frac{\partial z_l}{\partial w_l} \\[5pt]
\frac{\partial C}{\partial b_l}&amp;=\frac{\partial C}{\partial z_l}\frac{\partial z_l}{\partial b_l}
\end{aligned}</annotation></semantics></math></p>
<p>There are two observations we can make from this. The first is that it is straightfoward to numerically calculate the partial derivative for the last/output layer, and we can store this value in order to avoid repeated computation wherever possible. The second is that you need to calculate the change in the gradient for the last layer, then use that changed gradient for the layer before that one, and so on, ‘back-propagating’ the errors.</p>
<p>Let’s choose a nice activation function such as the sigmoid function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mi>−</mi><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma(x)=\frac{1}{1+e^{-x}}</annotation></semantics></math> for this. Let us precompute the partial derivative of the output layer, since we’ll be needing it. Note that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>⊙</mi><annotation encoding="application/x-tex">\odot</annotation></semantics></math> denotes the Hadamard, or element-wise product.</p>
<p>$$ == ’(z_o) \[5pt]</p>
<p>=-(-) \[5pt]</p>
<p>’(z_o)=a_l(1-a_l) \[5pt]</p>
<p>y \[5pt]</p>
<p>=a_o-y $$</p>
<p>This is a very nice result. It is now easy to see that for any inner layer, we can repeatedly apply the chain rule to derive the partial derivatives. If you go about doing this you end up with the following results:</p>
<p>$$ =w_{l+1}^T ’(z_l) \[5pt]</p>
<p>=a_{l-1} \[5pt]</p>
<p>a_{l-1}^T \[5pt]</p>
<p>=1 \[5pt]</p>
<p>= $$</p>
<p>A more complete derivation can be found <a href="https://towardsdatascience.com/deriving-backpropagation-with-cross-entropy-loss-d24811edeaf9">here</a>, but the fundamental idea is the same.</p>
<p>We can now implement this in Python and train the neural network from scratch.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">Neural Network Implementation in NumPy</span></span>
<span id="cb9-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">Inputs:</span></span>
<span id="cb9-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    None</span></span>
<span id="cb9-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">Outputs:</span></span>
<span id="cb9-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Fully functional neural network trained on synthetic data</span></span>
<span id="cb9-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-8"></span>
<span id="cb9-9"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb9-10"></span>
<span id="cb9-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> sigmoid(x: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb9-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Apply the sigmoid activation function element-wise</span></span>
<span id="cb9-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Inputs:</span></span>
<span id="cb9-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        x: a NumPy ndarray, the input array</span></span>
<span id="cb9-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Outputs:</span></span>
<span id="cb9-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        a NumPy ndarray with sigmoid applied element-wise</span></span>
<span id="cb9-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb9-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>np.exp(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>x))<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#sigmoid formula</span></span>
<span id="cb9-20"></span>
<span id="cb9-21"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> sigmoid_prime(x: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb9-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Compute the derivative of the sigmoid function element-wise</span></span>
<span id="cb9-24"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Inputs:</span></span>
<span id="cb9-25"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        x: a NumPy ndarray, the input array</span></span>
<span id="cb9-26"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Outputs:</span></span>
<span id="cb9-27"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        a NumPy ndarray with the derivative of sigmoid applied element-wise</span></span>
<span id="cb9-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb9-29">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> sigmoid(x)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>sigmoid(x))<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#sigmoid derivative formula</span></span>
<span id="cb9-30"></span>
<span id="cb9-31"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> NeuralNetwork:</span>
<span id="cb9-32">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-33"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Define a simple feedforward neural network</span></span>
<span id="cb9-34"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb9-35"></span>
<span id="cb9-36">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,architecture: np.ndarray):</span>
<span id="cb9-37">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-38"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        initializer for the neural network class</span></span>
<span id="cb9-39"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb9-40"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            architecture: a NumPy array representing the number of neurons in each layer</span></span>
<span id="cb9-41"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb9-42">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>architecture.size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#number of layers (excluding input layer)</span></span>
<span id="cb9-43">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>architecture<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#number of neurons in each layer</span></span>
<span id="cb9-44">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{}<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#dictionary to store weights, biases, and activations</span></span>
<span id="cb9-45"></span>
<span id="cb9-46">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#initialize weights and biases for each layer</span></span>
<span id="cb9-47">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb9-48">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(i)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n[i],<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n[i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#small random weights</span></span>
<span id="cb9-49">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(i)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.ones((<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n[i],<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#biases initialized to 1</span></span>
<span id="cb9-50">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'z'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(i)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.ones((<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n[i],<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#pre-activation values initialized to 1</span></span>
<span id="cb9-51">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(i)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.ones((<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n[i],<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#activations initialized to 1</span></span>
<span id="cb9-52">        </span>
<span id="cb9-53">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a0'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.ones((<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>],<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#input layer activation</span></span>
<span id="cb9-54">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#placeholder for cost value</span></span>
<span id="cb9-55">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.derivatives<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{}<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#dictionary to store derivatives</span></span>
<span id="cb9-56"></span>
<span id="cb9-57">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward_propagate(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,X: np.ndarray):</span>
<span id="cb9-58">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-59"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Perform forward propagation</span></span>
<span id="cb9-60"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb9-61"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            X: a column vector representing one training example</span></span>
<span id="cb9-62"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb9-63"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            None</span></span>
<span id="cb9-64"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb9-65">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a0'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#set input layer activation</span></span>
<span id="cb9-66">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> l <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb9-67">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'z'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.dot(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)],<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)]<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#W*a+b</span></span>
<span id="cb9-68">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sigmoid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'z'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)])<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#apply sigmoid activation</span></span>
<span id="cb9-69"></span>
<span id="cb9-70">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compute_cost(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,y: np.ndarray):</span>
<span id="cb9-71">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-72"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        function to compute the cost for one training example</span></span>
<span id="cb9-73"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb9-74"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            y: the true label for the input sample</span></span>
<span id="cb9-75"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb9-76"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            None</span></span>
<span id="cb9-77"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb9-78">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span>(y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>np.log(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L)])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>y)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>np.log(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L)]))<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#binary cross-entropy loss</span></span>
<span id="cb9-79"></span>
<span id="cb9-80">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compute_derivatives(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,y: np.ndarray):</span>
<span id="cb9-81">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-82"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        function to compute gradients for all parameters</span></span>
<span id="cb9-83"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb9-84"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            y: the true label for the input sample</span></span>
<span id="cb9-85"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb9-86"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            None</span></span>
<span id="cb9-87"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb9-88">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.derivatives[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dz'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>y<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#last layer gradient</span></span>
<span id="cb9-89">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.derivatives[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dW'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.dot(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.derivatives[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dz'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L)],<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)].T)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#last layer weights gradient</span></span>
<span id="cb9-90">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.derivatives[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'db'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.derivatives[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dz'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L)]<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#last layer bias gradient</span></span>
<span id="cb9-91"></span>
<span id="cb9-92">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> l <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb9-93">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.derivatives[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dz'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.dot(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)].T,<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.derivatives[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dz'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>sigmoid_prime(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'z'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)])<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#hidden layer gradient</span></span>
<span id="cb9-94">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.derivatives[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dW'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.dot(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.derivatives[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dz'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)],<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)].T)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#hidden layer weights gradient</span></span>
<span id="cb9-95">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.derivatives[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'db'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.derivatives[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dz'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)]<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#hidden layer bias gradient</span></span>
<span id="cb9-96"></span>
<span id="cb9-97">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> update_parameters(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,alpha: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>):</span>
<span id="cb9-98">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-99"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        function to update network parameters using gradient descent</span></span>
<span id="cb9-100"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb9-101"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            alpha: learning rate</span></span>
<span id="cb9-102"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb9-103"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            None</span></span>
<span id="cb9-104"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb9-105">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> l <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb9-106">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.derivatives[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dW'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)]<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#update weights</span></span>
<span id="cb9-107">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.derivatives[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'db'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(l)]<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#update biases</span></span>
<span id="cb9-108"></span>
<span id="cb9-109">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,x: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb9-110">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-111"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        function to predict the output for a given input</span></span>
<span id="cb9-112"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb9-113"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            x: a column vector representing one input sample</span></span>
<span id="cb9-114"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb9-115"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            a NumPy array representing the predicted output</span></span>
<span id="cb9-116"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb9-117">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.forward_propagate(x)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#perform forward propagation</span></span>
<span id="cb9-118">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.L)]<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#return output layer activation</span></span>
<span id="cb9-119"></span>
<span id="cb9-120">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,X: np.ndarray,Y: np.ndarray,num_iter: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>,alpha: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>):</span>
<span id="cb9-121">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-122"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        function to train the neural network</span></span>
<span id="cb9-123"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb9-124"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            X: a NumPy array where each row is a training example</span></span>
<span id="cb9-125"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            Y: a NumPy array of true labels</span></span>
<span id="cb9-126"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            num_iter: number of iterations</span></span>
<span id="cb9-127"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            alpha: learning rate</span></span>
<span id="cb9-128"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb9-129"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            None</span></span>
<span id="cb9-130"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb9-131">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_iter):</span>
<span id="cb9-132">            c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#cumulative cost</span></span>
<span id="cb9-133">            n_c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#correct predictions count</span></span>
<span id="cb9-134"></span>
<span id="cb9-135">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]):</span>
<span id="cb9-136">                x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X[i].reshape((X[i].size,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#reshape input to column vector</span></span>
<span id="cb9-137">                y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Y[i]<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#true label</span></span>
<span id="cb9-138">                <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.forward_propagate(x)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#forward propagation</span></span>
<span id="cb9-139">                <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.compute_cost(y)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#compute cost</span></span>
<span id="cb9-140">                <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.compute_derivatives(y)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#compute gradients</span></span>
<span id="cb9-141">                <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.update_parameters(alpha)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#update parameters</span></span>
<span id="cb9-142">                c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.parameters[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C'</span>]<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#accumulate cost</span></span>
<span id="cb9-143">                y_pred<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.predict(x)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#make prediction</span></span>
<span id="cb9-144">                y_pred<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(y_pred<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#convert probability to binary</span></span>
<span id="cb9-145">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> y_pred<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span>y:</span>
<span id="cb9-146">                    n_c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#correct prediction count</span></span>
<span id="cb9-147"></span>
<span id="cb9-148">            c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#average cost</span></span>
<span id="cb9-149">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Iteration:'</span>,<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>)</span>
<span id="cb9-150">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Cost:"</span>,c)</span>
<span id="cb9-151">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Accuracy:"</span>,(n_c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb9-152"></span>
<span id="cb9-153"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#generate synthetic data</span></span>
<span id="cb9-154">np.random.seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#reproducibility</span></span>
<span id="cb9-155">X<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.rand(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#200 samples, 7 features</span></span>
<span id="cb9-156">y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(X,axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5</span>).astype(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>).reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#labels based on sum of features</span></span>
<span id="cb9-157"></span>
<span id="cb9-158"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#split data into training and testing sets</span></span>
<span id="cb9-159">split_ratio<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#70% training data</span></span>
<span id="cb9-160">split_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>split_ratio)</span>
<span id="cb9-161">indices<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.arange(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb9-162">np.random.shuffle(indices)</span>
<span id="cb9-163"></span>
<span id="cb9-164">X_train,X_test<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X[indices[:split_index]],X[indices[split_index:]]</span>
<span id="cb9-165">y_train,y_test<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y[indices[:split_index]],y[indices[split_index:]]</span>
<span id="cb9-166"></span>
<span id="cb9-167"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#define architecture</span></span>
<span id="cb9-168">architecture<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#7 input features, 5 hidden neurons, 1 output</span></span>
<span id="cb9-169"></span>
<span id="cb9-170"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#initialize and train the neural network</span></span>
<span id="cb9-171">nn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>NeuralNetwork(architecture)</span>
<span id="cb9-172">nn.fit(X_train,y_train,num_iter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>)</span>
<span id="cb9-173"></span>
<span id="cb9-174"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#evaluate the model</span></span>
<span id="cb9-175">correct_predictions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb9-176"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(X_test.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]):</span>
<span id="cb9-177">    x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_test[i].reshape((X_test[i].size,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#reshape to column vector</span></span>
<span id="cb9-178">    y_true<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y_test[i]<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#true label</span></span>
<span id="cb9-179">    y_pred<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>nn.predict(x)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#prediction</span></span>
<span id="cb9-180">    y_pred<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(y_pred<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>).astype(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#convert to binary</span></span>
<span id="cb9-181">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> y_pred<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span>y_true:</span>
<span id="cb9-182">        correct_predictions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#count correct predictions</span></span>
<span id="cb9-183"></span>
<span id="cb9-184"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#calculate test accuracy</span></span>
<span id="cb9-185">test_accuracy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(correct_predictions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>X_test.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb9-186"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Test Accuracy:"</span>,test_accuracy)</span></code></pre></div></div>
<pre><code>Iteration: 0
Cost: [[0.72566273]]
Accuracy: 64.28571428571429
Iteration: 1
Cost: [[0.71523798]]
Accuracy: 66.42857142857143
Iteration: 2
Cost: [[0.70509586]]
Accuracy: 66.42857142857143
Iteration: 3
Cost: [[0.6864599]]
Accuracy: 67.85714285714286
Iteration: 4
Cost: [[0.65347682]]
Accuracy: 75.71428571428571
Iteration: 5
Cost: [[0.60664836]]
Accuracy: 82.14285714285714
Iteration: 6
Cost: [[0.55558826]]
Accuracy: 91.42857142857143
Iteration: 7
Cost: [[0.5072171]]
Accuracy: 93.57142857142857
Iteration: 8
Cost: [[0.46296521]]
Accuracy: 95.0
Iteration: 9
Cost: [[0.42292781]]
Accuracy: 96.42857142857143
Iteration: 10
Cost: [[0.38716204]]
Accuracy: 97.85714285714285
Iteration: 11
Cost: [[0.35557885]]
Accuracy: 97.85714285714285
Iteration: 12
Cost: [[0.32788988]]
Accuracy: 97.85714285714285
Iteration: 13
Cost: [[0.30368962]]
Accuracy: 97.85714285714285
Iteration: 14
Cost: [[0.28254309]]
Accuracy: 97.85714285714285
Iteration: 15
Cost: [[0.26403564]]
Accuracy: 97.85714285714285
Iteration: 16
Cost: [[0.24779326]]
Accuracy: 97.85714285714285
Iteration: 17
Cost: [[0.23348834]]
Accuracy: 98.57142857142858
Iteration: 18
Cost: [[0.22083889]]
Accuracy: 98.57142857142858
Iteration: 19
Cost: [[0.20960497]]
Accuracy: 98.57142857142858
Iteration: 20
Cost: [[0.19958386]]
Accuracy: 98.57142857142858
Iteration: 21
Cost: [[0.190605]]
Accuracy: 98.57142857142858
Iteration: 22
Cost: [[0.18252507]]
Accuracy: 98.57142857142858
Iteration: 23
Cost: [[0.17522359]]
Accuracy: 98.57142857142858
Iteration: 24
Cost: [[0.16859908]]
Accuracy: 98.57142857142858
Iteration: 25
Cost: [[0.16256584]]
Accuracy: 98.57142857142858
Iteration: 26
Cost: [[0.15705123]]
Accuracy: 98.57142857142858
Iteration: 27
Cost: [[0.15199344]]
Accuracy: 98.57142857142858
Iteration: 28
Cost: [[0.14733967]]
Accuracy: 98.57142857142858
Iteration: 29
Cost: [[0.14304459]]
Accuracy: 98.57142857142858
Iteration: 30
Cost: [[0.13906911]]
Accuracy: 98.57142857142858
Iteration: 31
Cost: [[0.13537942]]
Accuracy: 98.57142857142858
Iteration: 32
Cost: [[0.1319461]]
Accuracy: 98.57142857142858
Iteration: 33
Cost: [[0.12874349]]
Accuracy: 98.57142857142858
Iteration: 34
Cost: [[0.12574908]]
Accuracy: 98.57142857142858
Iteration: 35
Cost: [[0.12294312]]
Accuracy: 99.28571428571429
Iteration: 36
Cost: [[0.12030816]]
Accuracy: 99.28571428571429
Iteration: 37
Cost: [[0.11782877]]
Accuracy: 99.28571428571429
Iteration: 38
Cost: [[0.11549126]]
Accuracy: 99.28571428571429
Iteration: 39
Cost: [[0.11328345]]
Accuracy: 100.0
Iteration: 40
Cost: [[0.11119447]]
Accuracy: 100.0
Iteration: 41
Cost: [[0.1092146]]
Accuracy: 100.0
Iteration: 42
Cost: [[0.10733513]]
Accuracy: 100.0
Iteration: 43
Cost: [[0.10554823]]
Accuracy: 100.0
Iteration: 44
Cost: [[0.10384686]]
Accuracy: 100.0
Iteration: 45
Cost: [[0.10222465]]
Accuracy: 100.0
Iteration: 46
Cost: [[0.10067586]]
Accuracy: 100.0
Iteration: 47
Cost: [[0.09919528]]
Accuracy: 100.0
Iteration: 48
Cost: [[0.09777818]]
Accuracy: 100.0
Iteration: 49
Cost: [[0.09642027]]
Accuracy: 100.0
Test Accuracy: 93.33333333333333</code></pre>
<p>Here’s a neat observation. If a neural network is simple a two-layer network with non-linear activation, <em>it is simply a matrix multiplication to new inputs</em>. Therefore, we now have a way to <strong>learn</strong> that second-order probability matrix!</p>
<p>Given a neural network which can learn vector-vector relationships, it is easy to see that we can reconstruct our word-pair combinations from a vector that is the result of an attention step. Suppose we have a vector corresponding to the words ‘My’, ‘rabbit’, and ‘likes’, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}1\\1\\1 \end{pmatrix}</annotation></semantics></math>, the result of making it non-future-dependent is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}1\\1\\0 \end{pmatrix}</annotation></semantics></math>, and our attention step results in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}1\\0\\0 \end{pmatrix}</annotation></semantics></math>. We now pass this as <em>input</em> to a neural network and compute the output. The output will simply map our input vector to an output vector. The insight is that you can train the network to accurately map the result of our attention step to word pairs! The next obvious question is how to generate these word pairs. But before that, we need to notice that our neural network gets trained quickly when the input-output training sets have a small number of elements. It quickly becomes unwieldy when you think about practical languages, like the 260,000 Italian words mentioned above. This moves us on to our next topic, <strong>embeddings</strong>.</p>
</section>
</section>
<section id="embeddings" class="level3">
<h3 class="anchored" data-anchor-id="embeddings">Embeddings</h3>
<p>To make our neural network work well, we need a large amount of input-output data. This is impractical at the scale of even small, real languages - there are simply too many words. Generating one-hot encoding matrices by vertically stacking the vectors, even with techniques to store sparse matrices, is still impractical once we think about storing word pairs and triplets. We need some way to reduce these one-hot matrices in size so storing them becomes more efficient. This is the same problem we tried to solve with neural networks: converting an input vector into another vector. In our case, we want to convert a large vector to a smaller vector such that enough information is retained. This smaller vector will be called the <strong>embedding</strong> vector.</p>
<p>Based on all that we’ve seen so far, it is obvious that this conversion will be done with matrix multiplication. The question is how to make this new matrix. We can do the same thing we did before (training a neural network) or we can do something completely different. Here is an example. Suppose we want to embed ‘My’, ‘rabbit’, ‘likes’, and ‘bananas’ into 2 dimensions. We know that their one-hot encoding is an identity matrix, possibly with its columns shuffled. We can arbitrarily define a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">4 \times 2</annotation></semantics></math> matrix that will project this matrix into a smaller matrix. We can then say that column 1 (representing, say, ‘rabbit’) of the initial matrix is now replaced by column 1 of the new matrix. This is perfectly fine. But is this meaningful?</p>
<p>What do we want from a ‘good’ embedding? Broadly, a good embedding should be useful for practical tasks. There is no use in embedding words and making the transformer’s neural network harder to train. We might want to apply clustering algorithms to word embeddings to find out, for example, how many nouns there are in a large <em>corpus</em> (plural <em>corpora</em>) of text. We might also want to know what words are related in an unknown language. For someone trying to embed English, making sure that the embeddings for ‘rabbit’ and ‘hare’ are closer(i.e.&nbsp;their difference is closer to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mn>0</mn><mo accent="true">→</mo></mover><annotation encoding="application/x-tex">\vec{0}</annotation></semantics></math>) than the embeddings for ‘rabbit’ and ‘desk’ is important if training a model to explain what it says in pictures of lagomorphs in office environments. Embeddings should probably also capture <strong>context awareness</strong> - ‘hot dog’ must have a different embedding compared to both ‘hot’ and ‘dog’. They should also not be too low-dimensional; we might lose important information.</p>
<p>It is beyond the scope of this tutorial to discuss good embedding algorithms. Fortunately, there is a straightforward algorithm we can use to embed our four-word language. Let’s map them on the unit circle with a randomly generated matrix, as we have been doing so far.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb11-3"></span>
<span id="cb11-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#define words</span></span>
<span id="cb11-5">words<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'My'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rabbit'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'likes'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bananas'</span>]</span>
<span id="cb11-6"></span>
<span id="cb11-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#ensure that the words are sufficiently apart for better visibility-say 30 degrees</span></span>
<span id="cb11-8">angles<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.sort(np.random.choice(np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>np.pi,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">360</span>,endpoint<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>),<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(words),replace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>))</span>
<span id="cb11-9"></span>
<span id="cb11-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#compute unit circle coordinates</span></span>
<span id="cb11-11">unit_circle_vectors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.array([[np.cos(angle),np.sin(angle)] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> angle <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> angles])</span>
<span id="cb11-12"></span>
<span id="cb11-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#define initial one-hot vectors</span></span>
<span id="cb11-14">one_hot_vectors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.eye(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(words), dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>)</span>
<span id="cb11-15"></span>
<span id="cb11-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#plot embeddings</span></span>
<span id="cb11-17">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>))</span>
<span id="cb11-18"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i,word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(words):</span>
<span id="cb11-19">    x,y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>unit_circle_vectors[i]</span>
<span id="cb11-20">    plt.scatter(x,y,label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>word)</span>
<span id="cb11-21">    plt.text(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>,y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>,word,fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>)</span>
<span id="cb11-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#draw the arrow</span></span>
<span id="cb11-23">    plt.arrow(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,x,y,head_width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>,head_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>,fc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'blue'</span>,ec<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>,alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>)</span>
<span id="cb11-24"></span>
<span id="cb11-25"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Draw the unit circle</span></span>
<span id="cb11-26">theta<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>np.pi,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb11-27">circle_x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.cos(theta)</span>
<span id="cb11-28">circle_y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.sin(theta)</span>
<span id="cb11-29">plt.plot(circle_x,circle_y,color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gray'</span>,linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>)</span>
<span id="cb11-30"></span>
<span id="cb11-31">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"2D Embedding of a 4-Word Language on Unit Circle"</span>)</span>
<span id="cb11-32">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"X-axis"</span>)</span>
<span id="cb11-33">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Y-axis"</span>)</span>
<span id="cb11-34">plt.axhline(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gray'</span>,linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb11-35">plt.axvline(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gray'</span>,linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb11-36">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb11-37">plt.legend()</span>
<span id="cb11-38">plt.axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'equal'</span>)</span>
<span id="cb11-39">plt.show()</span>
<span id="cb11-40"></span>
<span id="cb11-41"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Print initial one-hot vectors and unit circle embeddings</span></span>
<span id="cb11-42"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Initial One-Hot Vectors:"</span>)</span>
<span id="cb11-43"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i,word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(words):</span>
<span id="cb11-44">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>word<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">:</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>one_hot_vectors[i]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb11-45"></span>
<span id="cb11-46"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Embedded Vectors on Unit Circle:"</span>)</span>
<span id="cb11-47"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i,word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(words):</span>
<span id="cb11-48">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>word<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">:</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>unit_circle_vectors[i]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://ksd3.github.io/blog/2024-12-07-transformers/images/transformers_13_0.png" class="img-fluid figure-img"></p>
<figcaption>png</figcaption>
</figure>
</div>
<pre><code>Initial One-Hot Vectors:
My:[1 0 0 0]
rabbit:[0 1 0 0]
likes:[0 0 1 0]
bananas:[0 0 0 1]

Embedded Vectors on Unit Circle:
My:[0.9961947  0.08715574]
rabbit:[-0.54463904 -0.83867057]
likes:[ 0.60181502 -0.79863551]
bananas:[ 0.99254615 -0.12186934]</code></pre>
<section id="giving-important-to-the-position-of-words-in-a-sentence-while-embedding-text" class="level4">
<h4 class="anchored" data-anchor-id="giving-important-to-the-position-of-words-in-a-sentence-while-embedding-text">Giving important to the position of words in a sentence while embedding text</h4>
<p>Let’s think a bit about our two-word non-future-dependent vectors. The only condition we have applied so far is that the value in the column matrix row wherever a word appears ahead of our current word should be 0. If a word appeared 1364 places before our current word but its value was deemed ‘important’ by attention, its corresponding row value would be 1. This is impractical. We know that it is unlikely that a word appearing 1364 places before the current word affects it. How can we quantify this?</p>
<p>The solution is to do it heuristically. Let’s figure out what exactly the task is. We have to add some additional information in a word’s embedding that denotes the position of the word in a sentence. This additional information is called the <strong>positional encoding</strong>. We want to satisfy a few criteria:</p>
<ol type="1">
<li>The encoding must be unique for each word in the sequence even if that word appears again. The sentence ‘My rabbit likes bananas but my friend’s rabbit doesn’t.’ should have different encoding values for the first and second occurrences of ‘rabbit’.</li>
<li>If we have to add positional encoding to sentences of different lengths, the ‘distance’ between two pieces of information should remain constant. This means that ‘My rabbit likes bananas. My friend’s rabbit does not like bananas.’ should encode the first and second occurrences of ‘rabbit’ in a way that the difference between the additional information added should be the same as the difference between ‘likes’ and ‘does’. This ensures that the two sentences are recognized as part of a ‘speech’.</li>
<li>We should be able to generalize to any sentence length easily with <em>bounded</em> and <em>deterministic</em> values (i.e.&nbsp;do not train a neural network).</li>
</ol>
<p>Essentially, we need to find a function whose codomain is a vector of the same size of the embedding that is:</p>
<ol type="1">
<li>Easy to compute</li>
<li>Periodic</li>
<li>Has bounded values.</li>
</ol>
<p>and iterate through the sentence, computing the function at the index of every word. To add the information to the word embedding, we can literally add the two vectors. This encodes positional information in the embedding.</p>
<p>A function that satisfies these criteria is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><mi>ℕ</mi><mo>→</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">f: \mathbb{N} \rightarrow \mathbb{R}^d</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="0.222em"></mspace><msub><mi>f</mi><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>ω</mi><mi>k</mi></msub><mo>⋅</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>i</mi><mo>=</mo><mn>2</mn><mi>k</mi></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>ω</mi><mi>k</mi></msub><mo>⋅</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>i</mi><mo>=</mo><mn>2</mn><mi>k</mi><mo>+</mo><mn>1</mn></mtd></mtr></mtable></mrow><mspace width="0.222em"></mspace></mrow><annotation encoding="application/x-tex">
\
f_i(t) = 
\begin{cases} 
\sin(\omega_k \cdot t), &amp; \text{if } i = 2k \\ 
\cos(\omega_k \cdot t), &amp; \text{if } i = 2k + 1 
\end{cases}
\
</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> is the number of rows in the column vector representation of the embedding vector, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> are simply ways to denote even and odd positions (i.e.&nbsp;the first row of the encoding vector is a sine, the second row is a cosine, the third row is a sine, and so on) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>=</mo><mfrac><mn>1</mn><msup><mn>10000</mn><mfrac><mrow><mn>2</mn><mi>k</mi></mrow><mi>d</mi></mfrac></msup></mfrac></mrow><annotation encoding="application/x-tex">w=\frac{1}{10000^{\frac{2k}{d}}}</annotation></semantics></math>. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math> has been chosen completely by guesswork. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> simply denotes the row number. Another good property is that since the encoding are periodic functions, you have also put in some information saying ‘the encoding of a word <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> places away from the current word is so-and-so’. Their periodicity implies that they can be represented as a linear combination of earlier encodings. I want to reiterate that this is a heuristic that works and theoretical justifications for this do not really exist. It works because you have differentiated between sentences such as ‘I just computed five times three plus two’ and ‘I just computed five plus three times two’ which have different underlying meanings.</p>
</section>
<section id="converting-embeddings-back-into-words" class="level4">
<h4 class="anchored" data-anchor-id="converting-embeddings-back-into-words">Converting embeddings back into words</h4>
<p>We finally discuss actually choosing the next word in the sequence. Suppose that have taken a sentence, tokenized it, converted to one-hot encoding, embedded these encodings, added position embeddings, and then trained a neural network to predict an output vector. The final step is to convert this output vector, <em>which is also an embedding</em>, back into a vector that represents the target vocabulary. We do not want to convert it back into a one-hot vector. How will you choose the next word?</p>
<p>Let’s do what is straightforward - multiply it with a matrix. This time, we are taking a smaller vector and making it a larger one. This comes with some caveats. We are making a column matrix bigger. If we want to make <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}1\\2\\3\end{pmatrix}</annotation></semantics></math> bigger, say to twice its size, how can we do it? Should we do <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}1\\0\\2\\0\\3\\0\end{pmatrix}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}0\\1\\2\\0\\3\\0\end{pmatrix}</annotation></semantics></math>, or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix}0\\0\\1\\0\\2\\3\end{pmatrix}</annotation></semantics></math>? We can set up a matrix to do any one of these transformations, but we cannot set up a matrix that does this for all possible input vectors. This is because you will be solving an overdetermined system of equations. We have no choice but to accept this, so we have to assume that even if we find a matrix that makes the values as close to zero as possible, they will never all be 0 for any practical case. Going back to our initial task (English to Italian), we will end up with a vector that looks something like this</p>
$<br>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtable><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">Dios</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">mio</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">mangiato</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">una</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">and so on...</mtext></mtd></mtr></mtable><mspace width="1.0em"></mspace><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.00005</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.25</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">...</mtext></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{aligned}
&amp;\text{Dios}\\
&amp;\text{mio}\\
&amp;\text{mangiato}\\
&amp;\text{una}\\
&amp;\text{and so on...}
\end{aligned}
\quad
\begin{bmatrix}
0.1 \\ 0.00005 \\ 0.25 \\ 0 \\ \text{...}
\end{bmatrix}</annotation></semantics></math>
<p><br>
$</p>
<p>How do we select the next word? We can certainly pick the one with the largest value, but this is not so good. Fortunately, we have <em>already</em> looked at a way to emphasize the right word - softmaxing! Softmaxing and picking the highest probability allows us to enhance the probability of the right word (and it will be high because we train the neural network in this way - remember that matrix multiplications are just two-layer neural networks) being picked. An added bonus is that the softmax function is differentiable.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb13-2"></span>
<span id="cb13-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#define words</span></span>
<span id="cb13-4">words<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'My'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rabbit'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'likes'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bananas'</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#ensure vocabulary is 4 words long</span></span>
<span id="cb13-5"></span>
<span id="cb13-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Tokenized sentence:"</span>, words)</span>
<span id="cb13-7"></span>
<span id="cb13-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#ensure that the words are sufficiently apart for better visibility-say 30 degrees</span></span>
<span id="cb13-9">angles<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.sort(np.random.choice(np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>np.pi,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">360</span>,endpoint<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>),<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(words),replace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>))</span>
<span id="cb13-10"></span>
<span id="cb13-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#compute unit circle coordinates (2D embeddings)</span></span>
<span id="cb13-12">embedding_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#set embedding dimension to 2</span></span>
<span id="cb13-13">unit_circle_vectors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.array([[np.cos(angle),np.sin(angle)] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> angle <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> angles])</span>
<span id="cb13-14"></span>
<span id="cb13-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#define initial one-hot vectors</span></span>
<span id="cb13-16">one_hot_vectors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.eye(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(words),dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>)</span>
<span id="cb13-17"></span>
<span id="cb13-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#function for positional encoding as defined in "Attention is all you need"</span></span>
<span id="cb13-19"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> positional_encoding(seq_len,d_model):</span>
<span id="cb13-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#initialize positional encoding matrix</span></span>
<span id="cb13-21">    pos_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.zeros((seq_len,d_model))</span>
<span id="cb13-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> pos <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(seq_len):</span>
<span id="cb13-23">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,d_model,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>):</span>
<span id="cb13-24">            pos_enc[pos,i]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.sin(pos<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>d_model)))</span>
<span id="cb13-25">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span>d_model:  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#check to prevent index out of range</span></span>
<span id="cb13-26">                pos_enc[pos,i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.cos(pos<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>d_model)))</span>
<span id="cb13-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> pos_enc</span>
<span id="cb13-28"></span>
<span id="cb13-29"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#calculate positional encodings for the sentence</span></span>
<span id="cb13-30">seq_len<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(words)</span>
<span id="cb13-31">positional_encodings<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>positional_encoding(seq_len,embedding_dim)</span>
<span id="cb13-32"></span>
<span id="cb13-33"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#calculate the sum of position encoding and embedding vectors</span></span>
<span id="cb13-34">combined_vectors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>unit_circle_vectors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>positional_encodings</span>
<span id="cb13-35"></span>
<span id="cb13-36"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#decoder matrix to map combined vectors back to one-hot-like representations</span></span>
<span id="cb13-37">decoder_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.linalg.pinv(unit_circle_vectors)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#pseudo-inverse to decode</span></span>
<span id="cb13-38"></span>
<span id="cb13-39"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#decode the combined vectors</span></span>
<span id="cb13-40">decoded_vectors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.dot(combined_vectors,decoder_matrix)</span>
<span id="cb13-41"></span>
<span id="cb13-42"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#map decoded vectors to words by finding the closest match</span></span>
<span id="cb13-43"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> decode_to_words(decoded_vectors,word_embeddings,word_list):</span>
<span id="cb13-44">    result<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[]</span>
<span id="cb13-45">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> vec <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> decoded_vectors:</span>
<span id="cb13-46">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#project vec back into the original embedding space</span></span>
<span id="cb13-47">        reconstructed_vec<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.dot(vec,word_embeddings)</span>
<span id="cb13-48">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#compute distances and find the closest match</span></span>
<span id="cb13-49">        distances<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.linalg.norm(word_embeddings<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>reconstructed_vec,axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb13-50">        closest_word_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.argmin(distances)</span>
<span id="cb13-51">        result.append(word_list[closest_word_index])</span>
<span id="cb13-52">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> result</span>
<span id="cb13-53"></span>
<span id="cb13-54">decoded_words<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>decode_to_words(decoded_vectors,unit_circle_vectors,words)</span>
<span id="cb13-55"></span>
<span id="cb13-56"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#print initial one-hot vectors</span></span>
<span id="cb13-57"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Initial One-Hot Vectors:"</span>)</span>
<span id="cb13-58"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i,word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(words):</span>
<span id="cb13-59">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>word<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">:</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>one_hot_vectors[i]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb13-60"></span>
<span id="cb13-61"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#print embedded vectors on unit circle</span></span>
<span id="cb13-62"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Embedded Vectors on Unit Circle:"</span>)</span>
<span id="cb13-63"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i,word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(words):</span>
<span id="cb13-64">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>word<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">:</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>unit_circle_vectors[i]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb13-65"></span>
<span id="cb13-66"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#print positional encodings</span></span>
<span id="cb13-67"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Positional Encodings:"</span>)</span>
<span id="cb13-68"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i,word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(words):</span>
<span id="cb13-69">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>word<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">:</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>positional_encodings[i]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb13-70"></span>
<span id="cb13-71"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#print combined vectors</span></span>
<span id="cb13-72"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Combined Vectors (Embedding+Positional Encoding):"</span>)</span>
<span id="cb13-73"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i,word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(words):</span>
<span id="cb13-74">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>word<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">:</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>combined_vectors[i]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb13-75"></span>
<span id="cb13-76"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#print decoded words</span></span>
<span id="cb13-77"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Decoded tokenized sentence from Combined Vectors:"</span>)</span>
<span id="cb13-78"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(decoded_words)</span>
<span id="cb13-79"></span></code></pre></div></div>
<pre><code>Tokenized sentence: ['My', 'rabbit', 'likes', 'bananas']
Initial One-Hot Vectors:
My:[1 0 0 0]
rabbit:[0 1 0 0]
likes:[0 0 1 0]
bananas:[0 0 0 1]

Embedded Vectors on Unit Circle:
My:[0.1391731  0.99026807]
rabbit:[-0.5591929  -0.82903757]
likes:[-0.34202014 -0.93969262]
bananas:[ 0.89100652 -0.4539905 ]

Positional Encodings:
My:[0. 1.]
rabbit:[0.84147098 0.54030231]
likes:[ 0.90929743 -0.41614684]
bananas:[ 0.14112001 -0.9899925 ]

Combined Vectors (Embedding+Positional Encoding):
My:[0.1391731  1.99026807]
rabbit:[ 0.28227808 -0.28873527]
likes:[ 0.56727728 -1.35583946]
bananas:[ 1.03212653 -1.443983  ]

Decoded tokenized sentence from Combined Vectors:
['My', 'bananas', 'bananas', 'bananas']</code></pre>
</section>
</section>
<section id="attention" class="level3">
<h3 class="anchored" data-anchor-id="attention">Attention</h3>
<p>We have technically made an encoder-decoder model at this point. As you can see in the code above, even without attention, it is somewhat difficult to get consistent sequence reconstruction. Let’s think about attention again. So far, we have simply made an attention matrix and used it to suppress unimportant parts of the code. Our attention matrix was created by stacking one-hot encoded vectors on top of each other. This doesn’t make much sense. Some parts of a sentence may be important - maybe not as important as other parts - but important nonetheless. Multiplication with an attention matrix should result in a vector with parts that are suppressed but not necessarily zero. We can give importance to different parts by making our attention matrix elements fractions instead of ones and zeros.</p>
<p>We have so far not defined precisely what attention does. We have so far said that given a matrix of attention masks, we can pull out specific rows given our one-hot encoded non-future-dependent vectors, and suppress things even more. Now we focus on the big question. How do we generate this attention matrix? We’re in a completely different domain now - our words are no longer one-hot encoded, they’re embeddings, we’re adding positional information to them, and now we want to suppress irrelevant information.</p>
<p>Let’s get an intuitive explanation of attention first. We can naturally ask whether the attention in machine learning is the same as attention in human beings. What does this mean? For example, our brain is flooded with many different sensory inputs every second. There’s the internal sensory information from the body (such as level of hunger, blood pressure, pain, our balance), and there’s external sensory information from the environment (such as me hearing the distant hum of cars outside the window while typing this, but choosing to ignore it). How do we not get overwhelmed? We ‘tune out’ (suppress) irrelevant information and only focus on the one that matters. Only a small subset of the sensory input data is considered <em>relevant</em> enough to be perceived - this is what we mean when we say we are paying attention to something.</p>
<p>Simply put, we are assigning importance to items by filtering out the irrelevant ones. We also have a finite amount of attention. For example, watching a group of ants move around is significantly easier than tracking the path of more than a few ants in that group. You can either have a general idea of how the group is moving, or a specific idea of how some finite amount of ants in that group are moving, <strong>but not both</strong>.</p>
<p>We can specify this mathematically: Given a set of input items <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>,</mo><msub><mi>i</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><msub><mi>i</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">i_1,i_2,...i_n</annotation></semantics></math>, we assign nonzero weights to them, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><msub><mi>w</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>w</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">w_1,w_2,...,w_n</annotation></semantics></math> such that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></msubsup><msub><mi>w</mi><mi>k</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sum_{k=0}^{n}w_k=1</annotation></semantics></math>. Interpreting the weights as importances, they satisfy the two properties of human attention. We can than make a judgment about the items based on this attention. Since our inputs to the attention matrix are a collection of items (embedded word vectors), we can then assign a weight to them.</p>
<p>Where do these weights come from? This is where the learning in machine learning happens. We want to <em>learn</em> a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> to <em>compute</em> these weights. This function is typically one that <strong>first computes</strong> some ‘relevance’ score for each word in the sequence <strong>and then</strong> softmaxes the weights in order to make the weights nonzero. We have already computed this relevance score. This is simply the sum of the embedding vector and positional encoding vector. What do we do now?</p>
<p>So far we have talked about second-order models. Throughout the tutorial, we have constructed our one-hot encoded vectors assuming second-order relationships in the past. We briefly said that we could extend the word-pair vector construction to word triplets, but as the discussion so far should show, this is very impractical. We now try to answer the question: <strong>How important is every word to every other word in the sentence, and it is possible to calculate this in one go?</strong> Positional encodings don’t really answer this question, as they show how ‘relevant’ each word is in the overall sentence, but not in relation to the actual other words. We have to figure out three main things:</p>
<ol type="1">
<li>How do you tell a word to ‘ask’ other words in the sentence the following question: ‘How important are you to me?’?</li>
<li>If this question is asked for every possible word pair, how do you calculate the response of every other word?</li>
<li>If you are able to answer questions 1 and 2, how do you actually construct the vector that will be fed into the neural network?</li>
</ol>
<p>Let explicitly define what this means. Each word has some sort of <strong>intrinsic value</strong>, which we have said is the sum of its embedding and positional encoding. For each word to ask each other word <em>how important they are to it</em>, we have to compare its intrinsic value to the other words’ values. But <em>what</em> values? In the compound sentence ‘I went out to buy fruit and my sister answered some emails.’, ‘fruit’ and ‘emails’ are the objects of the subjects ‘I’ and ‘my sister’ in the two coordinate clauses. Even though they are very important within their own clauses, they are more or less irrelevant for the other clause. If there was an earlier relation such as ‘my sister likes fruit’ much earlier (say 1000 words behind ‘buy fruit’) in the corpus, it would make sense to compare the intrinsic values of the embeddings. But since we practically do not want to look 1000 words behind for all the reasons outlined above, it makes sense to assign each word a <strong>response value</strong> i.e.&nbsp;if asked a question by another word, the word being asked will return a response value <em>that may be different from its intrinsic value</em>. Based on what the response value is, the initial word will decide what <strong>information value</strong> it ‘passes on’ to the neural network so that it can reconstruct this efficiently. Also, it does not make much sense for ‘I’ and ‘emails’ to be compared, so you also have to <strong>figure out how much ‘I’ will pay attention to the other words</strong>. This triplet of <em>intrinsic value</em>, <em>response value</em>, and <em>information value</em> are what defines the attention process.</p>
<p>How do we make a word get a vector response from every other word? Like we have been doing all along, we define an <strong>attention matrix</strong> that is <em>not</em> one-hot encoded this time around. The attention matrix <em>prepares each word</em> for asking questions. Multiplying a word vector by this attention matrix results in a vector that contains the information ‘how much attention should this word pay to other words?’. To make this process fast, you can <em>stack</em> each intrinsic value into a matrix and multiply it with the attention matrix. This results in a matrix (let’s call this matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>) that contains information about ‘how much should every word pay attention to every other word’? Note that this is not pair-specific; we are <em>not</em> saying that ‘I’ should pay lesser attention to ‘emails’ than to ‘went’. This is the behavior that is <em>learned</em> by the transformer.</p>
<p>Next, we multiply <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> with a matrix that contains a matrix of responses of the other words. Like before, we are not explicitly telling ‘I’ to provide a worse response if asked ‘How important are you to me?’ by ‘emails’, but we are <em>learning</em> this behavior. Let’s call the result of this matrix multiplication <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>. You can now <em>normalize</em> the weights and <em>softmax</em> them in order for the matrix to have stable values. We then finally multiply this matrix by the matrix of information values to get <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> <strong>the result of the attention step</strong>. <em>This</em> is the idea behind attention. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> contains information about <em>how much weight</em> each word should give to every other word, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> contains information that makes our two-layer neural network make a judgment. These matrices are <em>learned</em> by the transformer by backpropagation.</p>
<section id="multi-head-attention" class="level4">
<h4 class="anchored" data-anchor-id="multi-head-attention">Multi-head attention</h4>
<p>Since we are now no longer dealing with nonnegative zero-one matrices, the question of how to do this calculation efficiently for large corpora still remains. Unfortunately we cannot. We can only rely on specialized and accelerated hardware. We are also still not sure whether the attention process actually captures meanings in a way that is human-understandable. The solution is straightforward: instead of having <em>one</em> set of matrices to perform attention, have <em>more than one set</em>. Apply each matrix set’s attention independently, and for every newly returned information vector, combine them together in a specified way (usually, just concatenate, then use a neural network to predict outputs). The hope is that each set of matrices will learn something different about the text - sentence structure, word meanings, subject-object relationships. Each such set of matrices is called an <strong>attention head</strong>. To make this process somewhat computationally palatable, the matrices in multihead attention have smaller output dimensions (usually what the size would be for a single head for the whole corpus divided by the number of heads). <em>In practice</em>, this does work.</p>
<p>The final thing is dealing with practical languages. We still have to <strong>stack</strong> these attention blocks and two-layer neural networks many times in order to actually encode and decode things. This is the main reason it takes so long to train transformers on real text items. There is still of course the problem of getting correct datasets, but this is fine.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb15-2"></span>
<span id="cb15-3"></span>
<span id="cb15-4">tokenized_sentence<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'My'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rabbit'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'likes'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bananas'</span>]</span>
<span id="cb15-5"></span>
<span id="cb15-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#combined_vectors is a np.ndarray of shape (4,2) that has been initialized in a different cell</span></span>
<span id="cb15-7"></span>
<span id="cb15-8">intrinsic_value_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>combined_vectors.T</span>
<span id="cb15-9"></span>
<span id="cb15-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The intrinsic value matrix is:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, intrinsic_value_matrix)</span>
<span id="cb15-11"></span>
<span id="cb15-12">l<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(tokenized_sentence) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#this is completely arbitrary - i want a machine learning model whose final dimension is 4</span></span>
<span id="cb15-13"></span>
<span id="cb15-14">attention_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.rand(l,l)</span>
<span id="cb15-15"></span>
<span id="cb15-16"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The initial attention matrix is:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, attention_matrix)</span>
<span id="cb15-17"></span>
<span id="cb15-18">A<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(intrinsic_value_matrix, attention_matrix)</span>
<span id="cb15-19"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Preparing the words for attention, we get:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, A)</span>
<span id="cb15-20"></span>
<span id="cb15-21">response_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.rand(l,l)</span>
<span id="cb15-22"></span>
<span id="cb15-23"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The initial response matrix is:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, response_matrix)</span>
<span id="cb15-24"></span>
<span id="cb15-25">B<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(A,response_matrix.T)</span>
<span id="cb15-26"></span>
<span id="cb15-27"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The responses given by each word to each other word is:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, B, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> and after normalizing, we get:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, B<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>np.sqrt(l))</span>
<span id="cb15-28">B<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>B<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>np.sqrt(l) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#explicitly rewrite the matrix</span></span>
<span id="cb15-29"></span>
<span id="cb15-30"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"We will softmax the attention matrix in order to boost closer words. The result of doing this is:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, np.exp(B)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(np.exp(B),axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span>
<span id="cb15-31">B<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.exp(B)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(np.exp(B),axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#store it again</span></span>
<span id="cb15-32"></span>
<span id="cb15-33">information_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.rand(l,l)</span>
<span id="cb15-34"></span>
<span id="cb15-35"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The information matrix is:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, information_matrix)</span>
<span id="cb15-36"></span>
<span id="cb15-37">C<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(B, information_matrix)</span>
<span id="cb15-38"></span>
<span id="cb15-39"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The actual information passed on to the two-layer neural network for decoding is:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, C)</span></code></pre></div></div>
<pre><code>The intrinsic value matrix is:
 [[ 0.1391731   0.28227808  0.56727728  1.03212653]
 [ 1.99026807 -0.28873527 -1.35583946 -1.443983  ]]
The initial attention matrix is:
 [[0.96775588 0.85687394 0.11764399 0.73973033]
 [0.31966683 0.98841599 0.48535898 0.1224339 ]
 [0.72518555 0.3230112  0.99707816 0.97978246]
 [0.65613402 0.12982645 0.686861   0.09303049]]
Preparing the words for attention, we get:
 [[ 1.31351514  0.71549621  1.42792633  0.78933853]
 [-0.09688701  0.79459977 -2.24969069 -0.02585148]]
The initial response matrix is:
 [[0.02936416 0.82295845 0.20975295 0.37902713]
 [0.97345739 0.3368405  0.29648978 0.918208  ]
 [0.11074845 0.75958241 0.98806557 0.90129601]
 [0.65166792 0.31406079 0.25432198 0.8192551 ]]
The responses given by each word to each other word is:
 [[ 1.22608639  2.66780165  2.81126061  2.09050766]
 [ 0.16939992 -0.51740933 -1.65330782 -0.40691028]] 
 and after normalizing, we get:
 [[ 0.61304319  1.33390082  1.4056303   1.04525383]
 [ 0.08469996 -0.25870467 -0.82665391 -0.20345514]]
We will softmax the attention matrix in order to boost closer words. The result of doing this is:
 [[0.14693005 0.30211697 0.32458379 0.22636919]
 [0.34953106 0.24794025 0.14050437 0.26202432]]
The information matrix is:
 [[0.87982191 0.52543491 0.24433994 0.11294321]
 [0.08717947 0.71907745 0.42011873 0.90535987]
 [0.7396713  0.19126042 0.61377742 0.43550545]
 [0.57724973 0.73806912 0.80619588 0.84373733]]
The actual information passed on to the two-layer neural network for decoding is:
 [[0.52636754 0.52360382 0.54454599 0.62247348]
 [0.5843209  0.58220905 0.48705008 0.54622243]]</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"This is a demonstration of multihead attention.</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb17-2"></span>
<span id="cb17-3"></span>
<span id="cb17-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Let's have two attention heads. The idea is to make our single-head attention model smaller."</span>)</span>
<span id="cb17-5"></span>
<span id="cb17-6">intrinsic_value_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>combined_vectors</span>
<span id="cb17-7"></span>
<span id="cb17-8">l2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(l<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#two heads</span></span>
<span id="cb17-9">matrix_sets<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[]</span>
<span id="cb17-10">l3<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(l2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#two heads, so the underlying projection dimesion will be smaller</span></span>
<span id="cb17-11"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(l2):</span>
<span id="cb17-12">    matrix_sets.append([np.random.rand(l2,l3),np.random.rand(l2,l3),np.random.rand(l2,l3)])</span>
<span id="cb17-13"></span>
<span id="cb17-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The multihead attention matrices are now:"</span>)</span>
<span id="cb17-15"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>([i <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> matrix_sets])</span>
<span id="cb17-16"></span>
<span id="cb17-17"></span>
<span id="cb17-18">passed_on<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[]</span>
<span id="cb17-19"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The attention process is now applied to the intrinsic value matrix for each head."</span>)</span>
<span id="cb17-20"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(l2):</span>
<span id="cb17-21">    A<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(intrinsic_value_matrix,matrix_sets[i][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb17-22">    B<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(A,matrix_sets[i][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].T)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>np.sqrt(l2)</span>
<span id="cb17-23">    B<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.exp(B)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(np.exp(B),axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb17-24">    C<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(B,matrix_sets[i][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>])</span>
<span id="cb17-25">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"For head"</span>,i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"the information passed on is:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, C)</span>
<span id="cb17-26">    passed_on.append(C)</span>
<span id="cb17-27"></span>
<span id="cb17-28"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Therefore, the total information passed on is:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>,np.hstack((passed_on[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>],passed_on[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])))</span></code></pre></div></div>
<pre><code>This is a demonstration of multihead attention.

Let's have two attention heads. The idea is to make our single-head attention model smaller.
The multihead attention matrices are now:
[[array([[0.25397487],
       [0.0311156 ]]), array([[0.73929474],
       [0.10634373]]), array([[0.31491959],
       [0.00381713]])], [array([[0.68166948],
       [0.16713201]]), array([[0.65528241],
       [0.99174262]]), array([[0.07267087],
       [0.07707801]])]]
The attention process is now applied to the intrinsic value matrix for each head.
For head 1 the information passed on is:
 [[0.16275392]
 [0.16155103]
 [0.16291437]
 [0.16692318]]
For head 2 the information passed on is:
 [[0.07498641]
 [0.07491223]
 [0.0749164 ]
 [0.07499548]]
Therefore, the total information passed on is:
 [[0.16275392 0.07498641]
 [0.16155103 0.07491223]
 [0.16291437 0.0749164 ]
 [0.16692318 0.07499548]]</code></pre>
</section>
<section id="masked-attention" class="level4">
<h4 class="anchored" data-anchor-id="masked-attention">Masked attention</h4>
<p>We have not exactly implemented future-proofing here. Masked attention is simply a term that makes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> an upper triangular matrix, as we can easily now see why it is a future proofed matrix.</p>
</section>
<section id="skip-connections-layer-normalization-and-xavier-initializations" class="level4">
<h4 class="anchored" data-anchor-id="skip-connections-layer-normalization-and-xavier-initializations">Skip connections, Layer Normalization, and Xavier Initializations</h4>
<p>Sometimes the result of the total information being passed on is small, so you can always add a <strong>skip connection</strong> to make it so that your vector is a result of embedding+position encoding AND attention. This is all empirical stuff that ‘just works’. Skip connections also serve the dual purpose of being good for gradient descent. One thing we have not spoken about is gradient descent in practice. It is easy to see that gradient descent may be bad in practice while sound in theory, when the error function’s plot in parameter space is just too hilly. Skip connections, things like <em>layer normalization</em>, and <em>Xavier initialization</em> for all parameters make ‘the gradients flow smoother’ in backpropagation. In practice, this means that the gradient calculation actually updates the values fairly frequently, instead of the gradient being calculated as 0 (because of a loss of numerical precision in computers - they have finite precision!).</p>
<p>For backpropagation through layer normalization, this is the mathematics: $$ x_{}=f_{} \[5pt] f_{}=\[5pt]</p>
<p>= _{i=1}^N x_i \[5pt]</p>
<p>^2 = _{i=1}^N (x_i - )^2 \[5pt]</p>
<p>\[5pt]</p>
<p> d_{out}=\[5pt]</p>
<p> = <em>{i=1}^N d</em>{out_{i}} (x_i - ) (- (^2 + )^{-3/2})\[5pt]</p>
<p> = <em>{i=1}^N d</em>{out_{i}} (-) + _{i=1}^N (-2 (x_i - ) / N )\[5pt]</p>
<p> = + + $$</p>
</section>
<section id="cross-attention" class="level4">
<h4 class="anchored" data-anchor-id="cross-attention">Cross-attention</h4>
<p>So far we have described attention as a standalone procedure. The idea is now to encode text with attention (for example, our neural network converts to a lower dimensional space) and pass the result of that into a decoder matrix. What does that mean? It means that we have converted text into an arbitrary dimension space <em>after</em> performing all of these procedures. But wait. These can simply be interpreted as the results of a separate instance of attention results! While <em>generating</em> text, we can simply pull those results in and feed them into the decoder’s decoding stage, providing the attention we need.</p>
</section>
<section id="text-generation" class="level4">
<h4 class="anchored" data-anchor-id="text-generation">Text generation</h4>
<p>Now we do our actual task. How do we actually choose the next word? The idea is to predict the next word and generate a probability distribution with softmaxing, and greedily picking the one with the highest probability. Then, <em>take the text that has been generated so far along with the output</em> and use it as an <em>input</em> to the decoder, apply attention, then use multihead to combine things with the encoded transformer. Transformers generate text one word at a time. The way you put the text in as input is by inserting a special word that translates to ‘SEQUENCE STARTS HERE’. This allows the decoder to train and learn to predict the next word.</p>
</section>
</section>
<section id="a-transformer-from-scratch-in-pure-numpy" class="level3">
<h3 class="anchored" data-anchor-id="a-transformer-from-scratch-in-pure-numpy">A transformer from scratch in pure numpy</h3>
<p>After all of this, we can finally implement a transformer in numpy. Note that when you backpropagate errors through the model, the updates are applied to all matrices as single neurons as well. Let us lay out what the overall task is:</p>
<ol type="1">
<li>Give an input sentence, tokenize it, embed the words, add positional encoding. Keep a copy of the output.</li>
<li>Initialize <em>random</em> attention, response, and value matrices, and run the matrix of inputs through the multi-head attention step.</li>
<li>Add the output copy and attention step results, and use that as input to a 2 layer neural network (whose neurons are randomly initialized) to convert the data into some weird representation that only the machine understands.</li>
<li>Put this weird or <em>latent</em> representation into another neural network to decode it, then project it to the target space vocabulary size, then softmax. Select the word with the largest probability as the output and compare which word in the other vocabulary’s embeddings is the closest. Once the next word is found, use that as the input to the model (that is, put it through its own set of attention steps, but this time they’re masked) and use that to predict the next token. Once the entire sequence has been predicted, compute the cross-entropy loss and apply backpropagation, updating each matrix.</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb19-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> List, Tuple, Optional, Dict</span>
<span id="cb19-3"></span>
<span id="cb19-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">###############################################################################</span></span>
<span id="cb19-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Utilities</span></span>
<span id="cb19-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">###############################################################################</span></span>
<span id="cb19-7"></span>
<span id="cb19-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> whitespace_tokenizer(sentence: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> List[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>]:</span>
<span id="cb19-9">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Splits a latin-character sentence into individual words assuming that each word is separated by a whitespace, and there's no punctuation</span></span>
<span id="cb19-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    </span></span>
<span id="cb19-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Input:</span></span>
<span id="cb19-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            sentence: a string </span></span>
<span id="cb19-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Output:</span></span>
<span id="cb19-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            List of strings containing each individual words</span></span>
<span id="cb19-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-17"></span>
<span id="cb19-18">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#we now start using python's inbuilt functions in order to make the code look more impressive</span></span>
<span id="cb19-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> sentence.strip().split()</span>
<span id="cb19-20"></span>
<span id="cb19-21"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> build_vocab(list_of_tokenized_sentences: List[List[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>]]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tuple[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>]:</span>
<span id="cb19-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Builds a vocabulary of all words in the a given corpus of languages.</span></span>
<span id="cb19-24"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        This means that if you have more than one sentence, it creates a list of words.</span></span>
<span id="cb19-25"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Input:</span></span>
<span id="cb19-26"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            list_of_tokenized_sentences: a list of list of strings, AKA a whitespace tokenized sentence</span></span>
<span id="cb19-27"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Output:</span></span>
<span id="cb19-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            A tuple of Python dictionaries, containing all words and their index. This index is random, because of the use of set()</span></span>
<span id="cb19-29"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-30">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#the idea is to store the vocabulary in a Python set() object, which randomly stores elements for faster access (there's more to it but oh well)</span></span>
<span id="cb19-31">    vocabulary<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>()</span>
<span id="cb19-32">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> tokenized_sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> list_of_tokenized_sentences: </span>
<span id="cb19-33">        vocabulary.update(tokenized_sentence) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#when you update a python set with a list, the elements of the list are added to the vocabulary</span></span>
<span id="cb19-34">    vocabulary<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(vocabulary) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#convert the vocabulary set() into a list, so you can enumerate through it</span></span>
<span id="cb19-35">    word_index_pairs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{w:i <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i,w <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(vocabulary)} <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#create a word:index pairing dictionary</span></span>
<span id="cb19-36">    index_word_pairs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{i:w <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i,w <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(vocabulary)} <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#create an index:word pairing dictionary</span></span>
<span id="cb19-37">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> word_index_pairs,index_word_pairs</span>
<span id="cb19-38"></span>
<span id="cb19-39"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> pad_sequences(sequences: List[List[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>]], max_length: Optional[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pad_value: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb19-40">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-41"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Pads sequences. This is done because it is easier to process sequences with the same length, so we artifically add numbers to smaller ones.</span></span>
<span id="cb19-42"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-43"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            sequences: a list of list of integers</span></span>
<span id="cb19-44"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            max_length: Optional. Allows you to set the maximum length for a sequence instead of computing it dynamically.</span></span>
<span id="cb19-45"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                     For example, in a corpus of sentences each under 20 words long, you can set the max length as 294</span></span>
<span id="cb19-46"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            pad_value: What number you want to use to </span></span>
<span id="cb19-47"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Output:</span></span>
<span id="cb19-48"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            A numpy.ndarray of a padded sequence</span></span>
<span id="cb19-49"></span>
<span id="cb19-50"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-51">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#dynamically figure out the max length, as you have to pad to this</span></span>
<span id="cb19-52">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> max_length <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb19-53">        max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(sequence) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> sequence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sequences)</span>
<span id="cb19-54">    padded<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[]</span>
<span id="cb19-55">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> sequence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sequences:</span>
<span id="cb19-56">        padded_sequence<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sequence<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>[pad_value]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(sequence)) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#simply append a list containing pad_value</span></span>
<span id="cb19-57">        padded.append(padded_sequence)</span>
<span id="cb19-58">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.array(padded, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.int32) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#return an ndarray</span></span>
<span id="cb19-59"></span>
<span id="cb19-60"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> create_mask_for_removing_future_dependency(sequence: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb19-61">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-62"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Creates a non-future-dependent mask for the decoder, so that the decoder doesn't use the future for generation. This is also called autoregressive behavior.</span></span>
<span id="cb19-63"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-64"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            sequence: An input ndarray whose shape is (batch_size, batch_size) - note that this assumes you are feeding it into the decoder at scale</span></span>
<span id="cb19-65"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-66"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            An ndarray of the same shape as the input sequence which is upper triangular. The data type is Bool for faster processing. The upper triangular part has 'True'.</span></span>
<span id="cb19-67"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-68">    sequence_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sequence.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb19-69">    autoregressive_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.triu(np.ones((sequence_length,sequence_length)),k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).astype(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bool</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#again, using the inbuilt functions</span></span>
<span id="cb19-70">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> autoregressive_mask</span>
<span id="cb19-71"></span>
<span id="cb19-72"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> one_hot(indices: np.ndarray, vocabulary_size: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb19-73">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-74"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Converts input sequences into one-hot encoding in one go. </span></span>
<span id="cb19-75"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Since we are feeding sequences in one go to the encoder in batches, we need a 3D ndarray. </span></span>
<span id="cb19-76"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-77"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            indices: An input ndarray whose shape is (batch_size, sequence_length), where each element has an integer index. This is why we created indices above</span></span>
<span id="cb19-78"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            vocabulary_size: The number of words in the vocabulary of the language</span></span>
<span id="cb19-79"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-80">    batched_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.zeros((indices.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], indices.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], vocabulary_size)) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#create your zero ndarray, and populate it</span></span>
<span id="cb19-81">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#use a nested loop to set one-hot encodings</span></span>
<span id="cb19-82">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> batch_index <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(indices.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]):</span>
<span id="cb19-83">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> token_index <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(indices.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]):</span>
<span id="cb19-84">            batched_tokens[b,t,indices[batch_index,token_index]]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb19-85">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> batched_tokens</span>
<span id="cb19-86"></span>
<span id="cb19-87"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">###############################################################################</span></span>
<span id="cb19-88"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Normalization, activation, and loss</span></span>
<span id="cb19-89"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">###############################################################################</span></span>
<span id="cb19-90"></span>
<span id="cb19-91"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> layer_norm(x: np.ndarray, eps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-6</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tuple[np.ndarray,np.ndarray,np.ndarray]:</span>
<span id="cb19-92">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-93"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Normalizes the input ndarray. This is done to make backpropagation not run into numerical errors.</span></span>
<span id="cb19-94"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-95"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            x: ndarray of inputs. usually just a vector</span></span>
<span id="cb19-96"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            eps: this is done to prevent division by zero during normalization</span></span>
<span id="cb19-97"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-98"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            Tuple of ndarrays containing the normalized input ndarray, the mean, and the variance</span></span>
<span id="cb19-99"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-100">    mean<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.mean(x, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb19-101">    var<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.var(x, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb19-102">    x_norm<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>mean)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>np.sqrt(var<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>eps)</span>
<span id="cb19-103">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> x_norm,mean,var</span>
<span id="cb19-104"></span>
<span id="cb19-105"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> layer_norm_backprop(dout: np.ndarray, x: np.ndarray, mean: np.ndarray, var: np.ndarray, eps: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-6</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span>np.ndarray:</span>
<span id="cb19-106"></span>
<span id="cb19-107">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-108"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Compute the gradient of the loss with respect to the input x of a layer normalization operation, implementing backpropagation as defined in the tutorial</span></span>
<span id="cb19-109"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs: </span></span>
<span id="cb19-110"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            dout: an ndarray containing the gradient with respect to the normalized input</span></span>
<span id="cb19-111"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            x: an ndarray containing the original input</span></span>
<span id="cb19-112"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            mean: mean of the input ndarray x along the last axis, as computed above</span></span>
<span id="cb19-113"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            var: exactly like the mean</span></span>
<span id="cb19-114"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            eps: an optional value for numerical stabillity</span></span>
<span id="cb19-115"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-116"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            an ndarray containing the gradient with respect to the layer normalization procedure</span></span>
<span id="cb19-117"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-118"></span>
<span id="cb19-119">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#numerically backpropagate by calculating the derivatives. unfortunately, this just requires knowing the formula as shown above</span></span>
<span id="cb19-120">    N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb19-121">    dx_norm<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>np.sqrt(var<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>eps)</span>
<span id="cb19-122">    dvar<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(dout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>mean)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(var<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>eps)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>), axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb19-123">    dmean<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(dout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>np.sqrt(var<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>eps), axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>dvar<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>mean), axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>N)</span>
<span id="cb19-124">    dx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dx_norm<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>dvar<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>mean)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>dmean<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>N</span>
<span id="cb19-125">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> dx</span>
<span id="cb19-126"></span>
<span id="cb19-127"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> softmax(x: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb19-128">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-129"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Compute the softmax of an input ndarray and generate a probability distribution</span></span>
<span id="cb19-130"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-131"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            x: an ndarray that you want to softmax</span></span>
<span id="cb19-132"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-133"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            an ndarray containing the softmaxed version along the last axis</span></span>
<span id="cb19-134"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-135">    x_shifted<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(x, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#this is done to improve numerical stability. softmaxing is invariant to shifts by a constant value</span></span>
<span id="cb19-136">    exp_x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.exp(x_shifted)</span>
<span id="cb19-137">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> exp_x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(exp_x, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb19-138"></span>
<span id="cb19-139"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> cross_entropy_loss(predictions: np.ndarray, targets: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb19-140">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-141"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Compute the cross-entropy loss as defined above, measuring the difference between two probability predicted distributions</span></span>
<span id="cb19-142"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-143"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            predictions: an ndarray containing whatever you have predicted</span></span>
<span id="cb19-144"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            targets: an ndarray containing whatever the real targets are</span></span>
<span id="cb19-145"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-146"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            a float of computed cross-entropy loss, averaged over all elements in the batch</span></span>
<span id="cb19-147"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-148">    epsilon<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-12</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#more numerical precision constants</span></span>
<span id="cb19-149">    predictions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.clip(predictions, epsilon, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>epsilon) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#values smaller than epsilon become epsilon, values larger than 1-epsilon become 1-epsilon</span></span>
<span id="cb19-150">    flat_targets<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>targets.flatten() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#maybe the arrays are not 1d, but the cross-entropy loss needs the 1d</span></span>
<span id="cb19-151">    flat_preds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>predictions.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, predictions.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#same reason here for flattening</span></span>
<span id="cb19-152">    loss<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span>np.mean(np.log(flat_preds[np.arange(flat_targets.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]), flat_targets])) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#compute the crossentropy loss</span></span>
<span id="cb19-153">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> loss</span>
<span id="cb19-154"></span>
<span id="cb19-155"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> cross_entropy_derivative(predictions: np.ndarray, targets: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb19-156">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-157"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Compute the backpropagation for cross-entropy loss</span></span>
<span id="cb19-158"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-159"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            predictions: an ndarray containing whatever you have predicted</span></span>
<span id="cb19-160"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            targets: an ndarray containing whatever the real targets are</span></span>
<span id="cb19-161"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-162"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            an ndarray of the gradient of the cross-entropy derivative required for backpropagation</span></span>
<span id="cb19-163"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-164">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#the gradient was also defined above so this is just a way to get batches of data in and publish it</span></span>
<span id="cb19-165">    batch,length,vocab_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>predictions.shape</span>
<span id="cb19-166">    grad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>predictions.copy()</span>
<span id="cb19-167">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> b <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(batch):</span>
<span id="cb19-168">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(length):</span>
<span id="cb19-169">            grad[b, t, targets[b,t]]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb19-170">    grad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/=</span>(batch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>length)</span>
<span id="cb19-171">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> grad</span>
<span id="cb19-172"></span>
<span id="cb19-173"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">###############################################################################</span></span>
<span id="cb19-174"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Multi-Head Attention</span></span>
<span id="cb19-175"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">###############################################################################</span></span>
<span id="cb19-176"></span>
<span id="cb19-177"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> split_heads(x: np.ndarray, num_heads: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb19-178">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-179"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Attention heads attend to different parts of the data, so this is a function to simply split the data </span></span>
<span id="cb19-180"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-181"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            x: an ndarray of input data</span></span>
<span id="cb19-182"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            num_heads: the number of attention instances you want</span></span>
<span id="cb19-183"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-184"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            a reshaped x split into the number of heads</span></span>
<span id="cb19-185"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-186">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#at this point it should be familiar, split the data into batches and make it work</span></span>
<span id="cb19-187">    batch,length,d_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x.shape</span>
<span id="cb19-188">    head_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span>num_heads <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#the heads attend to different parts of the model</span></span>
<span id="cb19-189">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> x.reshape(batch, length, num_heads, head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#reshape x so that every ndarray is for heads</span></span>
<span id="cb19-190"></span>
<span id="cb19-191"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> merge_heads(x: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb19-192">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-193"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        After the result of multihead attention, you need to recombine them, so this function does it</span></span>
<span id="cb19-194"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-195"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            x: an ndarray containing information of shape (batch, num_heads, length, head_dim)</span></span>
<span id="cb19-196"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-197"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            a combined x after (presumably) multihead attention has been done</span></span>
<span id="cb19-198"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-199">    batch,num_heads,length,head_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x.shape</span>
<span id="cb19-200">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> x.transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>).reshape(batch, length, num_heads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>head_dim)</span>
<span id="cb19-201"></span>
<span id="cb19-202"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> scaled_dot_product_attention(attention_matrix: np.ndarray, response_matrix: np.ndarray, information_matrix: np.ndarray, mask: Optional[np.ndarray]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tuple[np.ndarray, np.ndarray]:</span>
<span id="cb19-203">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-204"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        This is the official formula of the attention. As we have seen, we use an attention matrix to make each word ask each other word a question.</span></span>
<span id="cb19-205"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        The response of each other word is the response matrix and the information passed on is the information matrix. </span></span>
<span id="cb19-206"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        inputs:</span></span>
<span id="cb19-207"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            attention_matrix: An ndarray of attention values</span></span>
<span id="cb19-208"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            response_matrix: An ndarray of response values</span></span>
<span id="cb19-209"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            information_matrix: An ndarray of information values that is passed on</span></span>
<span id="cb19-210"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            mask: An ndarray of masks, but with Bool values</span></span>
<span id="cb19-211"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-212"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            a tuple of the information passed on and the attention weights</span></span>
<span id="cb19-213"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span> </span>
<span id="cb19-214">    normalization_factor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_matrix.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb19-215">    scores<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(attention_matrix, response_matrix.transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>np.sqrt(normalization_factor) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#we are now using 'official' terminology</span></span>
<span id="cb19-216">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb19-217">        scores<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.where(mask[np.newaxis, np.newaxis,:,:], <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e9</span>, scores) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#mask if required</span></span>
<span id="cb19-218">    attn_weights<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>softmax(scores) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#softmax the product</span></span>
<span id="cb19-219">    output<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(attn_weights, information_matrix) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#send information on</span></span>
<span id="cb19-220">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> output, attn_weights</span>
<span id="cb19-221"></span>
<span id="cb19-222"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> multi_head_attention(batched_input_intrinsic_value_matrix: np.ndarray, batched_input_response_matrix: np.ndarray, batched_input_information_matrix: np.ndarray, batched_input_intrinsic_value_projection_matrix: np.ndarray, batched_input_response_projection_matrix: np.ndarray, batched_input_information_projection_matrix: np.ndarray, final_reshaper_matrix: np.ndarray, num_heads: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, mask: Optional[np.ndarray]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tuple[np.ndarray, np.ndarray]:</span>
<span id="cb19-223">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-224"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Implementation of multihead attention. Simply put, apply attention on smaller parts of the sequence, and then recombine them by concatenation.</span></span>
<span id="cb19-225"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Because we are splitting the sequence, we need to generate different attention, response, and information matrices for each attention instance.</span></span>
<span id="cb19-226"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        This means we have to project the input into different matrices every time, and the matrix that does this projection is also learned.</span></span>
<span id="cb19-227"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        This is the trick behind multihead attention.</span></span>
<span id="cb19-228"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-229"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            batched_input_intrinsic_value_matrix: an ndarray of your batched input</span></span>
<span id="cb19-230"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            batched_input_response_matrix: an ndarray that is essentially the same as batched_input_intrinsic_value</span></span>
<span id="cb19-231"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            batched_input_information_matrix: same as above. you have to declare that these matrices exist </span></span>
<span id="cb19-232"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            batched_input_intrinsic_value_projection_matrix: an ndarray that projects the intrinsic value to the size for multihead attention</span></span>
<span id="cb19-233"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            batched_input_response_projection_matrix: same thing for response_matrix</span></span>
<span id="cb19-234"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            batched_input_information_projection_matrix: same thing as above</span></span>
<span id="cb19-235"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Returns:</span></span>
<span id="cb19-236"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            A tuple of ndarrays that have the concatenated result of multihead attention and also the attention weights respectively. </span></span>
<span id="cb19-237"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-238">    attention_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batched_input_intrinsic_value_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> batched_input_intrinsic_value_projection_matrix <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#we now use the inbuilt @ operator to do matrix multiplication quickly</span></span>
<span id="cb19-239">    response_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batched_input_response_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> batched_input_intrinsic_value_projection_matrix</span>
<span id="cb19-240">    information_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batched_input_information_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> batched_input_information_projection_matrix</span>
<span id="cb19-241"></span>
<span id="cb19-242">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#extract dimensions from the attention matrix (query tensor)</span></span>
<span id="cb19-243">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#attention_matrix` is the query tensor with shape (batch_size, seq_len_q, d_model)</span></span>
<span id="cb19-244">    batch, lq, d_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_matrix.shape</span>
<span id="cb19-245"></span>
<span id="cb19-246">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#extract the length of the key tensor from the response matrix</span></span>
<span id="cb19-247">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#response_matrix is the key tensor with shape (batch_size, seq_len_k, d_model)</span></span>
<span id="cb19-248">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#lk represents seq_len_k (sequence length of the response_matrix), which may differ from lq (seq_len_q)</span></span>
<span id="cb19-249">    lk<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>response_matrix.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb19-250"></span>
<span id="cb19-251">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#we don't explicitly compute `lv` (seq_len_v, length of values) because information_matrix is expected to have the same sequence length as response_matrix</span></span>
<span id="cb19-252"></span>
<span id="cb19-253">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#reshape and transpose attention_matrix to prepare for multi-head attention</span></span>
<span id="cb19-254">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#step 1: reshape attention_matrix from (batch_size, seq_len_q, d_model) to (batch_size, seq_len_q, num_heads, head_dim), where head_dim = d_model//num_heads</span></span>
<span id="cb19-255">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#step 2: transpose to (batch_size, num_heads, seq_len_q, head_dim) for easier computation per head</span></span>
<span id="cb19-256">    A<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_matrix.reshape(batch, lq, num_heads, d_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span>num_heads).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb19-257"></span>
<span id="cb19-258">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#reshape and transpose response_matrix similarly</span></span>
<span id="cb19-259">    B<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>response_matrix.reshape(batch, lk, num_heads, d_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span>num_heads).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb19-260"></span>
<span id="cb19-261">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#reshape and transpose information_matrix similarly</span></span>
<span id="cb19-262">    C<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>information_matrix.reshape(batch, lk, num_heads, d_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span>num_heads).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb19-263"></span>
<span id="cb19-264">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#compute the attention each head</span></span>
<span id="cb19-265">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#input ndarrays (A, B, B) are now split into individual heads for parallel processing</span></span>
<span id="cb19-266">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#'mask` is optional and is used to block certain positions (e.g., future positions in autoregressive decoding)</span></span>
<span id="cb19-267">    out_heads, attn_weights<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>scaled_dot_product_attention(A, B, C, mask)</span>
<span id="cb19-268"></span>
<span id="cb19-269">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#concatenate the output</span></span>
<span id="cb19-270">    out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(merge_heads(out_heads))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>final_reshaper_matrix  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Apply a linear projection to combine the head outputs into `d_model` dimensions.</span></span>
<span id="cb19-271"></span>
<span id="cb19-272">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> out, attn_weights</span>
<span id="cb19-273"></span>
<span id="cb19-274"></span>
<span id="cb19-275"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> mha_backprop(</span>
<span id="cb19-276">        dout: np.ndarray,</span>
<span id="cb19-277">        batched_input_intrinsic_value_matrix: np.ndarray,</span>
<span id="cb19-278">        batched_input_response_matrix: np.ndarray,</span>
<span id="cb19-279">        batched_input_information_matrix: np.ndarray, </span>
<span id="cb19-280">        batched_input_intrinsic_value_projection_matrix: np.ndarray,</span>
<span id="cb19-281">        batched_input_response_projection_matrix: np.ndarray, </span>
<span id="cb19-282">        batched_input_information_projection_matrix: np.ndarray, </span>
<span id="cb19-283">        final_projection_matrix: np.ndarray,</span>
<span id="cb19-284">        attention_weights: np.ndarray, </span>
<span id="cb19-285">        num_heads: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, </span>
<span id="cb19-286">        mask: Optional[np.ndarray]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb19-287">    ) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:</span>
<span id="cb19-288">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-289"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        This is the big one. This is the function that backpropagates through multihead attention. </span></span>
<span id="cb19-290"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        It computes the gradients of the loss with respect to the sequence matrices and the weight matrices used in the multi-head attention mechanism.</span></span>
<span id="cb19-291"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Like before, we are in fact feeding it the loss.</span></span>
<span id="cb19-292"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-293"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            dout: an ndarray of the gradient of the loss with respect to the output of multihead attention</span></span>
<span id="cb19-294"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            batched_input_intrinsic_value_matrix: an ndarray as defined in the multihead attention function above,</span></span>
<span id="cb19-295"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            batched_input_response_matrix: an ndarray as defined in the multihead attention function above ,</span></span>
<span id="cb19-296"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            batched_input_information_matrix: an ndarray as defined in the multihead attention function above,   </span></span>
<span id="cb19-297"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            batched_input_intrinsic_value_projection_matrix: an ndarray as defined in the multihead attention function above,</span></span>
<span id="cb19-298"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            batched_input_response_projection_matrix: an ndarray as defined in the multihead attention function above, </span></span>
<span id="cb19-299"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            batched_input_information_projection_matrix: an ndarray as defined in the multihead attention function above, </span></span>
<span id="cb19-300"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            final_projection_matrix: an ndarray as defined in the multihead attention function above,</span></span>
<span id="cb19-301"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            attention_weights: an ndarray that is the output of multihead attention function, required for backpropagation</span></span>
<span id="cb19-302"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            mask: an optional ndarray of masks (Bool data type)</span></span>
<span id="cb19-303"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-304"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            the following tuple: Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:</span></span>
<span id="cb19-305"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - differential_intrinsic_value_matrix (np.ndarray): Gradient w.r.t. the query input `batched_input_intrinsic_value_matrix`, shape (batch_size, seq_len_q, d_model).</span></span>
<span id="cb19-306"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - differential_response_matrix (np.ndarray): Gradient w.r.t. the key input `batched_input_response_matrix`, shape (batch_size, seq_len_k, d_model).</span></span>
<span id="cb19-307"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - differential_information_matrix (np.ndarray): Gradient w.r.t. the value input `batched_input_information_matrix`, shape (batch_size, seq_len_k, d_model).</span></span>
<span id="cb19-308"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - original_shape_intrinsic_value_differential (np.ndarray): Gradient w.r.t. the query weight matrix `batched_input_intrinsic_value_projection_matrix`, shape (d_model, d_model).</span></span>
<span id="cb19-309"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - original_shape_response_differential (np.ndarray): Gradient w.r.t. the key weight matrix `batched_input_response_projection_matrix`, shape (d_model, d_model).</span></span>
<span id="cb19-310"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - original_shape_information_differential (np.ndarray): Gradient w.r.t. the value weight batched_input_information_projection_matrix `W_v`, shape (d_model, d_model).</span></span>
<span id="cb19-311"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - original_shape_attention_differential (np.ndarray): Gradient w.r.t. the output weight matrix `final_projection_matrix`, shape (d_model, d_model).</span></span>
<span id="cb19-312"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-313">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#throughout the code, a variable starting with 'd' denotes a derivative/gradient, aside from d_model, which is the model dimension (user-defined really)</span></span>
<span id="cb19-314">    batch, attention_sequence_length, d_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batched_input_intrinsic_value_matrix.shape <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#compute the gradient matrix</span></span>
<span id="cb19-315">    sequence_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batched_input_response_matrix.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#length of the sequence, obtained from really any matrix</span></span>
<span id="cb19-316">    head_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span>num_heads <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#just the model head size</span></span>
<span id="cb19-317"></span>
<span id="cb19-318">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#recompute the forward step</span></span>
<span id="cb19-319">    attention_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batched_input_intrinsic_value_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> batched_input_intrinsic_value_projection_matrix <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#standard stuff</span></span>
<span id="cb19-320">    response_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batched_input_response_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> batched_input_response_projection_matrix</span>
<span id="cb19-321">    information_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batched_input_information_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> batched_input_information_projection_matrix</span>
<span id="cb19-322"></span>
<span id="cb19-323">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#similarly reshaping stuff as done before</span></span>
<span id="cb19-324">    head_attention_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_matrix.reshape(batch, attention_sequence_length, num_heads, head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb19-325">    head_response_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>response_matrix.reshape(batch, sequence_length, num_heads, head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb19-326">    head_information_matrix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>information_matrix.reshape(batch, sequence_length, num_heads, head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb19-327"></span>
<span id="cb19-328">    d_merged_heads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dout.reshape(batch, attention_sequence_length, d_model) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#reshape dout into the correct shape</span></span>
<span id="cb19-329">    d_out_heads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_merged_heads.reshape(batch, attention_sequence_length, num_heads, head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb19-330"></span>
<span id="cb19-331">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#check shape consistency, because it makes for easier debugging</span></span>
<span id="cb19-332">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> d_out_heads.shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span>(batch, num_heads, attention_sequence_length, head_dim), <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Shape mismatch in d_out_heads: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>d_out_heads<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#first time we've used 'assert'</span></span>
<span id="cb19-333"></span>
<span id="cb19-334">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#attention weights and value gradients</span></span>
<span id="cb19-335">    d_attention_weights<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(d_out_heads, head_information_matrix.transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#this is pretty standard fare, start getting the stuff out from backprop</span></span>
<span id="cb19-336">    d_vh<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(attention_weights.transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), d_out_heads) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#this is also the same thing</span></span>
<span id="cb19-337"></span>
<span id="cb19-338">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#calculate the differential element of the attention scores, for backpropagation</span></span>
<span id="cb19-339">    sum_over_j<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(attention_weights<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>d_attention_weights, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb19-340">    d_scores<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_weights<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(d_attention_weights<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>sum_over_j)</span>
<span id="cb19-341"></span>
<span id="cb19-342">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#calculate the differential elements for the attention and response matrices, per head</span></span>
<span id="cb19-343">    d_attention_head<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(d_scores, head_response_matrix)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>np.sqrt(head_dim)</span>
<span id="cb19-344">    d_response_head<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(d_scores.transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), head_attention_matrix)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>np.sqrt(head_dim)</span>
<span id="cb19-345"></span>
<span id="cb19-346">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#combine the elements back for concatenation</span></span>
<span id="cb19-347">    total_attention_differential<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_attention_head.transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>).reshape(batch, attention_sequence_length, d_model)</span>
<span id="cb19-348">    total_response_differential<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_response_head.transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>).reshape(batch, sequence_length, d_model)</span>
<span id="cb19-349">    total_information_differential<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_vh.transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>).reshape(batch, sequence_length, d_model)</span>
<span id="cb19-350"></span>
<span id="cb19-351">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#now calculate the total gradients for all elements</span></span>
<span id="cb19-352">    original_shape_intrinsic_value_differential<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(batched_input_intrinsic_value_matrix.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, d_model).T, total_attention_differential.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, d_model))</span>
<span id="cb19-353">    original_shape_response_differential<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(batched_input_response_matrix.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, d_model).T, total_response_differential.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, d_model))</span>
<span id="cb19-354">    original_shape_information_differential<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(batched_input_information_matrix.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, d_model).T, total_information_differential.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, d_model))</span>
<span id="cb19-355">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#original_shape_attention_differential=np.matmul(merge_heads(attention_weights @ information_matrix).reshape(batch*attention_sequence_length, d_model).T, dout.reshape(batch*attention_sequence_length, d_model))</span></span>
<span id="cb19-356">    C <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> information_matrix.reshape(batch, sequence_length, num_heads, head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb19-357"></span>
<span id="cb19-358">    original_shape_attention_differential <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.matmul(</span>
<span id="cb19-359">    merge_heads(np.matmul(attention_weights, C)).reshape(batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> attention_sequence_length, d_model).T,</span>
<span id="cb19-360">    dout.reshape(batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> attention_sequence_length, d_model)</span>
<span id="cb19-361">        )</span>
<span id="cb19-362">    differential_intrinsic_value_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> total_attention_differential <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> batched_input_intrinsic_value_projection_matrix.T</span>
<span id="cb19-363">    differential_response_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> total_response_differential <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> batched_input_response_projection_matrix.T</span>
<span id="cb19-364">    differential_information_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> total_information_differential <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> batched_input_information_projection_matrix.T</span>
<span id="cb19-365"></span>
<span id="cb19-366">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> differential_intrinsic_value_matrix, differential_response_matrix, differential_information_matrix, original_shape_intrinsic_value_differential, original_shape_response_differential, original_shape_information_differential, original_shape_attention_differential</span>
<span id="cb19-367"></span>
<span id="cb19-368"></span>
<span id="cb19-369"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">###############################################################################</span></span>
<span id="cb19-370"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Feed Forward Network</span></span>
<span id="cb19-371"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">###############################################################################</span></span>
<span id="cb19-372"></span>
<span id="cb19-373"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#now we define the feedforward neural network (2 layers) with backprop. we use the ReLU activation function for easy differentials</span></span>
<span id="cb19-374"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#note the different output in the definition of the feedforward network</span></span>
<span id="cb19-375"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> feed_forward(x: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tuple[np.ndarray, Tuple[np.ndarray, np.ndarray]]:</span>
<span id="cb19-376">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">""" </span></span>
<span id="cb19-377"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        This is a simple two layer feedforward neural network. The reason a cache is returned at all is because it is immensely helpful in backpropagation</span></span>
<span id="cb19-378"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-379"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            x: input ndarray of data</span></span>
<span id="cb19-380"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            W1: weight matrix for the first linear transformation</span></span>
<span id="cb19-381"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            b1: bias vector for first layer</span></span>
<span id="cb19-382"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            W2, b2: same as above for second layer</span></span>
<span id="cb19-383"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-384"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            Tuple[np.ndarray, Tuple[np.ndarray, np.ndarray]]:</span></span>
<span id="cb19-385"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - z2 (np.ndarray): the output tensor after the feed-forward computation of shape (batch_size, seq_len, d_model).</span></span>
<span id="cb19-386"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - cache (Tuple[np.ndarray, np.ndarray]): a tuple containing:</span></span>
<span id="cb19-387"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - z1 (np.ndarray): the output of the first linear transformation before the ReLU activation, of shape (batch_size, seq_len, d_ff).</span></span>
<span id="cb19-388"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - relu (np.ndarray): The output of the ReLU activation, of shape (batch_size, seq_len, d_ff).</span></span>
<span id="cb19-389"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-390">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#this is just straighforward - two matrix multiplications with a ReLU in between</span></span>
<span id="cb19-391">    z1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>W1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>b1</span>
<span id="cb19-392">    relu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.maximum(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, z1)</span>
<span id="cb19-393">    z2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>relu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>W2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>b2</span>
<span id="cb19-394">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> z2, (z1, relu)</span>
<span id="cb19-395"></span>
<span id="cb19-396"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> feed_forward_backprop(dz2: np.ndarray, x: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray, cache: Tuple[np.ndarray, np.ndarray]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:</span>
<span id="cb19-397">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-398"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        The backpropagation for a feedforward layer. Note that we are working backwards, so the input variables are defined that way</span></span>
<span id="cb19-399"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-400"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            dz2: ndarray containing gradient of the loss function with respect to output of the second layer</span></span>
<span id="cb19-401"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            x: input ndarray of data</span></span>
<span id="cb19-402"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            W1-cache: same as above</span></span>
<span id="cb19-403"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-404"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            Tuple of losses (ndarrays) with respect to x,W1,b1,W2,b2 respectively</span></span>
<span id="cb19-405"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-406">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#backpropagation, step by step, is the exact formula outlines in the tutorial</span></span>
<span id="cb19-407">    (z1, relu)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cache <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#now you see why we utilized the cache at all</span></span>
<span id="cb19-408">    batch, length, d_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x.shape</span>
<span id="cb19-409">    dW2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(relu.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, d_model).T, dz2.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, d_model))</span>
<span id="cb19-410">    db2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dz2.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb19-411">    d_relu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dz2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>W2.T</span>
<span id="cb19-412">    d_z1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_relu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(z1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb19-413">    dW1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.matmul(x.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, d_model).T, d_z1.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, d_model))</span>
<span id="cb19-414">    db1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_z1.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb19-415">    dX<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_z1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>W1.T</span>
<span id="cb19-416">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> dX, dW1, db1, dW2, db2</span>
<span id="cb19-417"></span>
<span id="cb19-418"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">###############################################################################</span></span>
<span id="cb19-419"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Encoder and Decoder Blocks</span></span>
<span id="cb19-420"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">###############################################################################</span></span>
<span id="cb19-421"></span>
<span id="cb19-422"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#we now build the actual encoder-decoder layer.</span></span>
<span id="cb19-423"></span>
<span id="cb19-424"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> encoder_layer(x: np.ndarray, intrinsic_value_projector: np.ndarray, response_projector: np.ndarray, information_projector: np.ndarray, attention_projector: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray, num_heads: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tuple[np.ndarray, Tuple]:</span>
<span id="cb19-425">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-426"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Now we build the transformer in earnest from all of our classes. This implements the encoder layer using the structure outlined in the tutorial</span></span>
<span id="cb19-427"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-428"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            x: ndarray of input data</span></span>
<span id="cb19-429"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            intrinsic_value_projector, response_projector, information_projector, attention_projector: ndarrays containing projection matrices for input data</span></span>
<span id="cb19-430"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            W1, b1, W2, b2: ndarrays of the two-layer neural network</span></span>
<span id="cb19-431"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            num_heads: number of attention heads</span></span>
<span id="cb19-432"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-433"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            out: output ndarray of the entire encoder</span></span>
<span id="cb19-434"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            cache: a Tuple of all cached values for backpropagation</span></span>
<span id="cb19-435"></span>
<span id="cb19-436"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-437">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#at this point i start using smaller variable names because the program becomes tedious to read. anyway, this code is self-explanatory </span></span>
<span id="cb19-438">    x_norm, mean1, var1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>layer_norm(x)</span>
<span id="cb19-439">    attn_out, attn_w<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>multi_head_attention(x_norm, x_norm, x_norm, intrinsic_value_projector, response_projector, information_projector, attention_projector, num_heads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_heads)</span>
<span id="cb19-440">    x2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>attn_out</span>
<span id="cb19-441"></span>
<span id="cb19-442">    x2_norm, mean2, var2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>layer_norm(x2)</span>
<span id="cb19-443">    ff_out, ff_cache<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>feed_forward(x2_norm,W1,b1,W2,b2)</span>
<span id="cb19-444">    out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>ff_out</span>
<span id="cb19-445">    cache<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(x, x2, x_norm, x2_norm, mean1, var1, mean2, var2, attn_w, ff_cache)</span>
<span id="cb19-446">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> out, cache</span>
<span id="cb19-447"></span>
<span id="cb19-448"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> decoder_layer(x, encoder_out,</span>
<span id="cb19-449">                  intrinsic_weight_masked, response_weight_masked, information_weight_masked, attention_weight_masked,</span>
<span id="cb19-450">                  intrinsic_crossattention_weight, response_crossattention_weight, information_crossattention_weight, attention_crossattention_weight, W1, b1, W2, b2, mask, num_heads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>):</span>
<span id="cb19-451">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-452"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        We implement the decoder layer with masked attention.</span></span>
<span id="cb19-453"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs: </span></span>
<span id="cb19-454"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            x: ndarray of input data</span></span>
<span id="cb19-455"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            intrinsic_weight_masked: projection matrix for masked self-attention for the intrinsic value weight</span></span>
<span id="cb19-456"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            response_weight_masked: projection matrix for masked self-attention for the response weight</span></span>
<span id="cb19-457"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            information_weight_masked: projection matrix for masked self-attention for the information weight</span></span>
<span id="cb19-458"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            attention_weight_masked: projection matrix for masked self-attention for the attention weights</span></span>
<span id="cb19-459"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            intrinsic_crossattention_weight: projection matrix for the intrinsic value weight during cross-attention</span></span>
<span id="cb19-460"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            response_crossattention_weight: projection matrix for the response weight during cross-attention</span></span>
<span id="cb19-461"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            information_crossattention_weight: projection matrix for the information weight during cross-attention</span></span>
<span id="cb19-462"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            attention_crossattention_weight: projection matrix for the attention weights during cross-attention</span></span>
<span id="cb19-463"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            W1,b1,W2,b2, mask: all defined above</span></span>
<span id="cb19-464"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            num_heads: number of attention instances</span></span>
<span id="cb19-465"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-466"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            Tuple[np.ndarray, Tuple]:</span></span>
<span id="cb19-467"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - out: output ndarray of the decoder layer, shape (batch_size, seq_len, d_model)</span></span>
<span id="cb19-468"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - cache (Tuple): cached values for backpropagation, including:</span></span>
<span id="cb19-469"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - x, x2, x3 : intermediate ndarrays at various stages</span></span>
<span id="cb19-470"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - x_norm_dec1, x2_norm, x3_norm : layer-normalized ndarrays</span></span>
<span id="cb19-471"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - mean_dec1, var_dec1, mean_dec2, var_dec2, mean_dec3, var_dec3: Means and variances (in ndarrays) from layer normalization</span></span>
<span id="cb19-472"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - masked_attn_w, cross_attn_w: attention weight ndarrays from masked self-attention and cross-attention</span></span>
<span id="cb19-473"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - ff_cache_dec (Tuple): cached values from the feed-forward network</span></span>
<span id="cb19-474"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-475">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#this just implements the decoder layer</span></span>
<span id="cb19-476"></span>
<span id="cb19-477">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#first, apply layer normalization, then compute multihead attention</span></span>
<span id="cb19-478">    x_norm_dec1,mean_dec1,var_dec1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>layer_norm(x)</span>
<span id="cb19-479">    masked_attn_out, masked_attn_w<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>multi_head_attention(x_norm_dec1, x_norm_dec1, x_norm_dec1,</span>
<span id="cb19-480">                                                          intrinsic_weight_masked, response_weight_masked, information_weight_masked, attention_weight_masked, </span>
<span id="cb19-481">                                                          num_heads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_heads, mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>mask)</span>
<span id="cb19-482">    x2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>masked_attn_out</span>
<span id="cb19-483">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#then layernorm again</span></span>
<span id="cb19-484">    x2_norm, mean_dec2, var_dec2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>layer_norm(x2)</span>
<span id="cb19-485">    cross_attn_out, cross_attn_w<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>multi_head_attention(x2_norm, encoder_out, encoder_out,</span>
<span id="cb19-486">                                                        intrinsic_crossattention_weight, response_crossattention_weight, information_crossattention_weight, attention_crossattention_weight,</span>
<span id="cb19-487">                                                        num_heads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_heads, mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>)</span>
<span id="cb19-488">    x3<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>cross_attn_out</span>
<span id="cb19-489">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#layer norm thrice</span></span>
<span id="cb19-490">    x3_norm, mean_dec3, var_dec3<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>layer_norm(x3)</span>
<span id="cb19-491">    ff_out, ff_cache_dec<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>feed_forward(x3_norm, W1, b1, W2, b2)</span>
<span id="cb19-492">    out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x3<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>ff_out</span>
<span id="cb19-493">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#prepare the cache for backprop</span></span>
<span id="cb19-494">    cache<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(x, x2, x3, x_norm_dec1, x2_norm, x3_norm, mean_dec1, var_dec1, mean_dec2, var_dec2, mean_dec3, var_dec3, masked_attn_w, cross_attn_w, ff_cache_dec)</span>
<span id="cb19-495">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> out, cache</span>
<span id="cb19-496"></span>
<span id="cb19-497"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">###############################################################################</span></span>
<span id="cb19-498"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Forward and Backprop Through Model (Single Layer Encoder-Decoder)</span></span>
<span id="cb19-499"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">###############################################################################</span></span>
<span id="cb19-500"></span>
<span id="cb19-501"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#at this point </span></span>
<span id="cb19-502"></span>
<span id="cb19-503"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward_transformer(enc_in: np.ndarray, dec_in: np.ndarray,</span>
<span id="cb19-504">                        intrinsic_value_weight_enc: np.ndarray, response_weight_enc: np.ndarray, information_weight_enc: np.ndarray, attention_weight_enc: np.ndarray, W1_enc: np.ndarray, b1_enc: np.ndarray, W2_enc: np.ndarray, b2_enc: np.ndarray,</span>
<span id="cb19-505">                        intrinsic_value_weight_dec_masked: np.ndarray, response_weight_dec_masked: np.ndarray, information_weight_dec_masked: np.ndarray, attention_weight_dec_masked: np.ndarray,</span>
<span id="cb19-506">                        intrinsic_value_weight_dec_cross: np.ndarray, response_weight_dec_cross: np.ndarray, information_weight_dec_cross: np.ndarray, attention_weight_dec_cross: np.ndarray,</span>
<span id="cb19-507">                        W1_dec: np.ndarray, b1_dec: np.ndarray, W2_dec: np.ndarray, b2_dec: np.ndarray,</span>
<span id="cb19-508">                        W_embed_out: np.ndarray, b_embed_out: np.ndarray,</span>
<span id="cb19-509">                        src_mask: np.ndarray, tgt_mask: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tuple[np.ndarray, Tuple]:</span>
<span id="cb19-510">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-511"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        We implement the forward pass for the transformer model. This is fairly straightforward and how we've defined it</span></span>
<span id="cb19-512"></span>
<span id="cb19-513"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-514"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            enc_in: ndarray of input data to the encoder, shape (batch_size, src_len, d_model)</span></span>
<span id="cb19-515"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            dec_in: ndarray of input data to the decoder, shape (batch_size, tgt_len, d_model)</span></span>
<span id="cb19-516"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            intrinsic_value_weight_enc: projection matrix for the query in the encoder's self-attention</span></span>
<span id="cb19-517"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            response_value_weight_enc: projection matrix for the key in the encoder's self-attention</span></span>
<span id="cb19-518"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            information_value_weight_enc: projection matrix for the value in the encoder's self-attention</span></span>
<span id="cb19-519"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            attention_value_weight_enc: projection matrix for the output in the encoder's self-attention</span></span>
<span id="cb19-520"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            W1_enc, b1_enc, W2_enc, b2_enc: feed-forward network weights and biases for the encoder</span></span>
<span id="cb19-521"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            intrinsic_value_weight_dec_masked: projection matrix for the query in the decoder's masked self-attention</span></span>
<span id="cb19-522"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            response_value_weight_dec_masked: projection matrix for the key in the decoder's masked self-attention</span></span>
<span id="cb19-523"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            information_value_weight_dec_masked: projection matrix for the value in the decoder's masked self-attention</span></span>
<span id="cb19-524"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            attention_value_weight_dec_masked: projection matrix for the output in the decoder's masked self-attention</span></span>
<span id="cb19-525"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            intrinsic_value_weight_dec_cross: projection matrix for the query in the decoder's cross-attention</span></span>
<span id="cb19-526"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            response_value_weight_dec_cross: projection matrix for the key in the decoder's cross-attention</span></span>
<span id="cb19-527"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            information_value_weight_dec_cross: projection matrix for the value in the decoder's cross-attention</span></span>
<span id="cb19-528"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            attention_value_weight_dec_cross: projection matrix for the output in the decoder's cross-attention</span></span>
<span id="cb19-529"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            W1_dec, b1_dec, W2_dec, b2_dec: feed-forward network weights and biases for the decoder</span></span>
<span id="cb19-530"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            W_embed_out: projection matrix for mapping decoder output to the vocabulary space, shape (d_model, vocab_size)</span></span>
<span id="cb19-531"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            b_embed_out: bias vector for mapping decoder output to the vocabulary space, shape (vocab_size,)</span></span>
<span id="cb19-532"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            src_mask: ndarray mask for the encoder, shape (src_len, src_len)</span></span>
<span id="cb19-533"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            tgt_mask: ndarray mask for the decoder, shape (tgt_len, tgt_len)</span></span>
<span id="cb19-534"></span>
<span id="cb19-535"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-536"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            Tuple[np.ndarray, Tuple]:</span></span>
<span id="cb19-537"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - probs: output probabilities over the vocabulary, shape (batch_size, tgt_len, vocab_size)</span></span>
<span id="cb19-538"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - cache (Tuple): cached values for backpropagation, including:</span></span>
<span id="cb19-539"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - enc_out: ndarray of the encoder output, shape (batch_size, src_len, d_model)</span></span>
<span id="cb19-540"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - enc_cache: cached intermediate values from the encoder</span></span>
<span id="cb19-541"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - dec_out: ndarray of the decoder output, shape (batch_size, tgt_len, d_model)</span></span>
<span id="cb19-542"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - dec_cache: cached intermediate values from the decoder</span></span>
<span id="cb19-543"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-544"></span>
<span id="cb19-545">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#this should be fairly straighforward by now, how it's implemented. just feed your weights in and go through the entire layer</span></span>
<span id="cb19-546">    enc_out, enc_cache<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>encoder_layer(enc_in, intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc, W1_enc, b1_enc, W2_enc, b2_enc)</span>
<span id="cb19-547">    dec_out, dec_cache<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>decoder_layer(dec_in, enc_out, intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,</span>
<span id="cb19-548">                                       intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,</span>
<span id="cb19-549">                                       W1_dec, b1_dec, W2_dec, b2_dec,</span>
<span id="cb19-550">                                       mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tgt_mask)</span>
<span id="cb19-551">    logits<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dec_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>W_embed_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>b_embed_out</span>
<span id="cb19-552">    probs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>softmax(logits)</span>
<span id="cb19-553">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> probs, (enc_out, enc_cache, dec_out, dec_cache)</span>
<span id="cb19-554"></span>
<span id="cb19-555"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> backward_transformer(dprobs: np.ndarray,</span>
<span id="cb19-556">    enc_in: np.ndarray,</span>
<span id="cb19-557">    dec_in: np.ndarray,</span>
<span id="cb19-558">    enc_out: np.ndarray,</span>
<span id="cb19-559">    enc_cache: Tuple,</span>
<span id="cb19-560">    dec_out: np.ndarray,</span>
<span id="cb19-561">    dec_cache: Tuple,</span>
<span id="cb19-562">    intrinsic_value_weight_enc: np.ndarray,</span>
<span id="cb19-563">    response_weight_enc: np.ndarray,</span>
<span id="cb19-564">    information_weight_enc: np.ndarray,</span>
<span id="cb19-565">    attention_weight_enc: np.ndarray,</span>
<span id="cb19-566">    W1_enc: np.ndarray,</span>
<span id="cb19-567">    b1_enc: np.ndarray,</span>
<span id="cb19-568">    W2_enc: np.ndarray,</span>
<span id="cb19-569">    b2_enc: np.ndarray,</span>
<span id="cb19-570">    intrinsic_value_weight_dec_masked: np.ndarray,</span>
<span id="cb19-571">    response_weight_dec_masked: np.ndarray,</span>
<span id="cb19-572">    information_weight_dec_masked: np.ndarray,</span>
<span id="cb19-573">    attention_weight_dec_masked: np.ndarray,</span>
<span id="cb19-574">    intrinsic_value_weight_dec_cross: np.ndarray,</span>
<span id="cb19-575">    response_weight_dec_cross: np.ndarray,</span>
<span id="cb19-576">    information_weight_dec_cross: np.ndarray,</span>
<span id="cb19-577">    attention_weight_dec_cross: np.ndarray,</span>
<span id="cb19-578">    W1_dec: np.ndarray,</span>
<span id="cb19-579">    b1_dec: np.ndarray,</span>
<span id="cb19-580">    W2_dec: np.ndarray,</span>
<span id="cb19-581">    b2_dec: np.ndarray,</span>
<span id="cb19-582">    W_embed_out: np.ndarray,</span>
<span id="cb19-583">    b_embed_out: np.ndarray,</span>
<span id="cb19-584">    src_mask: np.ndarray,</span>
<span id="cb19-585">    tgt_mask: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tuple[Dict[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, np.ndarray], np.ndarray]:</span>
<span id="cb19-586">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb19-587"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        This is possibly the hardest part in the code. This is complete backpropagation for the transformer, through all layers.</span></span>
<span id="cb19-588"></span>
<span id="cb19-589"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Inputs:</span></span>
<span id="cb19-590"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            dprobs: ndarray of gradients with respect to output probabilities, shape (batch_size, tgt_len, vocab_size)</span></span>
<span id="cb19-591"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            enc_in: ndarray of input data to the encoder, shape (batch_size, src_len, d_model)</span></span>
<span id="cb19-592"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            dec_in: ndarray of input data to the decoder, shape (batch_size, tgt_len, d_model)</span></span>
<span id="cb19-593"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            enc_out: ndarray of encoder outputs, shape (batch_size, src_len, d_model)</span></span>
<span id="cb19-594"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            enc_cache: cached values from the encoder forward pass</span></span>
<span id="cb19-595"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            dec_out: ndarray of decoder outputs, shape (batch_size, tgt_len, d_model)</span></span>
<span id="cb19-596"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            dec_cache: cached values from the decoder forward pass</span></span>
<span id="cb19-597"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            intrinsic_value_weight_enc: projection matrix for the query in the encoder's self-attention</span></span>
<span id="cb19-598"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            response_weight_enc: projection matrix for the key in the encoder's self-attention</span></span>
<span id="cb19-599"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            information_weight_enc: projection matrix for the value in the encoder's self-attention</span></span>
<span id="cb19-600"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            attention_weight_enc: projection matrix for the output in the encoder's self-attention</span></span>
<span id="cb19-601"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            W1_enc, b1_enc, W2_enc, b2_enc: feed-forward network weights and biases for the encoder</span></span>
<span id="cb19-602"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            intrinsic_value_weight_dec_masked: projection matrix for the query in the decoder's masked self-attention</span></span>
<span id="cb19-603"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            response_weight_dec_masked: projection matrix for the key in the decoder's masked self-attention</span></span>
<span id="cb19-604"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            information_weight_dec_masked: projection matrix for the value in the decoder's masked self-attention</span></span>
<span id="cb19-605"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            attention_weight_dec_masked: projection matrix for the output in the decoder's masked self-attention</span></span>
<span id="cb19-606"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            intrinsic_value_weight_dec_cross: projection matrix for the query in the decoder's cross-attention</span></span>
<span id="cb19-607"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            response_weight_dec_cross: projection matrix for the key in the decoder's cross-attention</span></span>
<span id="cb19-608"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            information_weight_dec_cross: projection matrix for the value in the decoder's cross-attention</span></span>
<span id="cb19-609"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            attention_weight_dec_cross: projection matrix for the output in the decoder's cross-attention</span></span>
<span id="cb19-610"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            W1_dec, b1_dec, W2_dec, b2_dec: feed-forward network weights and biases for the decoder</span></span>
<span id="cb19-611"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            W_embed_out: projection matrix for mapping decoder outputs to the vocabulary, shape (d_model, vocab_size)</span></span>
<span id="cb19-612"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            b_embed_out: bias vector for mapping decoder outputs to the vocabulary, shape (vocab_size,)</span></span>
<span id="cb19-613"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            src_mask: mask for the source sequence, shape (src_len, src_len)</span></span>
<span id="cb19-614"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            tgt_mask: mask for the target sequence, shape (tgt_len, tgt_len)</span></span>
<span id="cb19-615"></span>
<span id="cb19-616"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Outputs:</span></span>
<span id="cb19-617"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            Tuple[Dict[str, np.ndarray], np.ndarray]:</span></span>
<span id="cb19-618"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - grads: dictionary containing gradients for all trainable weights and biases, including:</span></span>
<span id="cb19-619"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc: gradients for encoder self-attention weights</span></span>
<span id="cb19-620"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - W1_enc, b1_enc, W2_enc, b2_enc: gradients for encoder feed-forward network</span></span>
<span id="cb19-621"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked: gradients for decoder masked self-attention weights</span></span>
<span id="cb19-622"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross: gradients for decoder cross-attention weights</span></span>
<span id="cb19-623"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - W1_dec, b1_dec, W2_dec, b2_dec: gradients for decoder feed-forward network</span></span>
<span id="cb19-624"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">                - W_embed_out, b_embed_out: gradients for output projection layer</span></span>
<span id="cb19-625"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            - dx_enc1: gradient with respect to the encoder input, shape (batch_size, src_len, d_model)</span></span>
<span id="cb19-626"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb19-627"></span>
<span id="cb19-628">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#this is how backprop is implemented</span></span>
<span id="cb19-629">    batch, length, d_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dec_out.shape  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#first, extract the output of the decoder. what we are really interested in is the model dimension</span></span>
<span id="cb19-630">    vocab_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>W_embed_out.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#and get the vocabulary size</span></span>
<span id="cb19-631"></span>
<span id="cb19-632">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#start: backprop through the final layer</span></span>
<span id="cb19-633">    </span>
<span id="cb19-634">    dW_embed_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dec_out.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, d_model).T<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>dprobs.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, vocab_size)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Shape: (d_model, vocab_size)</span></span>
<span id="cb19-635">    db_embed_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dprobs.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Shape: (vocab_size,)</span></span>
<span id="cb19-636">    d_dec_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dprobs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> W_embed_out.T  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Gradient w.r.t decoder output, shape (batch, tgt_len, d_model)</span></span>
<span id="cb19-637"></span>
<span id="cb19-638">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#get all values from the decoder - this is called 'unpacking'</span></span>
<span id="cb19-639">    (x, x2, x3,</span>
<span id="cb19-640">     x_norm_dec1, x2_norm_dec, x3_norm_dec,</span>
<span id="cb19-641">     mean_dec1, var_dec1, mean_dec2, var_dec2, mean_dec3, var_dec3,</span>
<span id="cb19-642">     masked_attn_w, cross_attn_w, ff_cache_dec)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dec_cache</span>
<span id="cb19-643"></span>
<span id="cb19-644">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#backprop through the feedforward neural network for the decoder</span></span>
<span id="cb19-645">    d_x3_ff<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_dec_out</span>
<span id="cb19-646">    d_x3_ff, dW1_dec, db1_dec, dW2_dec, db2_dec<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>feed_forward_backprop(d_x3_ff, x3_norm_dec, W1_dec, b1_dec, W2_dec, b2_dec, ff_cache_dec)</span>
<span id="cb19-647">    dx3_norm<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_x3_ff</span>
<span id="cb19-648">    dx3<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>layer_norm_backprop(dx3_norm, x3_norm_dec, mean_dec3, var_dec3)</span>
<span id="cb19-649">    d_x3_skip<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dx3</span>
<span id="cb19-650"></span>
<span id="cb19-651">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#backprop through crossattention</span></span>
<span id="cb19-652">    d_x2_cross<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_x3_skip</span>
<span id="cb19-653">    dx_cross_Q, dx_cross_K, dx_cross_V, dintrinsic_value_weight_dec_cross_, dresponse_weight_dec_cross_, dinformation_weight_dec_cross_, dattention_weight_dec_cross_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mha_backprop(</span>
<span id="cb19-654">        d_x2_cross, x2_norm_dec, enc_out, enc_out,</span>
<span id="cb19-655">        intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,</span>
<span id="cb19-656">        cross_attn_w, num_heads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb19-657">    )</span>
<span id="cb19-658">    d_x2_skip<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dx_cross_Q</span>
<span id="cb19-659">    d_enc_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dx_cross_K<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>dx_cross_V</span>
<span id="cb19-660"></span>
<span id="cb19-661">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#backprop through masked selfattention</span></span>
<span id="cb19-662">    d_x2_masked<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_x2_skip</span>
<span id="cb19-663">    dx_masked_Q, dx_masked_K, dx_masked_V, dintrinsic_value_weight_dec_masked_, dresponse_weight_dec_masked_, dinformation_weight_dec_masked_, dattention_weight_dec_masked_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mha_backprop(</span>
<span id="cb19-664">        d_x2_masked, x_norm_dec1, x_norm_dec1, x_norm_dec1,</span>
<span id="cb19-665">        intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,</span>
<span id="cb19-666">        masked_attn_w, num_heads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tgt_mask</span>
<span id="cb19-667">    )</span>
<span id="cb19-668">    dx_norm_dec1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dx_masked_Q<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>dx_masked_K<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>dx_masked_V</span>
<span id="cb19-669">    dx_dec1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>layer_norm_backprop(dx_norm_dec1, x_norm_dec1, mean_dec1, var_dec1)</span>
<span id="cb19-670"></span>
<span id="cb19-671">    dx2_norm<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_x2_cross</span>
<span id="cb19-672">    dx2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>layer_norm_backprop(dx2_norm, x2_norm_dec, mean_dec2, var_dec2)</span>
<span id="cb19-673"></span>
<span id="cb19-674">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#combine different layers' gradients</span></span>
<span id="cb19-675">    dx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dx_dec1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>dx2</span>
<span id="cb19-676"></span>
<span id="cb19-677">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#unpack encoder values</span></span>
<span id="cb19-678">    (enc_x, enc_x2, enc_x_norm, enc_x2_norm, enc_mean1, enc_var1, enc_mean2, enc_var2, enc_attn_w, enc_ff_cache)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>enc_cache</span>
<span id="cb19-679"></span>
<span id="cb19-680">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#backprop through the encoder</span></span>
<span id="cb19-681">    d_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_enc_out</span>
<span id="cb19-682">    d_enc_ff, dW1_enc, db1_enc, dW2_enc, db2_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>feed_forward_backprop(d_enc, enc_x2_norm, W1_enc, b1_enc, W2_enc, b2_enc, enc_ff_cache)</span>
<span id="cb19-683">    d_enc2_norm<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_enc_ff</span>
<span id="cb19-684">    d_enc2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>layer_norm_backprop(d_enc2_norm, enc_x2_norm, enc_mean2, enc_var2)</span>
<span id="cb19-685"></span>
<span id="cb19-686">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#backprop through encoder's attention</span></span>
<span id="cb19-687">    dx_enc_Q, dx_enc_K, dx_enc_V, dintrinsic_value_weight_enc_, dresponse_weight_enc_, dinformation_weight_enc_, dattention_weight_enc_<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>mha_backprop(</span>
<span id="cb19-688">        d_enc2, enc_x_norm, enc_x_norm, enc_x_norm,</span>
<span id="cb19-689">        intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc,</span>
<span id="cb19-690">        enc_attn_w, num_heads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb19-691">    )</span>
<span id="cb19-692">    d_enc_norm1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dx_enc_Q<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>dx_enc_K<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>dx_enc_V</span>
<span id="cb19-693">    dx_enc1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>layer_norm_backprop(d_enc_norm1, enc_x_norm, enc_mean1, enc_var1)</span>
<span id="cb19-694"></span>
<span id="cb19-695">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#combine all gradients in a dictionary </span></span>
<span id="cb19-696">    grads <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb19-697">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'intrinsic_value_weight_enc'</span>: dintrinsic_value_weight_enc_, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'response_weight_enc'</span>: dresponse_weight_enc_, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'information_weight_enc'</span>: dinformation_weight_enc_, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_weight_enc'</span>: dattention_weight_enc_,</span>
<span id="cb19-698">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W1_enc'</span>: dW1_enc, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b1_enc'</span>: db1_enc, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W2_enc'</span>: dW2_enc, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b2_enc'</span>: db2_enc,</span>
<span id="cb19-699">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'intrinsic_value_weight_dec_masked'</span>: dintrinsic_value_weight_dec_masked_, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'response_weight_dec_masked'</span>: dresponse_weight_dec_masked_, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'information_weight_dec_masked'</span>: dinformation_weight_dec_masked_, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_weight_dec_masked'</span>: dattention_weight_dec_masked_,</span>
<span id="cb19-700">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'intrinsic_value_weight_dec_cross'</span>: dintrinsic_value_weight_dec_cross_, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'response_weight_dec_cross'</span>: dresponse_weight_dec_cross_, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'information_weight_dec_cross'</span>: dinformation_weight_dec_cross_, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_weight_dec_cross'</span>: dattention_weight_dec_cross_,</span>
<span id="cb19-701">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W1_dec'</span>: dW1_dec, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b1_dec'</span>: db1_dec, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W2_dec'</span>: dW2_dec, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b2_dec'</span>: db2_dec,</span>
<span id="cb19-702">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W_embed_out'</span>: dW_embed_out, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b_embed_out'</span>: db_embed_out</span>
<span id="cb19-703">    }</span>
<span id="cb19-704"></span>
<span id="cb19-705">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> grads, dx_enc1</span>
<span id="cb19-706"></span>
<span id="cb19-707"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">###############################################################################</span></span>
<span id="cb19-708"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Main Training Setup</span></span>
<span id="cb19-709"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">###############################################################################</span></span>
<span id="cb19-710"></span>
<span id="cb19-711"></span>
<span id="cb19-712"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#we are finally done. let's now train the actual transformer</span></span>
<span id="cb19-713">english_sentences<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb19-714">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"My rabbit likes bananas"</span></span>
<span id="cb19-715">]</span>
<span id="cb19-716"></span>
<span id="cb19-717">italian_sentences<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb19-718">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Al mio coniglio piacciono le banane"</span>,</span>
<span id="cb19-719">]</span>
<span id="cb19-720"></span>
<span id="cb19-721">eng_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[whitespace_tokenizer(s) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> english_sentences]</span>
<span id="cb19-722">for_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[whitespace_tokenizer(s) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> italian_sentences]</span>
<span id="cb19-723"></span>
<span id="cb19-724">eng_word2idx, eng_idx2word<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>build_vocab(eng_tokens)</span>
<span id="cb19-725">for_word2idx, for_idx2word<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>build_vocab(for_tokens)</span>
<span id="cb19-726"></span>
<span id="cb19-727"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> encode(tokens, w2i):</span>
<span id="cb19-728">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> [w2i[t] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> tokens]</span>
<span id="cb19-729"></span>
<span id="cb19-730">eng_encoded<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[encode(t, eng_word2idx) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> eng_tokens]</span>
<span id="cb19-731">for_encoded<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[encode(t, for_word2idx) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> for_tokens]</span>
<span id="cb19-732"></span>
<span id="cb19-733">max_eng_len<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> eng_encoded)</span>
<span id="cb19-734">max_for_len<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> for_encoded)</span>
<span id="cb19-735"></span>
<span id="cb19-736"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Add start/end tokens</span></span>
<span id="cb19-737">start_token<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(for_word2idx)</span>
<span id="cb19-738">end_token<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(for_word2idx)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb19-739">for_word2idx[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;start&gt;"</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>start_token</span>
<span id="cb19-740">for_word2idx[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;end&gt;"</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>end_token</span>
<span id="cb19-741">for_idx2word[start_token]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;start&gt;"</span></span>
<span id="cb19-742">for_idx2word[end_token]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;end&gt;"</span></span>
<span id="cb19-743"></span>
<span id="cb19-744">for_idx2word<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{idx: token <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> token, idx <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> for_word2idx.items()}</span>
<span id="cb19-745"></span>
<span id="cb19-746">for_encoded_input<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[[start_token]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>seq <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> seq <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> for_encoded]</span>
<span id="cb19-747">for_encoded_target<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[seq<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>[end_token] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> seq <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> for_encoded]</span>
<span id="cb19-748"></span>
<span id="cb19-749">max_for_len_inp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(s) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> for_encoded_input)</span>
<span id="cb19-750">max_for_len_tgt<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(s) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> for_encoded_target)</span>
<span id="cb19-751"></span>
<span id="cb19-752">eng_padded<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pad_sequences(eng_encoded, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>max_eng_len, pad_value<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb19-753">for_inp_padded<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pad_sequences(for_encoded_input, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>max_for_len_inp, pad_value<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb19-754">for_tgt_padded<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pad_sequences(for_encoded_target, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>max_for_len_tgt, pad_value<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb19-755"></span>
<span id="cb19-756">batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(eng_padded)</span>
<span id="cb19-757">src_len<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eng_padded.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb19-758">tgt_len<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>for_inp_padded.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb19-759">vocab_size_src<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(eng_word2idx)</span>
<span id="cb19-760">vocab_size_tgt<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(for_word2idx)</span>
<span id="cb19-761">d_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#this is arbitrary. many AI companies sell access to their embeddings and dimensions</span></span>
<span id="cb19-762"></span>
<span id="cb19-763">src_embeddings<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(vocab_size_src, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#i chose 0.01 for a balance between numerical stability and demonstrating the power of transformers</span></span>
<span id="cb19-764">tgt_embeddings<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(vocab_size_tgt, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-765"></span>
<span id="cb19-766"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#we haven't actually defined the embedding function- let's embed our vocabulary</span></span>
<span id="cb19-767"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> embed(x, emb):</span>
<span id="cb19-768">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> emb[x] </span>
<span id="cb19-769"></span>
<span id="cb19-770"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#this next section is just defining random matrices</span></span>
<span id="cb19-771"></span>
<span id="cb19-772">W_embed_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, vocab_size_tgt)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#define this random matrix for embedding</span></span>
<span id="cb19-773">b_embed_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.zeros(vocab_size_tgt) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#and the bias of the weights as well</span></span>
<span id="cb19-774"></span>
<span id="cb19-775">intrinsic_value_weight_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span> </span>
<span id="cb19-776">response_weight_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-777">information_weight_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-778">attention_weight_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-779">W1_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-780">b1_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.zeros(d_model)</span>
<span id="cb19-781">W2_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-782">b2_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.zeros(d_model)</span>
<span id="cb19-783"></span>
<span id="cb19-784">intrinsic_value_weight_dec_masked<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-785">response_weight_dec_masked<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-786">information_weight_dec_masked<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-787">attention_weight_dec_masked<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-788"></span>
<span id="cb19-789">intrinsic_value_weight_dec_cross<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-790">response_weight_dec_cross<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-791">information_weight_dec_cross<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-792">attention_weight_dec_cross<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-793"></span>
<span id="cb19-794">W1_dec<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-795">b1_dec<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.zeros(d_model)</span>
<span id="cb19-796">W2_dec<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.random.randn(d_model, d_model)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-797">b2_dec<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.zeros(d_model)</span>
<span id="cb19-798"></span>
<span id="cb19-799"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#here we define the learning rate and epochs</span></span>
<span id="cb19-800">learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb19-801">epochs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span></span>
<span id="cb19-802"></span>
<span id="cb19-803">src_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb19-804">tgt_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>create_mask_for_removing_future_dependency(for_inp_padded)</span>
<span id="cb19-805"></span>
<span id="cb19-806"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#and implement the generic neural network training</span></span>
<span id="cb19-807"></span>
<span id="cb19-808"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(epochs):</span>
<span id="cb19-809">    enc_inp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>embed(eng_padded, src_embeddings)</span>
<span id="cb19-810">    dec_inp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>embed(for_inp_padded, tgt_embeddings)</span>
<span id="cb19-811"></span>
<span id="cb19-812">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#this is the forward pass</span></span>
<span id="cb19-813">    probs, cache<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>forward_transformer(enc_inp, dec_inp,</span>
<span id="cb19-814">                                       intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc, W1_enc, b1_enc, W2_enc, b2_enc,</span>
<span id="cb19-815">                                       intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,</span>
<span id="cb19-816">                                       intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,</span>
<span id="cb19-817">                                       W1_dec, b1_dec, W2_dec, b2_dec,</span>
<span id="cb19-818">                                       W_embed_out, b_embed_out,</span>
<span id="cb19-819">                                       src_mask, tgt_mask)</span>
<span id="cb19-820"></span>
<span id="cb19-821">    loss<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cross_entropy_loss(probs, for_tgt_padded)</span>
<span id="cb19-822">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Epoch </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>epoch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, Loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb19-823"></span>
<span id="cb19-824">    pred_indices<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.argmax(probs, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb19-825">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> b <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(batch_size):</span>
<span id="cb19-826">        predicted_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[for_idx2word[idx] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> idx <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> pred_indices[b]]</span>
<span id="cb19-827">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Predicted:"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>.join(predicted_tokens))</span>
<span id="cb19-828">    </span>
<span id="cb19-829">    dprobs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cross_entropy_derivative(probs, for_tgt_padded)</span>
<span id="cb19-830"></span>
<span id="cb19-831">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#this is the backward pass</span></span>
<span id="cb19-832">    grads, dx_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>backward_transformer(dprobs, enc_inp, dec_inp, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>cache,</span>
<span id="cb19-833">                                         intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc, W1_enc, b1_enc, W2_enc, b2_enc,</span>
<span id="cb19-834">                                         intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,</span>
<span id="cb19-835">                                         intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,</span>
<span id="cb19-836">                                         W1_dec, b1_dec, W2_dec, b2_dec,</span>
<span id="cb19-837">                                         W_embed_out, b_embed_out,</span>
<span id="cb19-838">                                         src_mask, tgt_mask)</span>
<span id="cb19-839"></span>
<span id="cb19-840">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#update all parameters after backprop</span></span>
<span id="cb19-841">    intrinsic_value_weight_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'intrinsic_value_weight_enc'</span>]</span>
<span id="cb19-842">    response_weight_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'response_weight_enc'</span>]</span>
<span id="cb19-843">    information_weight_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'information_weight_enc'</span>]</span>
<span id="cb19-844">    attention_weight_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_weight_enc'</span>]</span>
<span id="cb19-845">    W1_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W1_enc'</span>]</span>
<span id="cb19-846">    b1_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b1_enc'</span>]</span>
<span id="cb19-847">    W2_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W2_enc'</span>]</span>
<span id="cb19-848">    b2_enc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b2_enc'</span>]</span>
<span id="cb19-849"></span>
<span id="cb19-850">    intrinsic_value_weight_dec_masked<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'intrinsic_value_weight_dec_masked'</span>]</span>
<span id="cb19-851">    response_weight_dec_masked<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'response_weight_dec_masked'</span>]</span>
<span id="cb19-852">    information_weight_dec_masked<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'information_weight_dec_masked'</span>]</span>
<span id="cb19-853">    attention_weight_dec_masked<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_weight_dec_masked'</span>]</span>
<span id="cb19-854"></span>
<span id="cb19-855">    intrinsic_value_weight_dec_cross<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'intrinsic_value_weight_dec_cross'</span>]</span>
<span id="cb19-856">    response_weight_dec_cross<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'response_weight_dec_cross'</span>]</span>
<span id="cb19-857">    information_weight_dec_cross<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'information_weight_dec_cross'</span>]</span>
<span id="cb19-858">    attention_weight_dec_cross<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_weight_dec_cross'</span>]</span>
<span id="cb19-859"></span>
<span id="cb19-860">    W1_dec<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W1_dec'</span>]</span>
<span id="cb19-861">    b1_dec<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b1_dec'</span>]</span>
<span id="cb19-862">    W2_dec<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W2_dec'</span>]</span>
<span id="cb19-863">    b2_dec<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b2_dec'</span>]</span>
<span id="cb19-864"></span>
<span id="cb19-865">    W_embed_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W_embed_out'</span>]</span>
<span id="cb19-866">    b_embed_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>grads[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b_embed_out'</span>]</span>
<span id="cb19-867"></span>
<span id="cb19-868"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># After training, you can use the decoder in inference mode by feeding</span></span>
<span id="cb19-869"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># previously generated tokens (shifted) as input to the decoder and applying</span></span>
<span id="cb19-870"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># the masked multi-head attention to predict the next token</span></span></code></pre></div></div>
<pre><code>Epoch 1, Loss: 2.0795187664172397
Predicted: banane coniglio banane banane &lt;start&gt; piacciono &lt;end&gt;
Epoch 2, Loss: 2.0793373726502367
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 3, Loss: 2.0791564355830636
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 4, Loss: 2.078974718625546
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 5, Loss: 2.0787933552602227
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 6, Loss: 2.0786123118403346
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 7, Loss: 2.078431551323341
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 8, Loss: 2.078251032354273
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 9, Loss: 2.0780707083355354
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 10, Loss: 2.0778905181441116
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 11, Loss: 2.077710390742451
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 12, Loss: 2.077530273264538
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 13, Loss: 2.0773500940542484
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 14, Loss: 2.0771697955319044
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 15, Loss: 2.0769892772510743
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 16, Loss: 2.07680845041442
Predicted: banane coniglio banane banane mio piacciono &lt;end&gt;
Epoch 17, Loss: 2.0766272061537157
Predicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;
Epoch 18, Loss: 2.0764454984062057
Predicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;
Epoch 19, Loss: 2.076263321722837
Predicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;
Epoch 20, Loss: 2.0760807185682095
Predicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;</code></pre>
</section>
<section id="observations" class="level3">
<h3 class="anchored" data-anchor-id="observations">Observations</h3>
<p>Well, after all of that, this poor of a performance is what we got? This is true. We were translating only one sequence, after all. Where transformers really excel is at scale, because of the differentiable attention matrix which can be parallelized. But you can see how it gets stuck depending on different learning rates and initialization. One of the difficulties is initialization to make gradient descent helpful. Xavier initialization could have worked, but it is much better to do it with numpy to get real understanding.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>This aims to be an implementation of transformers that works, end-to-end, in numpy. No pointing to paper that blew up - Attention is All You Need (no, not it isn’t). No awkward discussions of ‘keys’, ‘values’, ‘queries’ (replaced with <em>fundamental</em> names instead). No list of mathematical formulas for different attention mechanisms with ‘choose what you want’. No describing the transformer architecture in a way that is essentially ‘just look at the figure’. No code snippets without explanation for the basics. For the last point, the final implementation is, in fact, annoying. The assumption is that you read this point from start to end. However there is something I have not mentioned at all. That something is <strong>automatic differentiation</strong>.</p>
<section id="automatic-differentiation" class="level4">
<h4 class="anchored" data-anchor-id="automatic-differentiation">Automatic Differentiation</h4>
<p>You may have noticed just how obnoxious implementing backpropagation was. Every single function had to be done manually. Was there a way we could have done it recursively? The answer is yes. This is called <em>automatic differentiation</em>. Automatic differentiation relies on the technique that all mathematical operations done by a computer are at the end pulled from basic arithmetic functions, since that’s what’s implemented on their circuit boards. Therefore, all computations of functions are inherently limited by this. Even though multiplication is not repeated addition, to a computer it is. Libraries exist entirely to implement backpropagation and automatic differentiation for user-defined functions. One such library is <strong>PyTorch</strong>. It implements automatic differentiation and abstracts it away, allowing users to define new functions and not worry about how backpropagation deals with that object. How would the transformer from scratch look like in PyTorch? <a href="https://github.com/IParraMartin/An-Explanation-Is-All-You-Need/blob/main/model.py">:)</a></p>


</section>
</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>If you angrily clicked on the footnote after reading this sentence, are you an NLP enthusiast?↩︎</p></li>
<li id="fn2"><p>This is almost certainly me being lazy/not having any formal training in deep learning, but there <em>is</em> an awful lot of garbage out there, and most of it from full-time machine learning engineers! If I read another variation on ‘Transformers have revolutionized the field of natural language processing by leveraging the attention mechanism to embed data and make better predictions’….↩︎</p></li>
<li id="fn3"><p>The better commented implementations of transformers from scratch that I found were created in 2024. Does this say anything about the field of machine learning?↩︎</p></li>
<li id="fn4"><p>Slides are often combined with lectures, so I am willing to give this one a pass. They are also often meant for explaining new things to people who already have experience in the field, so it makes sense that I wouldn’t understand them. Unsurprisingly: https://sites.astro.caltech.edu/%7Egeorge/ay141/mermin.pdf↩︎</p></li>
<li id="fn5"><p>As of 2024, anyone claiming that transformers/LLMs can understand and reason like humans is pontificating.↩︎</p></li>
<li id="fn6"><p>Obviously, this goes wrong the instant you add punctuation, have strings like ‘abcdefghijklabcabcdabcdeab’ as input, have more words than your input stream can handle, and so on. However, implementing byte-pair encoding would go beyond the scope of the tutorial.↩︎</p></li>
<li id="fn7"><p>I have transposed the column vector to make it a row vector.↩︎</p></li>
<li id="fn8"><p>Foreshadowing↩︎</p></li>
<li id="fn9"><p>In the sense that the next word is ‘meaningful’.↩︎</p></li>
<li id="fn10"><p>I am deliberately including punctuation here, as the same technique can be used when punctuations are treated as unique words.↩︎</p></li>
<li id="fn11"><p>This is also partially the reason the appeal to etymology is incorrect.↩︎</p></li>
<li id="fn12"><p>Compare ‘hot’, ‘dog’, and ‘hot dog’.↩︎</p></li>
<li id="fn13"><p>See <a href="https://en.wikipedia.org/wiki/Stirling%27s_approximation">Stirling’s approximation</a>↩︎</p></li>
<li id="fn14"><p>Foreshadowing↩︎</p></li>
<li id="fn15"><p>Technically, this is what we have been attempting to do this entire time.↩︎</p></li>
<li id="fn16"><p>More on this later↩︎</p></li>
<li id="fn17"><p>For this reason, layers tend to have the same activation function, as it is easy to parallelize the computation↩︎</p></li>
<li id="fn18"><p>Making automatic differentiation easier led to the invention of many fundamental inventions in empirical learning↩︎</p></li>
<li id="fn19"><p>https://sacred-texts.com/hin/m01/m01002.htm, ‘I am (continued Sauti)…’↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>machine-learning</category>
  <category>tutorial</category>
  <category>deep-learning</category>
  <guid>https://ksd3.github.io/blog/2024-12-07-transformers/</guid>
  <pubDate>Sat, 07 Dec 2024 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Papers explained: Partitioning the Universe’s baryons (Connor et al 2024)</title>
  <link>https://ksd3.github.io/blog/2024-12-01-partitioning/</link>
  <description><![CDATA[ 





<section id="an-annotated-explanation-of-a-gas-rich-cosmic-web-revealed-by-partitioning-the-missing-baryons-by-connor-et-al.-2024" class="level1">
<h1>An annotated explanation of ‘A gas rich cosmic web revealed by partitioning the missing baryons’ by Connor et al.&nbsp;(2024)</h1>
<section id="about" class="level2">
<h2 class="anchored" data-anchor-id="about">About</h2>
<p>Connor et al.&nbsp;(2024) recently published a <a href="https://arxiv.org/html/2409.16952v1">paper</a> that shows a way to partition the universe’s baryons to reveal the underlying <a href="https://science.nasa.gov/resource/cosmic-web/">cosmic web</a> structure and showed that it contains a lot of gas. Is it possible to explain this paper to mildly intelligent people with an interest in astronomy?</p>
<section id="abstract" class="level3">
<h3 class="anchored" data-anchor-id="abstract">Abstract</h3>
<p>Approximately half of the universe’s dark matter (that is, matter that does not interact with electromagnetic radiation such as light, so we can’t see it) resides in structures known as ‘collapsed halos’. Collapsed halos are ellipsoidal or quasi-spherical structures surrounding a central object like a star. The initial universe’s density field was approximately uniform, but quantum fluctuations led to changes in the initial density field at different points. Changes in density eventually led to gravity taking over and clustering the matter into halo-like structures. Significantly less than half of the universe’s baryons (protons and neutrons; electrons are leptons because they interact only using the weak nuclear force and the electromagnetic force) are confined to halos. A small fraction of the baryons are present as stars and in the interstellar medium (ISM) between galaxies. A lot of them are very diffuse (less than 1000 per cubic centimeter) and ionized (less than one in ten thousand is ionized in the baryonic gas) and are located in the halos of galaxy clusters, galaxy groups (literally just some galaxies together but not enough to be a cluster), or individual galaxies. Because this gas is so diffuse and ionized, the quantity and spatial distribution is very difficult to measure. If we measure it really well and are able to provide an accurate answer to the question ‘what is this quantity and spatial distribution?’ then we have an answer to a lot of important questions in galaxy formation, astrophysical feedback (the ejection of matter by stars, black holes, etc into the universe, or the reverse!), and precision cosmology. Recently, people have used Fast Radio Bursts (FRBs), very violent emissions of radio waves first discovered in 2007, to measure the total content of cosmic baryons. The problem in those studies is that they did not contain methods to discriminate between the intergalactic material (IGM) and halo gases. In this paper we present a large cosmological sample of FRB sources localized to their host galaxies. Also we have managed to partition (that is, determine the distribution of) missing baryons into the IGM, galaxy clusters, and galaxies. As a reminder, the term missing baryons refers to the following problem: cosmological models say that the number of baryons in the universe should be so-and-so, but actual measurements show that there are much fewer baryons. Then where are the missing baryons? The study provides a late-Universe (that is, the current universe) measurement of the total baryon density of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Ω</mi><mi>b</mi></msub><mi>​</mi><msub><mi>h</mi><mn>70</mn></msub></mrow><annotation encoding="application/x-tex">\Omega_{b}​h_{70}</annotation></semantics></math> as 0.049 $$0.003, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Ω</mi><mi>b</mi></msub><annotation encoding="application/x-tex">\Omega_{b}</annotation></semantics></math> is the universe’s critical density (the density needed for a flat universe) that is made of baryonic matter. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>h</mi><mn>70</mn></msub><annotation encoding="application/x-tex">h_{70}</annotation></semantics></math> is simply a scaling factor related to the Hubble constant. The results indicate that feedback processes can expel gas from galaxy halos and into the intergalactic medium, agreeing with the enriched cosmic web scenario (that is, how the cosmic web contains elements heavier than hydrogen and helium) seen in cosmological simulations. We measured a large diffuse baryon fraction and that probably means that the distribution of stellar masses in a population of stars (the initial stellar mass function) is probably not bottom-heavy (it does not predict a large number of low mass stars, which in turn live for longer).</p>
</section>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>The Deep Synoptic Array-110 is a radio inteferometer (a system of radio telescopes that observe the same source at the same time) operating between 1.28-1.53 GHz and is the first radio telescope built to detect and localize FRBs (that is, find out the galaxy they originate from). Doing this helps use FRBs as cosmological tools and unveil their physical origin. 60 new FRBs were discovered by DSA-110 and 39 of them now have an associated host galaxy spectroscopic redshift. Our companion work shows the properties of the host galaxies of a uniformly selected subset of these. The work also includes 9 sources not in that sample. Three new FRBs near or beyond redshift 1 (redshift 1 means ~8-9 billion years old, as the wavelength of light has doubled) are also presented. Because these FRBs are at redshift 1, they constrain the IGM column (that is, the distribution of material along the line of sight) by virtue of their great distance.</p>
<p>We add our sample to 30 previously localized FRB sources. The distribution of extragalactic DM (dispersion measure - the TEC, but measured in terms of redshift) and redshift for the full sample is plotted in figure 1; the positions, DM, redshifts, and detection instruments are displayed in extended data table 1. The extragalactic DM of localized FRBs tells us how many diffuse baryons are in what place in the Universe. The total observed DM of an FRB can be split into several components. Let’s take a moment to think about this. When looking at a radio source occurring so far away, we need to consider several things. First, we are looking out of the Milky Way, so the Milky Way’s ISM contributes to the total DM. The Milky Way’s halo also contributes to it. The inter <em>galactic</em> medium along the line of sight is of course always present. There is also some ionized gas in stable halos (<em>virialized</em> halos). Then there are also the contributions from each ionized halo along the line of sight. To account for the expansion of the universe and relativistic motion the factor <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msub><mi>z</mi><mrow><mi>h</mi><mi>a</mi><mi>l</mi><mi>o</mi></mrow></msub></mrow></mfrac><annotation encoding="application/x-tex">\frac{1}{1+z_{halo}}</annotation></semantics></math> is applied to the observed dispersion measure. Finally there is the contribution of the FRB host galaxy’s matter (note that since we are now talking about plasmas, it means that we must consider fermionic matter such as electrons too), which may come from its halo, ISM, or any other plasma lying around the galaxy.</p>
<p>We can therefore write the observed FRB of a galaxy in the following way: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><msub><mi>M</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msub><mo>=</mo><mi>D</mi><msub><mi>M</mi><mrow><mi>M</mi><mi>W</mi></mrow></msub><mo>+</mo><mi>D</mi><msub><mi>M</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><munderover><mo>∑</mo><mi>i</mi><msub><mi>N</mi><mi>x</mi></msub></munderover><mfrac><mrow><mi>D</mi><msub><mi>M</mi><mi>X</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>M</mi><mi>i</mi></msub><mo>,</mo><msub><mi>b</mi><mo>⟂</mo></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><msub><mi>z</mi><mi>i</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>D</mi><msub><mi>M</mi><mrow><mi>h</mi><mi>o</mi><mi>s</mi><mi>t</mi></mrow></msub></mrow><mrow><mn>1</mn><mo>+</mo><msub><mi>z</mi><mi>s</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">
DM_{obs}=DM_{MW}+DM_{IGM}(z_s)+\sum_{i}^{N_x}\frac{DM_X(M_i,b_{\perp})}{1+z_i}+\frac{DM_{host}}{1+z_s}
</annotation></semantics></math></p>
<p>Note that the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>b</mi><mo>⟂</mo></msub><annotation encoding="application/x-tex">b_{\perp}</annotation></semantics></math> term is the <em>physical impact parameter</em>, which is a measure of how close the line of sight passes to higher-mass regions in the intersecting halo. A higher <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>b</mi><mo>⟂</mo></msub><annotation encoding="application/x-tex">b_{\perp}</annotation></semantics></math> means that the line of sight is on the outskirts of the matter distribution and vice versa. Also, the subscript <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> just means intersection. DM_{IGM} generally dominates for sources beyond <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>≈</mo><mn>0.2</mn></mrow><annotation encoding="application/x-tex">z \approx 0.2</annotation></semantics></math> unless the FRB is in an ununsual galaxy. Let us now define a ‘cosmological DM’ as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><msub><mi>M</mi><mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub><mo>≡</mo><mi>D</mi><msub><mi>M</mi><mi>X</mi></msub><mo>+</mo><mi>D</mi><msub><mi>M</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub></mrow><annotation encoding="application/x-tex">DM_{cos}\equiv DM_{X}+DM_{IGM}</annotation></semantics></math>. Our goal is to now find out what the average sightline’s DM from the IGM and intervening halos would be.</p>
<hr>
<section id="interlude-dm-and-tec" class="level4">
<h4 class="anchored" data-anchor-id="interlude-dm-and-tec">Interlude: DM and TEC</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Total_electron_content">Total Electron Content</a> is simply the columnar number density of electrons where the column is along a line of sight. It is obtained by integrating the electron density along a path <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">ds</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∫</mo><mn>0</mn><mi>l</mi></msubsup><msub><mi>n</mi><mi>e</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">\int_{0}^{l}n_e(s)ds</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math> is the straight-line distance the radio source and the observer. It is a measure of how the underlying plasma interferes with radio signals passing through it.</p>
<p>The <a href="https://arxiv.org/pdf/2407.16748">dispersion measure</a> is the exact same thing. It is a measure of how the intermediary plasma affects radio waves passing through it. The catch is that the DM is measured on galactic scales; we must take into account the effects of relativity. The dispersion measure is defined as <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∫</mo><mn>0</mn><mi>z</mi></msubsup><msub><mi>n</mi><mi>e</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">\int_{0}^{z}n_e(z)dl</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math> is the <strong>redshift</strong> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">dl</annotation></semantics></math> is the proper distance element. The proper distance is the actual distance between two objects in the universe at a certain point in cosmic time. What does this mean? We know the universe is expanding. If we want to measure the distance between two objects at a certain point in time, we simply freeze the universe and observe it from a god’s-eye perspective. The proper distance is now the distance between the two objects. The proper distance is related to the <em>comoving distance</em> by multiplying with a simple factor (called the scale factor), given the universe is described by the <a href="https://en.wikipedia.org/wiki/Friedmann%E2%80%93Lema%C3%AEtre%E2%80%93Robertson%E2%80%93Walker_metric">FLRW metric</a>. The differential of the proper distance element is related to the <a href="https://en.wikipedia.org/wiki/Hubble%27s_law">Hubble factor</a> and redshift by the equation <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>l</mi><mo>=</mo><mi>c</mi><mfrac><mrow><mi>d</mi><mi>z</mi></mrow><mrow><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">dl=c\frac{dz}{H(z)}</annotation></semantics></math>.</p>
<p>Because of how the redshift is <a href="https://en.wikipedia.org/wiki/Redshift">defined</a>, the physical baryon density scales with redshift as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup><annotation encoding="application/x-tex">(1+z)^3</annotation></semantics></math>. The DM is therefore a redshift corrected version of the TEC. It is correct to use it when considering things on intergalactic scales, not on interstellar scales.</p>
<hr>
<p>Since the DM is a measure of interference by electrons, we need to figure out how to relate this with baryons. We know that the number of protons and electrons are roughly the same in the universe. Of course, some of the baryons could be locked up in stars like neutrons in neutron stars; some of the protons could be flying around on their own without an electron to balance them. Let’s relate our dispersion measure to baryons. First, let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>d</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_d(z)</annotation></semantics></math> be the fraction of all baryons lying in the diffuse ionized IGM or halos (not in stars, cold gas, and so on), and let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>e</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_e(z)</annotation></semantics></math> be the effective number of free electrons per baryon (if they’re electrically neutral, how can they intefere with radio waves?).</p>
<p>Then <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>e</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>f</mi><mi>d</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>f</mi><mi>e</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">n_e(z)=f_d(z)f_e(z)</annotation></semantics></math>. The proper baryon density at redshift <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math> scales as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup><annotation encoding="application/x-tex">(1+z)^3</annotation></semantics></math>, so you can write <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>e</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>f</mi><mi>d</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>f</mi><mi>e</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>Ω</mi><mi>b</mi></msub><msub><mi>ρ</mi><mrow><mi>c</mi><mo>,</mo><mn>0</mn></mrow></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup><mi>/</mi><msub><mi>m</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">n_e(z)=f_d(z)f_e(z)\Omega_b \rho_{c,0} (1+z)^3/m_p</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Ω</mi><mi>b</mi></msub><annotation encoding="application/x-tex">\Omega_b</annotation></semantics></math> is the present-day baryon density parameter (the <em>cosmic baryon abundance</em>), <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ρ</mi><mrow><mi>c</mi><mo>,</mo><mn>0</mn></mrow></msub><mo>=</mo><mfrac><mrow><mn>3</mn><msubsup><mi>H</mi><mn>0</mn><mn>2</mn></msubsup></mrow><mrow><mn>8</mn><mi>π</mi><mi>G</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\rho_{c,0}=\frac{3H_0^2}{8\pi G}</annotation></semantics></math> is the present <a href="https://en.wikipedia.org/wiki/Friedmann_equations#Density_parameter">critical density</a> for a flat universe, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mi>p</mi></msub><annotation encoding="application/x-tex">m_p</annotation></semantics></math> is the proton mass (so <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><msub><mi>ρ</mi><mrow><mi>c</mi><mo>,</mo><mn>0</mn></mrow></msub><msub><mi>m</mi><mi>p</mi></msub></mfrac><annotation encoding="application/x-tex">\frac{\rho_{c,0}}{m_p}</annotation></semantics></math> counts the number of baryons).</p>
<p>Therefore, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>e</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>f</mi><mi>d</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>f</mi><mi>e</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Ω</mi><mi>b</mi></msub><mfrac><mrow><mn>3</mn><msubsup><mi>H</mi><mn>0</mn><mn>2</mn></msubsup></mrow><mrow><mn>8</mn><mi>π</mi><mi>G</mi></mrow></mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mfrac><mn>1</mn><msub><mi>m</mi><mi>p</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">n_e(z)=f_d(z)f_e(z)(\Omega_b\frac{3H_0^2}{8 \pi G}(1+z)^3)\frac{1}{m_p}</annotation></semantics></math>.</p>
<p>The Hubble parameter at redshift <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math> for a spatially flat FRW universe with matter fraction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Ω</mi><mi>m</mi></msub><annotation encoding="application/x-tex">\Omega_m</annotation></semantics></math> and cosmological constant (the <em>dark energy parameter</em>) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Ω</mi><mi>Λ</mi></msub><annotation encoding="application/x-tex">\Omega_{\Lambda}</annotation></semantics></math> is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>H</mi><mn>0</mn></msub><msqrt><mrow><msub><mi>Ω</mi><mi>m</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup><mo>+</mo><msub><mi>Ω</mi><mi>Λ</mi></msub></mrow></msqrt></mrow><annotation encoding="application/x-tex">H(z)=H_0 \sqrt{\Omega_m (1+z)^3+ \Omega_\Lambda}</annotation></semantics></math></p>
<p>and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>l</mi><mo>=</mo><mfrac><mi>c</mi><mrow><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>d</mi><mi>z</mi></mrow><annotation encoding="application/x-tex">dl=\frac{c}{H(z)}dz</annotation></semantics></math>, so <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>l</mi><mo>=</mo><mfrac><mi>c</mi><msub><mi>H</mi><mn>0</mn></msub></mfrac><mfrac><mrow><mi>d</mi><mi>z</mi></mrow><msqrt><mrow><msub><mi>Ω</mi><mi>m</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup><mo>+</mo><msub><mi>Ω</mi><mi>Λ</mi></msub></mrow></msqrt></mfrac></mrow><annotation encoding="application/x-tex">dl=\frac{c}{H_0}\frac{dz}{\sqrt{\Omega_m (1+z)^3+ \Omega_\Lambda}}</annotation></semantics></math></p>
<p>Therefore the average sightline DM would be <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo><mi>D</mi><msub><mi>M</mi><mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub><mo>&gt;</mo><mo>=</mo><mfrac><mrow><mn>3</mn><mi>c</mi></mrow><mrow><mn>8</mn><mi>π</mi></mrow></mfrac><mfrac><msub><mi>Ω</mi><mi>b</mi></msub><mi>G</mi></mfrac><mfrac><msub><mi>H</mi><mn>0</mn></msub><msub><mi>m</mi><mi>p</mi></msub></mfrac><msubsup><mo>∫</mo><mn>0</mn><msub><mi>z</mi><mi>s</mi></msub></msubsup><mfrac><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>f</mi><mi>d</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>f</mi><mi>e</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msqrt><mrow><msub><mi>Ω</mi><mi>m</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup><mo>+</mo><msub><mi>Ω</mi><mi>Λ</mi></msub></mrow></msqrt></mfrac></mrow><annotation encoding="application/x-tex">&lt;DM_{cos}&gt;=\frac{3c}{8\pi}\frac{\Omega_b}{G}\frac{H_0}{m_p} \int_{0}^{z_s}\frac{(1+z)f_d(z)f_e(z)}{\sqrt{\Omega_m (1+z)^3+ \Omega_\Lambda}}</annotation></semantics></math></p>
<p>where the extra <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>+</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">1+z</annotation></semantics></math> in the denominator comes from how many factors in the group-delay integral you want to account for.</p>
<p>We take <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>e</mi></msub><mo>=</mo><mn>0.875</mn></mrow><annotation encoding="application/x-tex">f_e=0.875</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mi>d</mi></msub><annotation encoding="application/x-tex">f_d</annotation></semantics></math> as a constant, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mn>70</mn></msub><mo>≡</mo><msub><mi>H</mi><mn>0</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mn>70</mn><mi>k</mi><mi>m</mi><msup><mi>s</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>M</mi><mi>p</mi><msup><mi>c</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h_{70}\equiv H_0(70 km s^{-1} Mpc^{-1})</annotation></semantics></math>, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo><mi>D</mi><msub><mi>M</mi><mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub><mo>&gt;</mo><mo>≈</mo><mn>1085</mn><mi>z</mi><msub><mi>f</mi><mi>d</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ω</mi><mi>b</mi></msub><msub><mi>h</mi><mn>70</mn></msub></mrow><mrow><mn>0</mn><mi>/</mi><mn>04703</mn></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mfrac><mrow><mi>p</mi><mi>c</mi></mrow><mrow><mi>c</mi><msup><mi>m</mi><mn>3</mn></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">&lt;DM_{cos}&gt; \approx 1085zf_d(\frac{\Omega_b h_{70}}{0/04703}) \frac{pc}{cm^3}</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>s</mi></msub><mo>≲</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">z_s \lesssim 1</annotation></semantics></math>.</p>
<p>For each FRB, we compute the extragalactic DM distribution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><msub><mi>M</mi><mrow><mi>e</mi><mi>x</mi><mi>t</mi></mrow></msub><mo>=</mo><mi>D</mi><msub><mi>M</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msub><mo>−</mo><mi>D</mi><msub><mi>M</mi><mrow><mi>M</mi><mi>W</mi></mrow></msub></mrow><annotation encoding="application/x-tex">DM_{ext}=DM_{obs}-DM_{MW}</annotation></semantics></math> as a function of the source redshift <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>s</mi></msub><annotation encoding="application/x-tex">z_s</annotation></semantics></math>. This means that we compute a 1D likelihood function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><msub><mi>M</mi><mrow><mi>e</mi><mi>x</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>z</mi><mi>s</mi></msub><mo>,</mo><mover><mi>θ</mi><mo accent="true">→</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(DM_{ex} | z_s, \vec{\theta})</annotation></semantics></math>, where the model parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>θ</mi><mo accent="true">→</mo></mover><annotation encoding="application/x-tex">\vec{\theta}</annotation></semantics></math> are given by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>θ</mi><mo accent="true">→</mo></mover><mo>=</mo><mo stretchy="false" form="prefix">{</mo><msub><mi>f</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub><mo>,</mo><msub><mi>f</mi><mi>X</mi></msub><mo>,</mo><msub><mi>μ</mi><mrow><mi>h</mi><mi>o</mi><mi>s</mi><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>σ</mi><mrow><mi>h</mi><mi>o</mi><mi>s</mi><mi>t</mi></mrow></msub><mo stretchy="false" form="postfix">}</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">\vec{\theta}=\{f_{IGM}, f_X, \mu_{host}, \sigma_{host}\}.</annotation></semantics></math> Here <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub><annotation encoding="application/x-tex">f_{IGM}</annotation></semantics></math> is the fraction of baryons in the IGM (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><msub><mi>Ω</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub><msub><mi>Ω</mi><mi>b</mi></msub></mfrac><annotation encoding="application/x-tex">\frac{\Omega_{IGM}}{\Omega_b}</annotation></semantics></math>) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>X</mi></msub><mo>=</mo><mfrac><msub><mi>Ω</mi><mrow><mi>h</mi><mi>a</mi><mi>l</mi><mi>o</mi><mi>s</mi></mrow></msub><msub><mi>Ω</mi><mi>b</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">f_X=\frac{\Omega_{halos}}{\Omega_{b}}</annotation></semantics></math> is the fraction of baryons in the intersected halos referenced (normalized) to redshift 0.1. Why can this be done? It’s because the three components of the dispersion measure - <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><msub><mi>M</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub></mrow><annotation encoding="application/x-tex">DM_{IGM}</annotation></semantics></math>,<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><msub><mi>M</mi><mi>X</mi></msub></mrow><annotation encoding="application/x-tex">DM_{X}</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><msub><mi>M</mi><mrow><mi>M</mi><mi>W</mi></mrow></msub></mrow><annotation encoding="application/x-tex">DM_{MW}</annotation></semantics></math> all have different dependencies on the redshift (their distributions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><mi>M</mi><mo stretchy="false" form="prefix">|</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(DM|z)</annotation></semantics></math> are different) given a sufficiently large sample size. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>μ</mi><mrow><mi>h</mi><mi>o</mi><mi>s</mi><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">\mu_{host}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>σ</mi><mrow><mi>h</mi><mi>o</mi><mi>s</mi><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">\sigma_{host}</annotation></semantics></math> are the mean dispersion measure and scattering coefficient of the host galaxy of the source, respectively. Note that the IGM is defined as the gas outside of <a href="https://en.wikipedia.org/wiki/Virial_theorem">virialized</a> (i.e.&nbsp;stable) dark matter halos.</p>
<hr>
</section>
<section id="interlude-bayes-theorem" class="level4">
<h4 class="anchored" data-anchor-id="interlude-bayes-theorem">Interlude: Bayes’ Theorem</h4>
<p>Bayes’ theorem allows one to compute the probability of a cause given its effect. Given a prior belief of the hypothesis (the <strong>prior</strong>), you update it with new data (the <strong>likelihood</strong>), and compute the probability of the cause given the effect (the <strong>posterior probability</strong> or <strong>posterior</strong>) according to the formula <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="false" form="prefix">|</mo><mi>D</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(\theta|D)=\frac{P(D|\theta) P(\theta)}{P(D)}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="false" form="prefix">|</mo><mi>D</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\theta|D)</annotation></semantics></math> is the posterior, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(D|\theta)</annotation></semantics></math> is the likelihood, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\theta)</annotation></semantics></math> is the prior, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(D)</annotation></semantics></math> is the <strong>marginal likelihood</strong> or <strong>evidence</strong> - the total probability of the effect itself occurring, considering all possible causes.</p>
<hr>
<p>We want to find out the <a href="https://en.wikipedia.org/wiki/Joint_probability_distribution">joint distribution</a> that describes the probability that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><msub><mi>M</mi><mrow><mi>e</mi><mi>x</mi></mrow></msub></mrow><annotation encoding="application/x-tex">DM_{ex}</annotation></semantics></math> is a certain value given a certain value of the redshift <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>. To do this, we compute the 1D likelihood function given above for each FRB, and multiply them together. (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∏</mo><mi>i</mi><mi>n</mi></msubsup><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><msub><mi>M</mi><mrow><mi>e</mi><mi>x</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>z</mi><mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">→</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\prod_{i}^{n}P(DM_{ex,i}|z_{s,i})P(\vec{\theta})</annotation></semantics></math>). To do this, we apply Markov Chain Monte Carlo (MCMC) methods.</p>
<hr>
</section>
<section id="interlude-markov-chain-monte-carlo-mcmc-methods" class="level4">
<h4 class="anchored" data-anchor-id="interlude-markov-chain-monte-carlo-mcmc-methods">Interlude: Markov Chain Monte Carlo (MCMC) methods:</h4>
<p>Sampling from a probability distribution means randomly selecting points where the likelihood of selecting a given point is defined by the probability distribution. To sample from a one-dimensional probability distribution is easy - suppose someone said to you ‘I have a Gaussian probability distribution with a mean of 3.5 and a standard deviation of 2. You can take as many numbers as you want and I’ll give them to you. Since it is a normal distribution, you’re more likely to get a bunch of numbers around 3.5; once you get a bunch of numbers you can compute how many fell between 3.2 and 3.63, 1.32 and 1.39, and so on’. You could simply get a bunch of numbers (say 3266 numbers) and run some tests on them and go ‘okay, makes sense, I see that I have 299 numbers between 3.2 and 3.63 and 29 between 1.32 and 1.39. This is fairly reasonable’. The numbers you chose are <strong>samples</strong> of that probability distribution and you chose them one at a time.</p>
<p>Then the person (let’s say the ghost of your great grandfather) comes up to you and says ‘I have a probability distribution defined as the convolution of the Riemann Zeta function and the Von Mangoldt function for 55681 variables, but the convolution is only computed at roots of the normal numbers between 4 and 48575’. You are stuck. How would you even go about sampling this distribution? You’re not allowed to ask him to give you numbers, because even he doesn’t want to compute things. This is where MCMC methods come in.</p>
<p>Let us focus on how MCMC methods broadly work. You want to sample from a probability distribution. Why do you want to do this? If you have enough samples then you can compute averages (more precisely, <strong>expectation values</strong>), answer probabilistic questions, or even just flat out plot it. You say, “Okay, let me start with an initial guess. For lack of a better guess, I’ll start with a completely random guess. Then I look at points near to my guess and pick one. How? Let’s just go with any random point near me. Then I decide whether to move in that direction or not. If yes, I move in that direction and repeat the procedure. Otherwise I pick another point. If I keep moving for long enough I will probably cover the whole probability distribution.”</p>
<p>More precisely, given some underlying knowledge of the probability distribution, you can sample from it and construct a rough approximation of the actual probability distribution. This underlying knowledge may also be based on hunches: assume that at least, locally, the distribution is Gaussian.</p>
<p>The process of getting this sample from the underlying knowledge is called a <strong>Monte Carlo</strong> method and the construction of a series of steps (a straightforward example of a <strong>random walk</strong>) is an example of generating a <strong>Markov Chain</strong>. The implicit assumption is that the next step only depends on your current state, so this is a first-order Markov chain.</p>
<hr>
<p>What distribution do we assume for the host contribution to the dispersion measure? Let’s go with a log-normal distribution, as it is right-skewed and defined only for positive values. We are going to estimate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>μ</mi><mrow><mi>h</mi><mi>o</mi><mi>s</mi><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">\mu_{host}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>σ</mi><mrow><mi>h</mi><mi>o</mi><mi>s</mi><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">\sigma_{host}</annotation></semantics></math>. The prior values for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>μ</mi><mrow><mi>h</mi><mi>o</mi><mi>s</mi><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">\mu_{host}</annotation></semantics></math> are assumed to be from a uniform probability distribution between 0 and 7 (therefore the DM spans 0 to 1000 <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>c</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> </mtext><mspace width="0.333em"></mspace></mrow><mi>c</mi><msup><mi>m</mi><mrow><mi>−</mi><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">pc\text{ }cm^{-3}</annotation></semantics></math>) and the same flat prior shape is used for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub><annotation encoding="application/x-tex">f_{IGM}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mi>X</mi></msub><annotation encoding="application/x-tex">f_{X}</annotation></semantics></math>, but the values range from 0 to 1. The added constraint is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub><mo>+</mo><msub><mi>f</mi><mi>X</mi></msub><mo>≤</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">f_{IGM}+f_X\leq 1</annotation></semantics></math>.</p>
<p>Fitting this distribution to our dataset gives <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub><mo>=</mo><msubsup><mn>0.80</mn><mrow><mi>−</mi><mn>0.09</mn></mrow><mrow><mi>+</mi><mn>0.08</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">f_{IGM}=0.80^{+0.08}_{-0.09}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>X</mi></msub><mo>=</mo><msubsup><mn>0.11</mn><mrow><mi>−</mi><mn>0.07</mn></mrow><mrow><mi>+</mi><mn>0.10</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">f_{X}=0.11^{+0.10}_{-0.07}</annotation></semantics></math>. We calculate that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>μ</mi><mrow><mi>h</mi><mi>o</mi><mi>s</mi><mi>t</mi></mrow></msub><mo>=</mo><msubsup><mn>4.90</mn><mrow><mi>−</mi><mn>0.20</mn></mrow><mrow><mi>+</mi><mn>0.18</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">\mu_{host}=4.90^{+0.18}_{-0.20}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>σ</mi><mrow><mi>h</mi><mi>o</mi><mi>s</mi><mi>t</mi></mrow></msub><mo>=</mo><msubsup><mn>0.56</mn><mrow><mi>−</mi><mn>0.14</mn></mrow><mrow><mi>+</mi><mn>0.16</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">\sigma_{host}=0.56^{+0.16}_{-0.14}</annotation></semantics></math>. This means that the median rest-frame contribution of the host to the DM is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mn>130</mn><mrow><mi>−</mi><mn>23</mn></mrow><mrow><mi>+</mi><mn>25</mn></mrow></msubsup><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> </mtext><mspace width="0.333em"></mspace></mrow><mi>p</mi><mi>c</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> </mtext><mspace width="0.333em"></mspace></mrow><mi>c</mi><msup><mi>m</mi><mrow><mi>−</mi><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">130^{+25}_{-23}\text{ } pc \text{ }cm^{-3}</annotation></semantics></math> for our sample. We fit subsamples of the data (such as DSA-110 detected sources) and do jackknife resampling (such as leave-one-out and compute every sample, and compute each statistic for every sample set, and take the mean), and end up observing that sources with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>s</mi></msub><mo>≳</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">z_s\gtrsim 0.5</annotation></semantics></math> constrain the range of the parameters significantly. But our data prefer a large fraction of baryonic material in the IGm and a large total diffuse fraction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mi>d</mi></msub><annotation encoding="application/x-tex">f_d</annotation></semantics></math>. This is probably because there are too few sources with low <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><msub><mi>M</mi><mrow><mi>e</mi><mi>x</mi></mrow></msub></mrow><annotation encoding="application/x-tex">DM_{ex}</annotation></semantics></math> per unit redshift (called the <strong>DM cliff</strong>). As an example, none of our sources beyond redshift 0.1 has <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>D</mi><msub><mi>M</mi><mrow><mi>e</mi><mi>x</mi></mrow></msub></mrow><msub><mi>z</mi><mi>s</mi></msub></mfrac><mo>&lt;</mo><mn>800</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> </mtext><mspace width="0.333em"></mspace></mrow><mi>p</mi><mi>c</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> </mtext><mspace width="0.333em"></mspace></mrow><mi>c</mi><msup><mi>m</mi><mrow><mi>−</mi><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\frac{DM_{ex}}{z_s}&lt;800 \text{ }pc \text{ } cm^{-3}</annotation></semantics></math>. If the extragalactic DM was dominated by intervening halos or host galaxies then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><msub><mi>M</mi><mrow><mi>e</mi><mi>x</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>z</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(DM_{ex} | z_s)</annotation></semantics></math> would be smaller for low DMs because of the increased line-of-sight variance. Incidentally, this would be true for a universe where baryons perfectly trace the dark matter. But the IGM provides a significant statistical floor for DM per unit distance because most sightlines intersect dozens of filaments and even cosmic voids have a considerable electron column present. Our FRB sample rules out scenarios where baryons trace dark matter (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub><annotation encoding="application/x-tex">f_{IGM}</annotation></semantics></math> is low and a large portion of missing baryons are confined to galaxy halos).</p>
<p>Although FRB DMs are affected by ionized gas in galaxy groups and clusters, the most precise constraints on the baryon budget in massive halos come from X-ray and <a href="https://en.wikipedia.org/wiki/Sunyaev%E2%80%93Zeldovich_effect">SZ</a> measurements. As a reminder, thermal X-ray emission is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∝</mo><mo>∫</mo><msubsup><mi>n</mi><mi>e</mi><mn>2</mn></msubsup><mi>d</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">\propto \int n_e^2dl</annotation></semantics></math> and SZ is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∝</mo><mo>∫</mo><msub><mi>n</mi><mi>e</mi></msub><msub><mi>T</mi><mi>e</mi></msub><mi>d</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">\propto \int n_eT_edl</annotation></semantics></math>. This means that both measurements are sensitive to large, dense regions of hot gas. FRBs pick up DM from ionized plasmas in the way. The hot baryon fraction in halos (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mrow><mi>h</mi><mi>o</mi><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">f_{hot}</annotation></semantics></math>) is a function of halo mass, approaching the cosmological ratio <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≈</mo><mfrac><msub><mi>Ω</mi><mi>b</mi></msub><msub><mi>Ω</mi><mi>M</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">\approx \frac{\Omega_b}{\Omega_M}</annotation></semantics></math> for most massive galaxy clusters. It should be noted that this quantity is less certain for halos below <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>10</mn><mn>14</mn></msup><msubsup><mi>h</mi><mn>70</mn><mrow><mi>−</mi><mn>1</mn></mrow></msubsup><msub><mi>M</mi><mo>⊙</mo></msub></mrow><annotation encoding="application/x-tex">10^{14} h_{70}^{-1}M_\odot</annotation></semantics></math>, but recent advances in sample sizes and measurement precision has significantly improved our knowledge of both the cluster mass function and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mrow><mi>h</mi><mi>o</mi><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">f_{hot}</annotation></semantics></math>. Combining these multiwavelength observations allows us to estimate the fraction of the universe’s baryons in the hot gas of galaxy groups and clusters. We find <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>I</mi><mi>C</mi><mi>M</mi></mrow></msub><mo>=</mo><mn>3.75</mn><mo>±</mo><mn>0.5</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">f_{ICM}=3.75 \pm 0.5\%</annotation></semantics></math> of all baryons in the intracluster medium (ICM). For galaxy groups with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>10</mn><mn>12.7</mn></msup><msub><mi>M</mi><mo>⊙</mo></msub><mo>≤</mo><msub><mi>M</mi><mi>h</mi></msub><mo>≤</mo><msup><mn>10</mn><mn>14</mn></msup><msub><mi>M</mi><mo>⊙</mo></msub></mrow><annotation encoding="application/x-tex">10^{12.7}M_{\odot} \leq M_h \leq 10^{14} M_{\odot}</annotation></semantics></math> this number is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5.4</mn><mo>±</mo><mn>1.0</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">5.4 \pm 1.0 \%</annotation></semantics></math> and therefore, a simple addition shows that roughly 9% of baryons are in a diffuse ionized state in massive halos.</p>
<p>What about the baryons in galaxies, including stars and cold gas? They are the last major group of baryons left. The majority of the cold gas is neutral atomic hydrogen (along with a little molecular hydrogen and helium). 21cm HI surveys at low redshifts measure the HI mass function which is then integrated to estimate the neutral hydrogen density. Taking <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Ω</mi><mrow><mi>H</mi><mi>I</mi></mrow></msub><mo>,</mo><msub><mi>Ω</mi><msub><mi>H</mi><mn>2</mn></msub></msub></mrow><annotation encoding="application/x-tex">\Omega_{HI}, \Omega_{H_2}</annotation></semantics></math> from recent surveys we find <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>H</mi><mi>I</mi></mrow></msub><mo>=</mo><msubsup><mn>9.6</mn><mrow><mi>−</mi><mn>2.3</mn></mrow><mrow><mi>+</mi><mn>3.8</mn></mrow></msubsup><mo>×</mo><msup><mn>10</mn><mrow><mi>−</mi><mn>3</mn></mrow></msup><mo>,</mo><msub><mi>f</mi><msub><mi>H</mi><mn>2</mn></msub></msub><mo>=</mo><msubsup><mn>1.6</mn><mrow><mi>−</mi><mn>0.4</mn></mrow><mrow><mi>+</mi><mn>0.8</mn></mrow></msubsup><mo>×</mo><msup><mn>10</mn><mrow><mi>−</mi><mn>3</mn></mrow></msup><mo>,</mo><msub><mi>f</mi><mrow><mi>c</mi><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub><mo>=</mo><msubsup><mn>1.1</mn><mrow><mi>−</mi><mn>0.2</mn></mrow><mrow><mi>+</mi><mn>0.3</mn></mrow></msubsup><mo>×</mo><msup><mn>10</mn><mrow><mi>−</mi><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">f_{HI}=9.6^{+3.8}_{-2.3} \times 10^{-3}, f_{H_2}=1.6^{+0.8}_{-0.4} \times 10^{-3}, f_{cold}=1.1^{+0.3}_{-0.2} \times 10^{-2}</annotation></semantics></math>. This means just over one percent of the Universe’s baryons are in cold neutral gas within galaxies. For stars estimating this fraction is significantly more difficult. The fraction itself is larger as well. Most stellar mass is in low-mass stars and we can easily see from earlier discussion that the choice of the initial mass function affects the baryon fraction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mo>*</mo></msub><annotation encoding="application/x-tex">f_*</annotation></semantics></math> quite a bit. The <a href="https://en.wikipedia.org/wiki/Initial_mass_function#Salpeter_(1955)">Salpeter IMF</a> (bottom heavy) can return a value of 14%. Instead we can use the Chabrier IMF and a smooth fit to multiple measurements of the local stellar density to get about 4-7%.</p>
<p>After all of this, we can fully account for the <a href="https://en.wikipedia.org/wiki/Missing_baryon_problem">missing baryons</a> by combining the FRB results with other observations. We can also partition the baryons into the IGM, galaxy groups, galaxy clusters, and galaxies. A significant majority of baryonic matter resides in the IGM, outside of virialized halos. From the FRB-independent analysis X-ray groups and clusters, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mn>9.2</mn><mrow><mi>−</mi><mn>1.6</mn></mrow><mn>1.6</mn></msubsup><mi>%</mi></mrow><annotation encoding="application/x-tex">9.2^{1.6}_{-1.6}\%</annotation></semantics></math> are in an ionized phase occupying massive halos. Roughly one percent are in cold neutral gas in galaxies. The conclusion is that the circumgalactic medium (CGM) of individual galaxies cannot contain a substantial fraction of the baryons in the universe. This is in agreement with detailed studies of individual FRB source sightlines. Our FRBs that intersect one or more foreground galaxy CGM at low impact parameters do not have significant excess dispersion. We find <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>g</mi><mi>a</mi><mi>s</mi></mrow></msub><mo>=</mo><msubsup><mn>0.22</mn><mrow><mi>−</mi><mn>0.17</mn></mrow><mrow><mi>+</mi><mn>0.23</mn></mrow></msubsup><mfrac><msub><mi>Ω</mi><mi>b</mi></msub><msub><mi>Ω</mi><mi>M</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">f_{gas}=0.22^{+0.23}_{-0.17} \frac{\Omega_b}{\Omega_M}</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>10</mn><mn>9</mn></msup><msub><mi>M</mi><mo>⊙</mo></msub><mo>&lt;</mo><mi>M</mi><mo>&lt;</mo><mn>5</mn><mo>×</mo><msup><mn>10</mn><mn>12</mn></msup><msub><mi>M</mi><mo>⊙</mo></msub></mrow><annotation encoding="application/x-tex">10^9 M_\odot &lt; M &lt; 5 \times 10^{12} M_\odot</annotation></semantics></math>, below the cosmic average.</p>
<p>What do these results mean? The first basic observation is that feedback processes are required to expel gas and/or prevent gas from falling into their potential wells. We cannot differentiate between specific methods but our conclusion of a rich IGM and baryon-deficient CGM is consistent with simulations where feedback suppresses lower-mass baryon halos. As a reminder, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub><annotation encoding="application/x-tex">f_{IGM}</annotation></semantics></math> is about 0.8. This agrees with other simulations. In <a href="http://simba.roe.ac.uk/">SIMBA</a> simulation with feedback turned off, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub><mo>≈</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">f_{IGM}\approx 0.6</annotation></semantics></math> by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">z&lt;1</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo><mn>0.85</mn></mrow><annotation encoding="application/x-tex">&gt;0.85</annotation></semantics></math> with <a href="https://en.wikipedia.org/wiki/Active_galactic_nucleus">AGN</a> feedback turned on. In the <a href="https://www.tng-project.org/">IllustrisTNG</a> simulation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub><mo>≈</mo><mn>0.8</mn></mrow><annotation encoding="application/x-tex">f_{IGM}\approx 0.8</annotation></semantics></math> at low redshifts and baryons were missing from the CGM of Milky Way-like galaxies. Statistical cross-correlations of galaxy surveys with X-ray and kinematic SZ also agreed and showed that there were few baryons in galaxy halos.</p>
<p>So far we have shown that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub><annotation encoding="application/x-tex">f_{IGM}</annotation></semantics></math> is high and agrees with simulations. Let’s check if another method leads to the same answer. This other method should not rely on a partition of the cosmic baryons as the quantity is sensitive to both the intergalactic and intervening halo gas. To do this, we can use the average cosmic dispersion derived above. This results in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>d</mi></msub><mo>=</mo><msubsup><mn>0.93</mn><mrow><mi>−</mi><mn>0.05</mn></mrow><mrow><mi>+</mi><mn>0.04</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">f_d=0.93^{+0.04}_{-0.05}</annotation></semantics></math>, independent of any assumptions about <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><msub><mi>M</mi><mrow><mi>I</mi><mi>G</mi><mi>M</mi></mrow></msub><mo>,</mo><mi>D</mi><msub><mi>M</mi><mi>X</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_{cos}(DM_{IGM}, DM_{X})</annotation></semantics></math> and its redshift evolution.</p>
<p>The mean cosmological DM of FRBs places a ceiling on the total stellar mass of the universe because <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mo>*</mo></msub><mo>&lt;</mo><mn>1</mn><mo>−</mo><msub><mi>f</mi><mi>d</mi></msub><mo>−</mo><msub><mi>f</mi><mrow><mi>c</mi><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">f_*&lt;1-f_d-f_{cold}</annotation></semantics></math>. The results our our analysis suggest that over 90% of baryons are in a diffuse state in the IGM and in dark matter halos or cold gas (read: not in stars). This is independent of galaxy spectral energy distribution modeling, choice of the IMF, and the low mass cutoff for other methods. As we have repeatedly stated, most of the stellar mass is in numerous small stars. Our <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mo>*</mo></msub><annotation encoding="application/x-tex">f_*</annotation></semantics></math> constrains the mean stellar IMF. We place a 90% upper limit on the stellar baryon fraction at low redshifts of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>≤</mo><mn>9</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">f\leq 9\%</annotation></semantics></math> and therefore <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ρ</mi><mo>*</mo></msub><mo>≤</mo><mn>5.6</mn><mo>×</mo><msup><mn>10</mn><mn>8</mn></msup><msub><mi>M</mi><mo>⊙</mo></msub><mi>M</mi><mi>p</mi><msup><mi>c</mi><mrow><mi>−</mi><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\rho_* \leq 5.6 \times 10^8 M_\odot Mpc^{-3}</annotation></semantics></math> and therefore Salpeter IMFs can be ruled out for a low-mass cutoff below 0.10 <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>M</mi><mo>⊙</mo></msub><annotation encoding="application/x-tex">M_\odot</annotation></semantics></math>.</p>
<p>If the Universe’s total cosmic baryon content is made a free parameter then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Ω</mi><mi>b</mi></msub><msub><mi>h</mi><mn>70</mn></msub><mo>=</mo><msubsup><mn>0.049</mn><mrow><mi>−</mi><mn>0.003</mn></mrow><mrow><mi>+</mi><mn>0.004</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">\Omega_bh_{70}=0.049^{+0.004}_{-0.003}</annotation></semantics></math>. This measurement at a late time (remember that we are measuring at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>h</mi><mn>70</mn></msub><annotation encoding="application/x-tex">h_{70}</annotation></semantics></math>) is consistent at the sub-10% level with early Universe constraints of the physical baryon density from Big Bang Nucleosynthesis and the CMB. An equally precise constraint can be obtained for the Hubble constant - <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mn>0</mn></msub><mo>=</mo><msubsup><mn>71</mn><mrow><mi>−</mi><mn>6</mn></mrow><mrow><mi>+</mi><mn>6</mn></mrow></msubsup><mi>k</mi><mi>m</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> </mtext><mspace width="0.333em"></mspace></mrow><msup><mi>s</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> </mtext><mspace width="0.333em"></mspace></mrow><mi>M</mi><mi>p</mi><msup><mi>c</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">H_0=71^{+6}_{-6} km \text{ } s^{-1} \text{ } Mpc^{-1}</annotation></semantics></math> with the caveat that disactually calculating this value <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math> from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Ω</mi><mi>b</mi></msub><annotation encoding="application/x-tex">\Omega_b</annotation></semantics></math> requires fixing the baryon density parameter at the value it would be in the early universe. A possible future task could be cross-correlating a large sample of FRB DMs against other measurements that indicate the large-scale structure. This will enable even tighter bounds of some cosmological parameters and also allow measuring astrophysical feedback.</p>


</section>
</section>
</section>
</section>

 ]]></description>
  <category>astrophysics</category>
  <category>paper-explanation</category>
  <guid>https://ksd3.github.io/blog/2024-12-01-partitioning/</guid>
  <pubDate>Sun, 01 Dec 2024 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Courses I Took at BU</title>
  <dc:creator>Kshitij Duraphe</dc:creator>
  <link>https://ksd3.github.io/blog/2024-05-01-bu-courses/</link>
  <description><![CDATA[ 





<p>A rundown of the courses I took during my Master of Electrical and Computer Engineering at Boston University, along with my thoughts on each.</p>
<section id="fall-2022" class="level2">
<h2 class="anchored" data-anchor-id="fall-2022">Fall 2022</h2>
<section id="ec516-digital-signal-processing" class="level3">
<h3 class="anchored" data-anchor-id="ec516-digital-signal-processing">EC516 Digital Signal Processing</h3>
<p>This was the first real signal processing course I’d taken and it was taught by the great <a href="https://www.bu.edu/eng/profile/hamid-nawab/">Hamid Nawab</a>, who contributed a great deal to Oppenheim’s <a href="https://www.amazon.com/Signals-Systems-Willsky/dp/9332550239"><em>Signals and Systems</em></a>, the primary signal processing textbook used worldwide. The course mostly covered Fourier transforms for discrete signals and was largely mathematical, with very few practical applications. Professor Nawab is a great instructor and is lenient while grading, but the main appeal of this course is that he knows how to teach, and more importantly, how to solve problems. As far as I know, teaching duties for EC516 and EC520 are swapped every few years between Professors Nawab and <a href="https://sites.bu.edu/jkonrad/">Konrad</a>.</p>
</section>
<section id="ec504-advanced-data-structures" class="level3">
<h3 class="anchored" data-anchor-id="ec504-advanced-data-structures">EC504 Advanced Data Structures</h3>
<p>This course was taught by <a href="https://www.bu.edu/eng/profile/richard-brower/">Richard Brower</a>, who has a (fairly unfounded) poor rating on RateMyProfessor. In contrast to many others, I believe this class is one of the best in the ECE department. The content is fairly standard for an algorithms course: arrays, linked lists, trees, graphs, and then a few classical problems such as the knapsack problem. Be prepared to practice. As of May 2024, this course seems to be taught by <a href="https://people.bu.edu/trachten/">Ari Trachtenberg</a>.</p>
</section>
</section>
<section id="spring-2023" class="level2">
<h2 class="anchored" data-anchor-id="spring-2023">Spring 2023</h2>
<section id="ec503-learning-from-data" class="level3">
<h3 class="anchored" data-anchor-id="ec503-learning-from-data">EC503 Learning from Data</h3>
<p>This is a basic introduction to machine learning and is taught by different instructors every semester. I took it with <a href="francesco.orabona.com">Francesco Orabona</a>, who is now at KAUST. As an expert in optimization theory, Professor Orabona starts with PAC learning and devotes the latter two-thirds of his course to algorithms and their implementation. If taught by <a href="https://sites.bu.edu/pi/">Ishwar</a>, I believe the content is reversed—he starts with regression, nearest neighbors, and then moves to optimization algorithms and the underlying theory. Quite homework-heavy.</p>
</section>
<section id="as708-cosmic-plasma-physics" class="level3">
<h3 class="anchored" data-anchor-id="as708-cosmic-plasma-physics">AS708 Cosmic Plasma Physics</h3>
<p>Trial by fire if you haven’t taken AS703 Introduction to Space Physics like I hadn’t. Professor <a href="https://www.bu.edu/astronomy/profile/meers-oppenheim/">Oppenheim</a> is approachable, but the course content is as hard as it gets for linear plasma theory, and solving homework is a team effort. Very homework heavy and requires a background in physics and reading papers just to understand some problems.</p>
</section>
</section>
<section id="fall-2023" class="level2">
<h2 class="anchored" data-anchor-id="fall-2023">Fall 2023</h2>
<section id="ec574-physics-of-semiconductor-materials" class="level3">
<h3 class="anchored" data-anchor-id="ec574-physics-of-semiconductor-materials">EC574 Physics of Semiconductor Materials</h3>
<p>Introductory quantum mechanics and solid-state physics. Professor <a href="https://www.bu.edu/eng/profile/enrico-bellotti/">Bellotti</a> has a great sense of humor and is a good instructor, but his teaching methods are definitely old-school. The latter third of the course is hard because the introduction to solid-state physics does not draw from Brandsen and Joachin.</p>
</section>
<section id="py536-quantum-computing" class="level3">
<h3 class="anchored" data-anchor-id="py536-quantum-computing">PY536 Quantum Computing</h3>
<p>I was fairly disappointed with this course as it assumed the student had a background in graduate-level quantum mechanics, but was advertised as an algorithms course. The homework was obscure and confusing and relied on being able to peruse the literature and completely imbibe it. There is no hand-holding here.</p>
</section>
<section id="ec533-advanced-discrete-mathematics" class="level3">
<h3 class="anchored" data-anchor-id="ec533-advanced-discrete-mathematics">EC533 Advanced Discrete Mathematics</h3>
<p>I will fully admit that I smurfed in this course. The subject matter was something I had done since the age of 13 and Professor <a href="https://www.bu.edu/eng/profile/lev-levitin/">Levitin</a> is great. I wish I had taken his other course, EC534 Discrete Stochastic Models, but I couldn’t afford to.</p>
</section>
<section id="ec562-fourier-optics" class="level3">
<h3 class="anchored" data-anchor-id="ec562-fourier-optics">EC562 Fourier Optics</h3>
<p>While the subject itself is not necessarily difficult, the instructor (Luca Dal Negro) makes you work very hard. Be prepared to spend many hours practicing the work. The textbooks used are Joseph Goodman’s <a href="https://www.amazon.com/Introduction-Fourier-Optics-Joseph-Goodman/dp/0974707724">Introduction to Fourier Optics</a> and David Voelz’s <a href="https://www.spiedigitallibrary.org/ebooks/TT/Computational-Fourier-Optics-A-MATLAB-Tutorial/eISBN-9780819482051/10.1117/3.858456#_=_">Computation Fourier Optics: A MATLAB tutorial</a>.</p>


</section>
</section>

 ]]></description>
  <category>education</category>
  <category>BU</category>
  <category>graduate school</category>
  <guid>https://ksd3.github.io/blog/2024-05-01-bu-courses/</guid>
  <pubDate>Wed, 01 May 2024 04:00:00 GMT</pubDate>
</item>
</channel>
</rss>
