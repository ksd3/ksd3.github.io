[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research Overview",
    "section": "",
    "text": "An overview of my work at the Semeter Lab at Boston University."
  },
  {
    "objectID": "research.html#algorithms",
    "href": "research.html#algorithms",
    "title": "Research Overview",
    "section": "Algorithms",
    "text": "Algorithms\n\nData Processing\n\n1. Ionospheric Assumption Validation\nFor speed of computation, we use fast 2d and 3d natural neighbors interpolation (thick large-scale structures) and VISTA (thick medium-scale structures) (Video Imputation with SoftImpute, Temporal smoothing and Auxiliary data) to validate the ionospheric assumption.\n\n\n\nVISTA Algorithm\n\n\nThe assumption is (somewhat) invalid during solar storms, geophysical storms, magnetic reconnection with solar winds, coronal mass ejections, and so on.\n\n\n2. RINEX conversion\nDeveloped an internal Python package named rinexpy - looking to merge with georinex:\nrinexpy Operations:\n\nMerging and Editing RINEX Files: Provides the ability to merge multiple RINEX files into a single output. Allows for editing of the file contents, such as adjusting header information, modifying observation types, and managing data intervals.\nClock Jump Correction: Automatically detects and corrects clock jumps in GNSS observation data.\nIonospheric Correction: Applies higher-order ionospheric corrections to the GNSS data.\nFile Format Conversion: Supports converting BINEX and RTCM files into the RINEX format.\nInteractive RINEX File Viewer: Includes an interactive, web-based viewer for RINEX files.\n\n\n\n3. Analysis\nCycle slip correction: We design machine learning models for cycle slip correction. These are better than the heuristic ‘stiching’ process that may remove important TEC jumps that could identify an ionospheric phenomenon.\n\n\n\nCycle Slip Example\n\n\n\n\n\nPractical Applications\n\n\nThe technique used is LSTM-based autoencoders, for several reasons:\n\nThe Poker Flat setups are the same receiver in roughly the same area, so we are justified in using LSTM-based autoencoders for this.\nUnder the assumptions that the internals of Pixel phones suffer roughly the same wear-and-tear, autoencoders are straightforward to deploy on Sagemaker and call for inference.\n\n\n\n\nLSTM Architecture\n\n\nWe also account for multipath noise, temperature noise (Rideout and Coster), propagation errors, ionospheric delays, and utilize precise point positioning to estimate slant TEC as accurately as possible.\n\n\n\nSNR Graph\n\n\nTime-series classification: How does TEC change during an auroral event?\n\n\n\nGiven a dataset of time-series TEC curves at some frequencies, we want to classify them to make an early-warning system for auroral events (so citizen scientists can go out and take photographs!)\nYou can probe ionospheric conditions (such as from DMSP) and get extra time series:\n\n\n\nDMSP Data\n\n\nMethod for auroral phenomenon identification:\n\nUse 1D CNN to classify raw TEC changes as STEVE, SAPS, SAID, Discrete Aurora, Continuous Aurora, etc.\nAnalyze satellite flyby data to see what ionospheric, magnetospheric, and thermospheric parameters change in that time period."
  },
  {
    "objectID": "research.html#computer-vision-algorithms",
    "href": "research.html#computer-vision-algorithms",
    "title": "Research Overview",
    "section": "Computer Vision Algorithms",
    "text": "Computer Vision Algorithms\nWe use Max-Tree for faint phenomenon identification, active learning, monocular depth estimation (Depth Prediction Transformers), and Open Set recognition to identify STEVE in citizen science images.\nThe clustering model developed was to classify all-sky images of aurora - using ResNet-18 to extract features, used PCA to reduce dimensionality, and K-Means++ to cluster. Achieved silhouette score of 0.7, Davies-Bouldin index of 0.9, Calinski-Harabase index of 256 indicating high-quality cluster separation."
  },
  {
    "objectID": "research.html#life-on-mars",
    "href": "research.html#life-on-mars",
    "title": "Research Overview",
    "section": "Life on Mars",
    "text": "Life on Mars\nGenerated TEC maps from MARSIS data by modeling with two-layer Chapman functions for Martian ionosphere. Used VISTA to reconstruct (in 3D), 3d volumes of maps. Auxiliary guess was made using natural pixel decomposition, where the 3d structure estimated with tomography. Specifically, it is modeled as a Fredkin integral of the first kind and pLogMART is used as the matrix inversion algorithm. Simulated GPS propagation through it."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Kshitij. I work on human-computer interaction, reinforcement learning, foundation models, and related topics in AI/ML. I’m a May 2024 graduate from Boston University’s Master of Electrical and Computer Engineering program."
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About",
    "section": "",
    "text": "I’m Kshitij. I work on human-computer interaction, reinforcement learning, foundation models, and related topics in AI/ML. I’m a May 2024 graduate from Boston University’s Master of Electrical and Computer Engineering program."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "About",
    "section": "Research Interests",
    "text": "Research Interests\nTheory: Astrophysics and the application of inverse methods to astrophysical phenomena. SciML methods for faster and more reliable scientific understanding.\nPractice: Fast, efficient, and scalable multimodal signal and image processing systems for engineering problems in unstructured, noisy, and sparse scenarios."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n\nM.S. Electrical and Computer Engineering, Boston University, 2024\nB.Tech., College of Engineering Pune (COEP), 2022"
  },
  {
    "objectID": "about.html#cv",
    "href": "about.html#cv",
    "title": "About",
    "section": "CV",
    "text": "CV\n\nResume\nAcademic CV"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kshitij Duraphe",
    "section": "",
    "text": "I’m Kshitij. I work on human-computer interaction, reinforcement learning, foundation models, and all that jazz. I used to work on everything to do with AI understanding and the user experience for security professionals at Absentia Technologies. I previously worked on recommendation systems at Nevara.ai and also developed foundation models at Spatialise. I’m a May 2024 graduate from Boston University’s Master of Electrical and Computer Engineering program.\nAs a student, I worked in Boston University’s Space Physics Lab from October 2022 to May 2024, advised by Joshua Semeter. I also worked at the Hariri Center for NASA’s Life on Mars initiative, developing signal processing and machine learning techniques to analyze the Martian ionosphere and simulated GPS propagation through it. Previously, I was at COEP from 2018-2022.\nI was also a TA/mentor for CRANE 2025’s winter session, instructing students in computational plasma physics. I work on mechanistic interpretability in astrophysics at UniverseTBD.\nI am also at MIT as a SIPB member where I do DevOps + documentation for ArkOS.\nResume | Academic CV | LinkedIn | GitHub"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Kshitij Duraphe",
    "section": "Research Interests",
    "text": "Research Interests\nOn the theory side, I’m broadly interested in astrophysics and the application of inverse methods to astrophysical phenomena. I hope to branch out into more applied SciML and develop faster and more reliable methods to bound predictions for these phenomena.\nOn the practical side, I’m interested in developing fast, efficient, and scalable multimodal signal and image processing systems to solve engineering problems in unstructured, noisy, and sparse scenarios."
  },
  {
    "objectID": "index.html#selected-work",
    "href": "index.html#selected-work",
    "title": "Kshitij Duraphe",
    "section": "Selected Work",
    "text": "Selected Work\n\n(University of Zielona Gora, 2025): Timing and spectral analysis of Cygnus X-1 with a decade of NuSTAR observations. The first part of that work is under review at ApJ.\n(UniverseTBD, 2025): Mechanistic interpretability in astronomy. In particular, we showed that the Platonic Representation Hypothesis holds for astronomy in this paper, and published at the 2025 edition of NeurIPS’ ML4PS workshop. Update: This is now a spotlight paper (top 1%). I gave a small talk at NeurIPS 2025 and a longer AstroAI talk at Harvard in January 2026.\n(BU, 2022-2024): A variety of investigations into the STEVE phenomenon - fluid electrodynamic simulations, citizen science + scientific data-driven reconstruction, deep learning methods for automated detection systems. I defended a Master’s thesis in April 2024 (Committee: Joshua Semeter, Yukitoshi Nishimura, Michael Hirsch) on the latter two topics.\n(BU, 2023): I worked on the NASA Life on Mars initiative at Hariri. I simulated EM propagation through Mars’ ionosphere and developed models for detecting features on GNSS maps.\n(BU, 2024): During the April 8 full solar eclipse, the Semeter Lab members went around New England and into Canada to record GNSS data from consumer cell phones. I helped take observations at MIT and Ogunquit, ME. A poster was presented by Nina Servan-Schreiber at the 2024 CEDAR Workshop. The work is now a paper.\n(GaTech, 2024): A VLM-LLM integrated end-to-end system that generates insurance recommendation reports using a combination of tenant complaints and photos taken by an insurance agent for Hacklytics 2024.\n(MIT, 2024): Developed custom optimizers for linear quantum photonic GANs at iQuHack-24.\n(MIT, 2023): A QCBM-based music generator that won second place in the IBM x Covalent challenge at iQuHack-23.\n(COEP, 2022): Building a neutral hydrogen radio telescope from scratch, advised by Archana Thosar.\n(IIT Bombay, 2021, BU, 2022): Analysis of the binary star system QX Cas with PHOEBE.\n(COEP, 2021): Some linear models for solar power, advised by Suhas Kakade and Rohan Kulkarni."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Kshitij Duraphe",
    "section": "Contact",
    "text": "Contact\nI can be emailed at kshitijd [at] bu.edu or at kshitijduraphe5 [at] gmail.com or at kshitij.ark [at] mit.edu.\nInspired by Piotr Teterwak I am available for “office hours” (offline or through video conferencing) each week to talk about anything you like. I have gone through Masters admissions and the post-COVID US STEM job hunt, and also defended a thesis, so I have some perspective on that as well as graduate student life. Email me with the subject line “OFFICE HOURS” and give me a brief overview of what you want to talk about. Ideally you should be an undergraduate or new (US) Masters student in some field relevant to EECS, but I am happy to talk with new grads and high schoolers as well."
  },
  {
    "objectID": "index.html#miscellaneous",
    "href": "index.html#miscellaneous",
    "title": "Kshitij Duraphe",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nI keep a list of courses I took at BU along with my thoughts on them.\nCheck out some of my writings.\n\nWebsites I find interesting:\n\nscivision.dev: Michael Hirsch singlehandedly carries the GNSS community on his back with Georinex and his website is full of useful tips and tricks for wrangling with C Compilers.\nDan Luu’s website. Commentary on programming and tech-society.\nThe UCR Matrix Profile Page: The best time-series page on the internet."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Resume & CV",
    "section": "",
    "text": "A concise one-page summary of my experience and skills.\nDownload Resume (PDF)"
  },
  {
    "objectID": "cv.html#resume",
    "href": "cv.html#resume",
    "title": "Resume & CV",
    "section": "",
    "text": "A concise one-page summary of my experience and skills.\nDownload Resume (PDF)"
  },
  {
    "objectID": "cv.html#curriculum-vitae",
    "href": "cv.html#curriculum-vitae",
    "title": "Resume & CV",
    "section": "Curriculum Vitae",
    "text": "Curriculum Vitae\nA comprehensive academic CV including publications, presentations, and teaching.\nDownload CV (PDF)"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "PINOs in space physics\n\n\n\nphysics\n\nmachine-learning\n\nresearch\n\n\n\nAn overview of Physics-Informed Neural Operators and their applications in space physics.\n\n\n\n\n\nFeb 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow to get into AI research as an outsider\n\n\n\nmachine-learning\n\ncareer\n\nresearch\n\n\n\nMy perspective on how to break into AI research.\n\n\n\n\n\nFeb 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTransformers from scratch in numpy\n\n\n\nmachine-learning\n\ntutorial\n\ndeep-learning\n\n\n\nAn implementation and demonstration of the transformer architecture from scratch using as few predefined libraries as possible.\n\n\n\n\n\nDec 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPapers explained: Partitioning the Universe’s baryons (Connor et al 2024)\n\n\n\nastrophysics\n\npaper-explanation\n\n\n\nAn annotated explanation of ‘A gas rich cosmic web revealed by partitioning the missing baryons’ by Connor et al. (2024).\n\n\n\n\n\nDec 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCourses I Took at BU\n\n\n\neducation\n\nBU\n\ngraduate school\n\n\n\nA review of the courses I took during my Master’s at Boston University (2022-2024).\n\n\n\n\n\nMay 1, 2024\n\n\nKshitij Duraphe\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2024-05-01-bu-courses/index.html",
    "href": "blog/2024-05-01-bu-courses/index.html",
    "title": "Courses I Took at BU",
    "section": "",
    "text": "A rundown of the courses I took during my Master of Electrical and Computer Engineering at Boston University, along with my thoughts on each."
  },
  {
    "objectID": "blog/2024-05-01-bu-courses/index.html#fall-2022",
    "href": "blog/2024-05-01-bu-courses/index.html#fall-2022",
    "title": "Courses I Took at BU",
    "section": "Fall 2022",
    "text": "Fall 2022\n\nEC516 Digital Signal Processing\nThis was the first real signal processing course I’d taken and it was taught by the great Hamid Nawab, who contributed a great deal to Oppenheim’s Signals and Systems, the primary signal processing textbook used worldwide. The course mostly covered Fourier transforms for discrete signals and was largely mathematical, with very few practical applications. Professor Nawab is a great instructor and is lenient while grading, but the main appeal of this course is that he knows how to teach, and more importantly, how to solve problems. As far as I know, teaching duties for EC516 and EC520 are swapped every few years between Professors Nawab and Konrad.\n\n\nEC504 Advanced Data Structures\nThis course was taught by Richard Brower, who has a (fairly unfounded) poor rating on RateMyProfessor. In contrast to many others, I believe this class is one of the best in the ECE department. The content is fairly standard for an algorithms course: arrays, linked lists, trees, graphs, and then a few classical problems such as the knapsack problem. Be prepared to practice. As of May 2024, this course seems to be taught by Ari Trachtenberg."
  },
  {
    "objectID": "blog/2024-05-01-bu-courses/index.html#spring-2023",
    "href": "blog/2024-05-01-bu-courses/index.html#spring-2023",
    "title": "Courses I Took at BU",
    "section": "Spring 2023",
    "text": "Spring 2023\n\nEC503 Learning from Data\nThis is a basic introduction to machine learning and is taught by different instructors every semester. I took it with Francesco Orabona, who is now at KAUST. As an expert in optimization theory, Professor Orabona starts with PAC learning and devotes the latter two-thirds of his course to algorithms and their implementation. If taught by Ishwar, I believe the content is reversed—he starts with regression, nearest neighbors, and then moves to optimization algorithms and the underlying theory. Quite homework-heavy.\n\n\nAS708 Cosmic Plasma Physics\nTrial by fire if you haven’t taken AS703 Introduction to Space Physics like I hadn’t. Professor Oppenheim is approachable, but the course content is as hard as it gets for linear plasma theory, and solving homework is a team effort. Very homework heavy and requires a background in physics and reading papers just to understand some problems."
  },
  {
    "objectID": "blog/2024-05-01-bu-courses/index.html#fall-2023",
    "href": "blog/2024-05-01-bu-courses/index.html#fall-2023",
    "title": "Courses I Took at BU",
    "section": "Fall 2023",
    "text": "Fall 2023\n\nEC574 Physics of Semiconductor Materials\nIntroductory quantum mechanics and solid-state physics. Professor Bellotti has a great sense of humor and is a good instructor, but his teaching methods are definitely old-school. The latter third of the course is hard because the introduction to solid-state physics does not draw from Brandsen and Joachin.\n\n\nPY536 Quantum Computing\nI was fairly disappointed with this course as it assumed the student had a background in graduate-level quantum mechanics, but was advertised as an algorithms course. The homework was obscure and confusing and relied on being able to peruse the literature and completely imbibe it. There is no hand-holding here.\n\n\nEC533 Advanced Discrete Mathematics\nI will fully admit that I smurfed in this course. The subject matter was something I had done since the age of 13 and Professor Levitin is great. I wish I had taken his other course, EC534 Discrete Stochastic Models, but I couldn’t afford to.\n\n\nEC562 Fourier Optics\nWhile the subject itself is not necessarily difficult, the instructor (Luca Dal Negro) makes you work very hard. Be prepared to spend many hours practicing the work. The textbooks used are Joseph Goodman’s Introduction to Fourier Optics and David Voelz’s Computation Fourier Optics: A MATLAB tutorial."
  },
  {
    "objectID": "blog/2025-02-16-pino/index.html",
    "href": "blog/2025-02-16-pino/index.html",
    "title": "PINOs in space physics",
    "section": "",
    "text": "When I was just starting at the Semeter Lab in October 2022, one of the first tasks assigned to me (which eventually became the first section of my thesis) was to do what is known to computer graphics scientists as novel view synthesis using different 2D spectra viewing the ionosphere.\nAs with most deep learning tasks, the trick to this challenge often lies in data preprocessing, choosing the right architecture, activation function, designing the right type of layer for the data you are processing, and designing the loss function. When dealing with physical simulations, a natural choise is to start with a Physics-Informed Neural Network. There is no difference between an ANN and a PINN. ANNs are universal function approximators which are trained by backpropagating errors through the entire network and updating the weight of each node. Many fields in deep learning are built on defining this loss function itself. The famous YOLO algorithm is entirely a result of loss function engineering. While the idea itself predates machine learning, modifying the loss function and training for long enough with gradient descent gets you fairly accurate results in many scenarios.\nThe idea behind a PINN is to map a set of input points to their outputs, where the output is decided as the result of passing each point through a solution of the physical system being modeled. The most common way to get the output is to map input points to the solution of a particular differential equation that the physical system is being modeled as. Since ANNs can fit any function if trained for long enough, PINNs are designed as a neural network that are trained for shorter time periods and generalize to solutions of the differential equation that are not seen in the training data. The way they do this is by adding the differential equation itself to the loss function.\nThis idea can be taken further. A physics-informed neural operator tries to generalize to the entire domain. Specifically, given an operator F:X→YF:X \\rightarrow Y, the idea is to take some points from XX and their corresponding transformations from YY and learn FF. That is: given a neural network represented as a function N(x),x∈XN(x), x \\in X, we want to make NN arbitrarily close to FF. The resulting matrix multiplication that NN performs should be close to the transformation FF, which may not be a matrix multiplication. The way you feed points from XX into NN is often modalities such as an image. In simple terms: If you have a video of waves in the ocean, the PINO should be able to predict where the wave is going (that is, where it is as time t+1t+1) given only the picture of the wave (the current frame) at the current time tt.\nThe way this is done is also very simple. The idea is to take a Fourier transform of the input data, multiply it by a randomly-initialized weight matrix, and take the inverse Fourier transform and compare the results. Your backpropagation will update the weight matrix itself. In practice, noise in your input data is high frequency, so the weight matrix is often initialized to favour lower frequencies in the Fourier-transformed-data. The loss function is often modified as well to include the physics-informed loss.\nThe natural generalization is the implicit neural representation. This means training a neural network that more or less overfits to the specific function at all points. I have used INRs for modeling plasma bubbles in the ionosphere.\nThis is a very simple overview of an interesting topic. So far PINOs have found applications in CFD simulations. I, however, used them for MHD simulations. I modeled STEVE-like phenomena in the ionosphere for novel-view generation and 3D reconstruction. This was done to answer some outstanding questions in ionospheric plasma physics; namely, what can we infer about the physical processes going on in the atmosphere if the phenomenon is so-and-so large?"
  },
  {
    "objectID": "blog/2025-02-16-pino/index.html#plasma-bubbles",
    "href": "blog/2025-02-16-pino/index.html#plasma-bubbles",
    "title": "PINOs in space physics",
    "section": "Plasma Bubbles",
    "text": "Plasma Bubbles\nThe graphs below show simulations of plasma bubbles at low resolution using a PINO with the SIREN architecture. The reconstruction is pretty good, even if the SIREN architecture is not designed for diffuse boundaries. This works because SIREN architectures work well at multiple scales.\n\n\n\nPlasma Bubble 1\n\n\n\n\n\nPlasma Bubble 2\n\n\n\n\n\nPlasma Bubble Reconstruction"
  },
  {
    "objectID": "blog/2025-02-16-pino/index.html#steve-mhd",
    "href": "blog/2025-02-16-pino/index.html#steve-mhd",
    "title": "PINOs in space physics",
    "section": "STEVE MHD",
    "text": "STEVE MHD\nA full STEVE-like simulation can be done with Gemini3D. I modeled the basic underlying phenomena and simulated them over lower resolutions and larger timescales. You can still see the underlying structure in the video.\n\n\n\nFor this, I used 2D spectral convolutions. The PINO didn’t quite reconstruct the phenomenon because of the diffuse boundaries of ionospheric phenomena, which is what SIREN activations are designed for. I eventually ended up going with classical algorithms that helped me do a 3D reconstruction of STEVE.\nHere’s a single frame (frame 39) from that video. I only used 16% of the simulation as training data, and it still reconstructed everything further on pretty well!\n\n\n\nMHD PINO Frame"
  },
  {
    "objectID": "blog/2024-12-07-transformers/index.html",
    "href": "blog/2024-12-07-transformers/index.html",
    "title": "Transformers from scratch in numpy",
    "section": "",
    "text": "Transformers are a type of sequence to sequence model, i.e., given a sequence of characters, which may be split into words, transformers are able to convert that sequence to another sequence in a way that preserves the original sequence’s ‘meaning’ without using any predefined rules. An example of a sequence-to-sequence task is text translation, such as converting the English sentence “I ate an apple” into the Italian equivalent: “Ho mangiato una mela.”\nTransformers are an example of an encoder-decoder architecture. Encoder-decoder architectures take input data, squeeze it into a kind of secret code (often called ‘creating a latent representation’), and sometimes decode the squeezed data to perform useful tasks. An example of an encoder-decoder architecture and a task it may be used on would be using an autoencoder to denoise images.\nThis page contains an implementation and demonstration of the transformer architecture from scratch using as few predefined libraries as possible in order to give the reader an understanding of what really goes on in each step of the process. We avoid using predefined models or implementations of algorithms as much as possible.\n\n\nWe address the following questions:\n\nWhat is the fundamental principle of a transformer?\nIn very general terms, what does a transformer do?\nWhat specific tasks do we need to do in order to implement a transformer?\n\n\n\nThe fundamental principle of a transformer is that one-hot vectors can be used to look up particular rows of a matrix, and you can exploit this to selectively extract, combine, and mask information from your input to produce better outputs1. A lot of readers, especially NLP enthusiasts, may immediately have a problem with this statement. After all, we have not used the most famous term associated with a transformer (attention) while stating this fundamental principle. We also did not say anything about feature embeddings, long-range dependencies, contextual relationships, and encodings - all terms that are used when talking/reading about transformers. This is done for two reasons. First, I believe that it is extremely important to understand what exactly is going on in terms of as many elementary operations as possible. I believe that this necessarily precludes using domain-specific jargon. Second, I am tired of reading innumerable blogs2, code comments on GitHub3, and slides that fail to give you understanding4.\n\n\n\nA transformer takes in a sequence of elements (this sequence is often long), figures out how different elements in that sequence are related, then squeezes that sequence into a list of numbers that capture any inherent ‘meaning’5. It then takes that list of numbers and unsqueezes it into a different sequence that tries to preserve the ‘meaning’ of the original sequence. As shown in the welcome section, transformers can be used for language translation.\n\n\n\n\nFigure out a way to split individual elements in a sequence (hereafter ‘words and punctuation in a sentence’) and find a way to feed them to the computer.\nMake the computer squeeze these words into a list of numbers it can understand.\nMake the computer unsqueeze the list of numbers into words and form sentences in another language.\n\n\n\n\n\nSuppose our input language only has four words (‘My’, ‘rabbit’, ‘likes’, ‘bananas’), and no punctuation at all. Sentences from our language could be ‘rabbit likes bananas’ or ‘My rabbit likes bananas’ or ‘likes My bananas’ or ‘bananas rabbit My’. For the sake of sanity, let’s assume that our language only has the sentence ‘My rabbit likes bananas’. We want to translate this into Italian: ‘Al mio coniglio piacciono le banane’. How do we feed our initial sentence to a computer?\n\n\nWe first need to split the sentence into individual words. Because our language does not have any sort of punctuation, we can do what is called whitespace tokenization. This is the most natural way of splitting an English sentence - assume that individual words are separated by a blank space, read through the sentence, and store all characters between two whitespaces as a single word 6.\nfrom typing import List #for clean function annotation\n\ndef whitespace_tokenizer(sentence: str) -&gt; List[str]:\n    \"\"\"\n    Function that reads a string/sentence and outputs a list of strings, where each output string is a word in that sentence. Each word is considered to be delimited by whitespaces.\n\n    Input:\n        sentence: str - assumed nonempty for explanation purposes\n    Output:\n        list of strings\n    \"\"\"\n    tokenized_sentence=[] #final output, a list of words\n    current_word=[] #list to store the current word\n\n    #The technique to whitespace tokenize the sentence is to iterate through it, and store each non-whitespace character in current_word. \n    #Once a whitespace is encountered, append the contents of current_word to tokenized_sentence and clear current_word\n    for i in sentence:\n        if i==\" \":\n            if current_word:\n                tokenized_sentence.append(''.join(current_word)) #append to the list of tokens\n                current_word=[]#reset current_word\n        else:\n            current_word.append(i)\n    \n    #this still leaves the final word in, so add it last\n    if current_word:\n        tokenized_sentence.append(''.join(current_word))\n    \n    #delete the current word from memory explicitly (not required)\n    del current_word\n\n    return tokenized_sentence\n\nenglish_sentence=\"My rabbit likes bananas\"\nprint(\"Your list of tokens for the English sentence is:\", whitespace_tokenizer(english_sentence))\nenglish_tokenized_sentence=whitespace_tokenizer(english_sentence) #save the tokenization in a list\n\nitalian_sentence=\"Al mio coniglio piacciono le banane\"\nprint(\"Your list of tokens for the Italian sentence is:\", whitespace_tokenizer(italian_sentence))\nitalian_tokenized_sentence=whitespace_tokenizer(italian_sentence)\nYour list of tokens for the English sentence is: ['My', 'rabbit', 'likes', 'bananas']\nYour list of tokens for the Italian sentence is: ['Al', 'mio', 'coniglio', 'piacciono', 'le', 'banane']\n\n\n\nHow do we feed these words into a computer? One way of doing it would be by assigning each individual word to a real number: [‘My’, ‘rabbit’, ‘likes’, ‘bananas’] -&gt; [‘935.88’, ‘-28124.4483957’, ‘3’, ‘-2’]. This is inefficient, as the amount of precision you would need to implement would increase computatational costs and storage requirements. A better way of storing a word would be to store it in a vector. Here is how we can do this. Given a vector, stored as a column matrix with NN rows, where NN is the number of words in your vocabulary, replace one of the zeros with 1 such that the position of the 1 is unique for that particular word. Then, stack those vectors side by side to form a matrix where each row and column has only one 1 and all other elements are zero. Such vectors are called ‘one-hot’ vectors and this is a type of encoding called one-hot encoding.\nThis is illustrated below:\n‘My’=$\n(0010)\\begin{pmatrix}\n0\\\\\n0\\\\\n1\\\\\n0\n\\end{pmatrix}\n,′rabbit′=, 'rabbit'=\n(0100)\\begin{pmatrix}\n0\\\\\n1\\\\\n0\\\\\n0\n\\end{pmatrix}\n,′likes′=, 'likes'=\n(1000)\\begin{pmatrix}\n1\\\\\n0\\\\\n0\\\\\n0\n\\end{pmatrix}\n,′bananas′=, 'bananas'=\n(0001)\\begin{pmatrix}\n0\\\\\n0\\\\\n0\\\\\n1\n\\end{pmatrix}\n,′Myrabbitlikesbananas′=,\n'My rabbit likes bananas'=\n(0010010010000001)\\begin{pmatrix}\n0 & 0 & 1 & 0\\\\\n0 & 1 & 0 & 0\\\\\n1 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n$\nLet’s call the last matrix WW, for ‘word matrix’. Observe that when any one-hot encoded is multiplied with another matrix, by the rules of matrix multiplication, the column of the second matrix corresponding to the position of the 1 in the column vector is ‘pulled out’7. This is illustrated below:\n$\n(0010)\\begin{pmatrix}\n0 & 0 & 1 & 0\n\\end{pmatrix}\n$ $\n(0.20.4−91+i291245.32853902i3223212430.4820.212241eπTREE(3)−TREE(3))\\begin{pmatrix}\n0.2 & 0.4 & -9 & 1+i \\\\\n29 & 12 & 45.328539 & 0 \\\\\n2i & 32 & 2 & 32 \\\\\n12 & 43 & 0.482 & 0.212 \\\\\n241 & e^{\\pi} & TREE(3) & -TREE(3)\n\\end{pmatrix}\n$ =(−945.32853920.482TREE(3))=\\begin{pmatrix}\n-9 \\\\\n45.328539 \\\\\n2 \\\\\n0.482\\\\\nTREE(3)\n\\end{pmatrix}\nIf you’re able to construct this second matrix, then it can potentially lead to something interesting8. There are of course other ways to encode words, and for practical language tasks you take someone else’s encoding and use it, but it is important to understand the core principle.\nimport numpy as np\nfrom typing import List, Dict, Tuple\nimport random\n\ndef create_concatenated_matrix_from_tokens(tokens: List[str]) -&gt; np.ndarray:\n    \"\"\"\n    Function that creates a concatenated one-hot encoded matrix from a tokenized sentence.\n    \n    Input:\n        tokens: List containing tokens\n    Output:\n        tokenized_matrix: A 2-D one-hot encoded np.ndarray of tokens. Each row and column contains only one 1. Always square.\n    \"\"\"\n\n    #The idea is to simply generate a diagonal matrix which will be one-hot encoded by definition\n    #Create a dictionary to map each token to a unique index\n    token_to_index={token: idx for idx, token in enumerate(tokens)} #get index-token pair from the input list\n    #Initialize the matrix with zeros\n    tokenized_matrix=np.zeros((len(tokens), len(tokens)), dtype=int)\n    #Populate the one-hot encoded matrix\n    for token in tokens:\n        index=token_to_index[token]\n        tokenized_matrix[index][index]=1  #Set the diagonal element to 1\n\n    return tokenized_matrix, token_to_index #this second variable is returned to pull out a random token and corresponding vector later\n\nenglish_onehot_matrix, english_token_to_index=create_concatenated_matrix_from_tokens(english_tokenized_sentence)\nprint(\"The English one-hot encoded matrix is:\\n\", english_onehot_matrix)\n\n#let's pull out a random token and a one-hot encoded vector to see how it pulls out specific features\ndef get_random_token_and_vector(token_to_index: Dict[str, int], one_hot_matrix: np.ndarray) -&gt; Tuple[str, np.ndarray]:\n    \"\"\"\n    Function that pulls a random token and its corresponding one-hot vector.\n\n    Input:\n        token_to_index: Dictionary mapping tokens to their indices\n        one_hot_matrix: 2D NumPy array containing one-hot vectors\n    Output:\n        A tuple containing the random token and its corresponding one-hot vector\n    \"\"\"\n    #Randomly select a token\n    random_token=random.choice(list(token_to_index.keys()))\n    \n    #Get the corresponding one-hot vector\n    one_hot_vector=one_hot_matrix[token_to_index[random_token]]\n    \n    return random_token, one_hot_vector\n\nrandom_token, corresponding_one_hot_vector=get_random_token_and_vector(english_token_to_index, english_onehot_matrix)\nprint(\"Let's pick a random token:\", random_token, \"\\nThe corresponding one-hot vector is:\", corresponding_one_hot_vector)\n\n#generate a random matrix to demonstrate pulling out certain columns/rows\nrandom_matrix=np.random.rand(len(english_tokenized_sentence), len(english_tokenized_sentence))\nprint(\"Multiplying an example random matrix\\n\", random_matrix, \"\\nby\", random_token+\"'s one-hot vector\", corresponding_one_hot_vector, \"\\npulls out the row\", np.matmul(corresponding_one_hot_vector, random_matrix), \"\\nand multiplying by the tranpose of that vector pulls out the column:\\n\", np.matmul(random_matrix, corresponding_one_hot_vector[:, np.newaxis]))\nThe English one-hot encoded matrix is:\n [[1 0 0 0]\n [0 1 0 0]\n [0 0 1 0]\n [0 0 0 1]]\nLet's pick a random token: bananas \nThe corresponding one-hot vector is: [0 0 0 1]\nMultiplying an example random matrix\n [[0.48871169 0.52574402 0.78833029 0.7045616 ]\n [0.76188795 0.13720883 0.39406852 0.02774654]\n [0.27090269 0.35964049 0.20715361 0.3064574 ]\n [0.23701048 0.67718606 0.87441259 0.05116356]] \nby bananas's one-hot vector [0 0 0 1] \npulls out the row [0.23701048 0.67718606 0.87441259 0.05116356] \nand multiplying by the tranpose of that vector pulls out the column:\n [[0.7045616 ]\n [0.02774654]\n [0.3064574 ]\n [0.05116356]]\n\n\n\nAn immediate application of this specific kind of matrix multiplication is as follows. Suppose we have the following sentence in our four-word language: ‘My rabbit’. Our task is to predict the next word that comes after it9. One easy way of doing this is by observing that we only have four options. We can construct the following four sentences: | Next Word | Potential next (possible incomplete) sentence | |—-|—-| |My|My rabbit My| |rabbit|My rabbit rabbit| |likes|My rabbit likes| |bananas|My rabbit bananas|\nHow do we decide what word comes next? Well, we can’t decide on our own. Perhaps, to an alien whose language consists of only four words that sound exactly like English words, the sentence ‘My rabbit My’ would translate to English as ‘I am in need of two oranges and a deck of playing cards.’ The sentence ‘My rabbit rabbit’ would translate to ‘I am on fire’. The sentence ‘My rabbit rabbit rabbit rabbit bananas bananas rabbit bananas My likes likes bananas’ would translate to ‘Yes’ (remember, I have not put any limits on the length of the sentences!). The point of these examples is to show you that there is no way for us to predict the next word unless we have some idea of what it is going to be. One way to solve this problem is for a third party (say a talking dog) to step in and say, “I’ve been around these aliens, and I’ve observed that whenever they begin a sentence with ‘My rabbit’, the next word is ‘bananas’ 10% of the time, ‘likes’ 85% of the time, ‘rabbit’ 5% of the time, but ‘My’ never comes after ‘rabbit’. Is there some way for you to use this information? Also, whatever I say is always true.”\nSince we have no better option, let’s trust the talking dog. We can in fact use its information in the following way. We can construct the following vector that shows the probability of predicting the next word after ‘rabbit’, if spoken by an alien.\n$\n\nbananaslikesrabbitMy[0.10.850.050]\\begin{aligned}\n&\\text{bananas}\\\\\n&\\text{likes}\\\\\n&\\text{rabbit}\\\\\n&\\text{My}\\\\\n\\end{aligned}\n\\quad\n\\begin{bmatrix}\n0.1 \\\\ 0.85 \\\\ 0.05 \\\\ 0\n\\end{bmatrix}\n\n$\nThis is somewhat useful. We know that there is a high chance that the next word in the sentence will be ‘likes’, so the possible incomplete sentence will now probably be ‘My rabbit likes’. But wait a minute. This vector of probabilities is like the column we pulled out of the matrix above. Is it possible to reconstruct this matrix? We can certainly do so - just assume that the dog is always true and start interrogating the dog about the probabilities of the next word after each word in the language, regardless of the context. Let’s assume the dog is happy to tell us this, so we now have the following matrix:\n$\n\nbananaslikesrabbitMybananas0.1000likes0.900.070.03rabbit0.10.850.050My0.20.010.790\\begin{array}{r|cccc}\n\\text{} & \\text{bananas} & \\text{likes} & \\text{rabbit} & \\text{My} \\\\\n\\hline\n\\text{bananas} & 0.1 & 0 & 0 & 0 \\\\\n\\text{likes}   & 0.9 & 0 & 0.07 & 0.03  \\\\\n\\text{rabbit}  & 0.1 & 0.85 & 0.05 & 0 \\\\\n\\text{My}      & 0.2 & 0.01 & 0.79 & 0\\\\\n\\end{array}\n\n$\nThe matrix is read row-first, column-second i.e. the probability that the word ‘likes’ occurs after ‘rabbit’ is 0.85.\nThis tells us something about how the language is constructed. We know that if we hear an alien say ‘rabbit’, there is a very high chance that it will say ‘likes’ next. If we hear it say ‘likes’, there is a very high chance it will say ‘bananas’ next. There is also a very small chance it will say ‘My’ after ‘likes’, but it will never say ‘likes’ after ‘likes’. Therefore, to a first order, we can construct this matrix of probabilities that tells us what the next word in the language is going to be. In more formal terms, this is the stochastic matrix of a first-order Markov chain. It is first-order because the next word in the language only depends on the current word of the language.\nBut wait. Languages tend to have meaning when several words are used together. For example, in English, the word ‘cold’ refers to something whose molecules have a lower average kinetic energy than a reference object. However, the phrase ‘cold call’ means unsolicited phone calls typically made for business purposes. If you only know that the current word in the sentence is ‘cold’ and your probability matrix says that the word ‘call’ appears after ‘cold’ 70% of the time, you may say that the sentence ‘The water is cold.’ is incomplete and would complete it by saying ‘The water is cold call.’, which makes no sense10. What do we do?\nThe natural approach is to say, “I know combinations of words tend to change the meaning of a phrase11, but I don’t have any idea what constitutes a phrase in my unknown language, nor do I know if the ‘meaning’ of the sentence itself changes if a two words are present in adjacent positions12. Let me do the same thing I did for my first-order Markov chain. Instead of asking the talking dog the probabilities of the next word after my current word, I will look at the probabilities of the next word after my current word if another word is present in the sentence.”\nSpecifically, you can ask the talking dog the questions “If ‘rabbit likes’ is present in the sentence, what is the probability that the next word is ‘bananas’? What about ‘rabbit’, ‘My’, and ‘likes’? If ‘My rabbit’ is present, what is are the probabilities for the next word?” and construct the same matrix as we did above. Since we are looking at every pair of words, the number of rows of the matrix quickly grows in size. If there are 5 words in the language, the number of two-word pairs is 20 (obtained from $ 5 $) since the order matters. If there are 100 words, there are 4950 pairs. If there are 260,000 words (a quick Google search tells me that this is roughly the number of words in Italian) then there are 33799870000 pairs. And this is just for consecutive word pairings! If we attempt to look even further back i.e. three-word pairs, there will be even more. It is easy to see that the amount of space required to store this prediction matrix grows exponentially13.\n$\n\nbananaslikesrabbitMylikes bananas0TREE(4)TREE(5)00.02My rabbit00.826TREE(3)TREE(4)0.03rabbit likes0.0246820.5π10e0My bananas0.00400.120.0018256151and so on..\\begin{array}{r|cccc}\n\\text{} & \\text{bananas} & \\text{likes} & \\text{rabbit} & \\text{My} \\\\\n\\hline\n\\text{likes bananas} & 0 & \\frac{TREE(4)}{TREE(5)} & 0 & 0.02 \\\\\n\\text{My rabbit}   & 0 & 0.826 & \\frac{TREE(3)}{TREE(4)} & 0.03  \\\\\n\\text{rabbit likes}  & 0.024682 & 0.5 & \\frac{\\pi}{10e} & 0 \\\\\n\\text{My bananas}      & 0.004 & 0 & 0.12 & 0.0018256151\\\\\n\\text{and so on..}\n\\end{array}\n\n$\nGiven a sufficiently large prediction matrix containing all possible words and combinations of all possible lengths, we are able to predict the next word. Note that we have not said anything about actually choosing the next word in this situation, as this leads to problems. One problem is that we still do not know how to deal with cases where there is an equal chance of two words appearing after our current word. Let’s ignore this for now and focus on the biggest one: We want to avoid actually constructing any such matrix. Let’s try another trick. Let’s say, “The next word in a sentence is easier to predict if another word appears before the current word, but not necessarily directly before it. It may happen sometimes, but there is no reason why it should be like this. Here is my hypothesis. I think that it is easier to predict the next word in the sentence given a probability matrix containing all possible combinations of words where the second word is the current word.” This would look something like:\n$\n\nbananaslikesrabbitMylikes bananas0000.01My bananas0000.03rabbit bananas0.10.00000023eπ0bananas bananas0.0040.0468260.120.5151and so on...\\begin{array}{r|cccc}\n\\text{} & \\text{bananas} & \\text{likes} & \\text{rabbit} & \\text{My} \\\\\n\\hline\n\\text{likes bananas} & 0 & 0 & 0 & 0.01 \\\\\n\\text{My bananas}   & 0 & 0 & 0 & 0.03  \\\\\n\\text{rabbit bananas}  & 0.1 & 0.00000023 & \\frac{e}{\\pi} & 0 \\\\\n\\text{bananas bananas}      & 0.004 & 0.046826 & 0.12 & 0.5151\\\\\n\\text{and so on...}\n\\end{array}\n\n$\nThis is much better to work with. Note that this is no longer a representation of a Markov chain, as we cannot simply look at the row corresponding to the current word and predict the next one. What can we do instead? We can say: “Okay, let’s say that these probabilities represent how much these pairs contribute to the next word in the sequence. We call these probabilities as votes and to predict the next word, we can sum over each column and compare these sums to determine the next word.” This is good, because now we are capturing long-range/skip dependencies in the language/sequence. Each row now represents one of many features that can describe our sequence at a particular point.\nThis is more clearly illustrated when you have, say, only two possible sentences in the language, but the main takeaway from actually doing this task for a set vocabulary and finite amount of sentences in the language is the observation that many elements in this probability matrix do not matter. They can either be so small that they are practically zero, or something like 0.5, which means that the next word is equally likely to appear regardless of the sentence, so it may not matter too much. What we are really interested in are elements we can distinguish. For example, suppose that the two sentences that were possible in our language were ‘My rabbit likes bananas’ and ‘My bananas likes rabbit’. If we had the incomplete sentence ‘My rabbit likes’, then we could ask the talking dog to give us this matrix, and what we would see is that the matrix has a large number of zeros but a 1 for ‘bananas’, enabling us to do this sum-over-columns technique to accurately predict the next word, even with a deep dependency. To be fair, this example is a bit contrived and longer sentences would illustrate the point much more easily.\nThis is still pretty bad. Real languages have a large number of words. Our talking dog could have only been around aliens who lived on a certain continent of the alien planet, which led to them developing their own dialect. If you think about it for just a little bit, it is easy to see that this sum-over-columns approach can end up telling us that the next word in the incomplete sentence ‘Japan is east of’ can be ‘China’, with a vote total of 2339, and ‘Mongolia’, with a vote total of 2340. Sure, we can still pick ‘Mongolia’ as the next word, but such a small difference can naturally be induced by statistical noise, unknowingly biased probability matrices, and other factors (such as us messing up the addition!). Are there ways to overcome this?\nOne approach is to modify the values in the columns before you sum them up, in a way that allows us to differentiate between them even more. One way to do this is to simply sum all the values and divide each value by the sum, to get a fractional representation. This is not very helpful - it preserves the same relation between the numbers in terms of scaling. Converting a column of [1,2,3] to [0.1666, 0.3334, 0.5] preserves the scaling. To overcome this, we utilize the independence from irrelevant alternatives axiom of decision theory, which states that irrelevant choices should not affect the relative probability of choosing between the things you really want to choose between. In mathematical terms, this means that if you have a set of numbers x∈Xx \\in X and you want to decide between x1x_1 and x2x_2 but x3...xnx_3...x_n are small values that are affecting your confidence, you can suppress x3...xnx_3...x_n by replacing each variable in the following way:\nxi→exi∑i=1i=nexix_i\\rightarrow\\frac{e^{x_i}}{\\sum_{i=1}^{i=n}e^{x_i}}\nThis is the famous softmax function which is more or less used to convert a probability distribution to another probability distribution14. The important thing is that the softmax function suppressed irrelevant values (as ek→1e^k \\rightarrow 1 as k→0k \\rightarrow 0).\nHowever, the softmax function is also not applicable to our scenario. Suppose we did actually end up converting the votes to a probability distribution and summing them. What would it actually look like? Let’s do an example below:\n$\n(00.510)→(0.157060.2589480.4269330.15706)\\begin{pmatrix}\n0 \\\\ 0.5 \\\\ 1 \\\\ 0\n\\end{pmatrix}\n\\rightarrow\n\\begin{pmatrix}\n0.15706 \\\\ 0.258948 \\\\ 0.426933 \\\\ 0.15706\n\\end{pmatrix}\n$\nThe sum of the softmaxed vector elements is 1. This is correct, because we did just convert it to a probability distribution. So this approach, while it did ‘suppress’ the smaller values, does not actually help us with voting. What can we do?\n“Okay,” we say. “Let’s do something else. Instead of attempting to modify every value, let’s just discard the values that aren’t important15. First, let’s look at how to extract specific features from our matrix. We know one-hot encoded vectors pull relevant rows/columns out of the matrix, so let’s make a one-hot encoded vector to pull out the relevant features in the matrix in the following way. We construct a vector initially filled with zeros featuring all possible pairs in sentence where the second word is the current word, and the first word has all other words (possibly including the current word, depending on the dimensions of our matrix). Then, if the first word appears before the current word in the sentence, set that element to 1. This vector allows us to pull out the features of our probability matrix that are ‘active’ until that current point.”\nThis would look like:\n$\n\nMy likesrabbit likesbananas likes(110)\\begin{aligned}\n&\\text{My likes}\\\\\n&\\text{rabbit likes}\\\\\n&\\text{bananas likes}\\\\\n\\end{aligned}\n\\quad\n\\begin{pmatrix}\n1 \\\\ 1 \\\\ 0\n\\end{pmatrix}\n^\n$ $\n\nbananaslikesrabbitMyMy likes1000rabbit likes1000.03bananas likes10.00000023eπ0and so on...\\begin{array}{r|cccc}\n\\text{} & \\text{bananas} & \\text{likes} & \\text{rabbit} & \\text{My} \\\\\n\\hline\n\\text{My likes} & 1 & 0 & 0 & 0 \\\\\n\\text{rabbit likes}   & 1 & 0 & 0 & 0.03  \\\\\n\\text{bananas likes}  & 1 & 0.00000023 & \\frac{e}{\\pi} & 0 \\\\\n\\text{and so on...}\n\\end{array}\n\n$\nNote the transpose sign. We can see that the matrix multiplication will suppress those elements in the pulled out feature vectors where pairs taking into account words appearing after the current word in the sequence will be suppressed i.e. we cannot use knowledge of the entire sentence to predict the next sentence. We have now ‘suppressed the future’, but we still need to figure out what feature elements in our sequence are important. This is still an unknown, but what we can do is use another one-hot encoded vector to multiply this suppressed vector, to suppress even more. That is: we can compute the pairwise product to return a vector after multiplying our two vectors. Where can we get this second one-hot encoded vector? Let’s assume that the talking dog gave this to us. The point is that if we manage to suppress information then our voting becomes much stronger, as a lot of elements will be 0. The trick is now to find out how to create this second vector so that we suppress irrelevant information.\nIncidentally, the second form of suppression is the idea behind attention.\nimport numpy as np\nfrom typing import List, Dict, Tuple\nimport random\n\n#remember that the english sentence is \"My rabbit likes bananas\"\ndef generate_biased_probability_matrix(size: int) -&gt; np.ndarray:\n    \"\"\"\n    Function to generate a square probability matrix where each row and column \n    has one value significantly higher than the others.\n\n    Input:\n        size: The number of rows and columns in the square matrix\n    Output:\n        biased_matrix: A 2-D np.ndarray where each row and column has one high-probability value\n    \"\"\"\n    #the technique is to generate a uniform matrix and randomly assign biased high probability values in each row and column\n    biased_matrix=np.random.uniform(0.01, 0.05, (size, size))\n    \n    #generate high probability values for each row and column\n    high_probabilities=np.random.uniform(0.7, 0.9, size=size)\n    \n    #ghuffle indices to randomly distribute the high probabilities across columns\n    indices=np.arange(size)\n    np.random.shuffle(indices)\n    \n    #assign one high probability per row and column\n    for i in range(size):\n        biased_matrix[i, indices[i]]=high_probabilities[i]\n    \n    #normalize each row to sum to 1\n    biased_matrix=biased_matrix/biased_matrix.sum(axis=1, keepdims=True)\n    \n    return biased_matrix\n\nsize=4\nexample_probability_matrix=generate_biased_probability_matrix(size)\nprint(\"As an example, a second-order probability matrix with skip dependencies given to us by the talking dog can be this:\")\nprint(example_probability_matrix)\n\n\ndef softmax(numbers: List[int])-&gt;List[int]:\n    \"\"\"\n    Function to softmax a set of numbers\n    Input:\n        numbers: a list of integers\n    Output:\n        The list, softmaxed\n    \"\"\"\n\n    exponential_list=np.exp(numbers)\n    softmaxed_numbers=[np.exp(number)/sum(exponential_list) for number in numbers]\n    return softmaxed_numbers\n\n\ndef digram_one_hot_encoding(sentence: str, tokens: List[str], index_of_word: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate one-hot encoding vectors for digrams based on a user-defined index.\n\n    Input:\n        sentence: Original sentence (used for context if needed).\n        tokens: List of words (tokens) in the sentence.\n        index_of_word: Index of the target word in the tokens list. Zero-indexed\n\n    Output:\n        A tuple containing:\n            - A NumPy array of digrams (other words paired with the target word).\n            - A 1D NumPy array where each element is 1 if the other word appears before the target word, 0 otherwise.\n    \"\"\"\n    if index_of_word&lt;0 or index_of_word&gt;=len(tokens):\n        raise ValueError(\"Invalid user-defined index. Must be within the range of the tokens list.\")\n\n    #extract the target word. This of course assumes that the tokenization is sequential, but for illustrative purposes, it is fine\n    target_word = tokens[index_of_word]\n\n    #generate digrams and the one-hot vector. The idea is that if the word appears before our word then set the index to 1, else 0\n    digrams=[f\"{token},{target_word}\" for idx,token in enumerate(tokens) if idx!=index_of_word]\n    one_hot_vector=np.array([1 if idx&lt;index_of_word else 0 for idx in range(len(tokens)) if idx!=index_of_word],dtype=int)\n\n    return np.array(digrams), one_hot_vector\n\nprint(\"Our sentence is:\", english_sentence)\n\n#lets take index 2 ('likes' in \"My rabbit likes bananas\")\nindex_of_word=2\nincomplete_sentence=\" \".join([word for index,word in enumerate(english_tokenized_sentence) if index&lt;index_of_word+1])\nprint(\"We want an incomplete sentence. Our generation task is to predict the next word in:\", incomplete_sentence)\n#generate the digrams (word pairs)\ndigrams,digram_onehot_vector=digram_one_hot_encoding(english_sentence,english_tokenized_sentence,index_of_word)\nprint(\"Digrams:\")\nprint(digrams)\nprint(\"One-hot vector (without any future dependency):\")\nprint(digram_onehot_vector)\n\n#next, we create the attention mask by hand. specifically, we generate a ones vector equal to the size of the number of words in our sentence\n#then we randomly pick 2\nattention_mask=np.ones((len(whitespace_tokenizer(incomplete_sentence))), dtype=int)\nzero_indices = np.random.choice(len(whitespace_tokenizer(incomplete_sentence)), size=random.randrange(0,len(whitespace_tokenizer(incomplete_sentence))), replace=False)\nattention_mask[zero_indices]=0\nprint(\"Example attention mask: \", attention_mask.T)\n\nprint(\"Attention applied to the non-future dependency capturing one-hot vector:\", attention_mask*digram_onehot_vector)\nAs an example, a second-order probability matrix with skip dependencies given to us by the talking dog can be this:\n[[0.03888508 0.03133871 0.88147493 0.04830127]\n [0.89819378 0.03108927 0.03605569 0.03466125]\n [0.02927141 0.88815681 0.02795252 0.05461926]\n [0.01763851 0.04167578 0.05967272 0.881013  ]]\nOur sentence is: My rabbit likes bananas\nWe want an incomplete sentence. Our generation task is to predict the next word in: My rabbit likes\nDigrams:\n['My,likes' 'rabbit,likes' 'bananas,likes']\nOne-hot vector (without any future dependency):\n[1 1 0]\nExample attention mask:  [0 0 1]\nAttention applied to the non-future dependency capturing one-hot vector: [0 0 0]\n\n\n\nWe have so far our non-future dependent feature vector. We have used it so far in conjunction with the probability matrix to predict the next step. If we want to suppress the feature vector with attention, does it make sense to use another matrix in the same way? Let’s assume that we have a bunch of attention masks/vectors. We can stack them either vertically or horizontally (depending on how exactly we want to implement our lookup) and generate a matrix of attention masks. We can then send our feature vector through the attention matrix and then send the result of that product into our probability matrix to predict the next word.\ndef send_vector_through_two_matrices(vector: np.ndarray, probability_matrix: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Function to send an input vector through an attention matrix and then a probability matrix\n    Inputs:\n        vector: a 1D NumPy ndarray\n    Output:\n        a 1D NumPy ndarray of the same length as the input after being sent through two matrices\n    \"\"\"\n\n    #the idea is to generate a 1d array of ones, replace half of the elements with 0, and shuffle and reshape it\n\n    print(\"Your input vector is:\\n\", vector)\n\n    total_elements=vector.shape[0]**2\n    half=total_elements//2\n    flat=np.ones(total_elements, dtype=int)\n    flat[:half]=0\n    np.random.shuffle(flat)\n    example_attention_matrix=flat.reshape((vector.shape[0],vector.shape[0]))\n\n    print(\"An example attention matrix is:\\n\", example_attention_matrix)\n\n    result_1=np.matmul(vector, example_attention_matrix)\n    print(\"After multiplying, you get:\\n\",result_1)\n\n    print(\"After multiplying the result with the probability matrix, you get\", np.matmul(result_1, probability_matrix))\n\nexample_probability_matrix=np.random.rand(len(digram_onehot_vector), len(digram_onehot_vector))\nprint(\"An example probability matrix is:\\n\", example_probability_matrix)\n\nsend_vector_through_two_matrices(digram_onehot_vector, example_probability_matrix)\nAn example probability matrix is:\n [[0.6600921  0.31488209 0.01236613]\n [0.51302175 0.04638934 0.92027961]\n [0.38042472 0.01756476 0.29279993]]\nYour input vector is:\n [1 1 0]\nAn example attention matrix is:\n [[1 1 0]\n [1 0 1]\n [0 0 1]]\nAfter multiplying, you get:\n [2 1 1]\nAfter multiplying the result with the probability matrix, you get [2.21363067 0.69371827 1.2378118 ]\n\n\n\nWhat do we do with the result of the attention step? Sure, we have a vector that has encoded word pairs (a second-order model), but we don’t yet have a way to deconstruct that vector back into a word pair. How do we do this? So far, matrix multiplication has enabled us to encode sentences into vectors and selectively mask the irrelevant word pairs. Can we apply matrix multiplication to decode a word pair? The answer is yes. Matrix multiplications are in fact what neural networks do.\n\n\nNeural networks are a deep learning architecture based on the neuron-synapse structure of the human brain. Neural networks consist of a series of blocks called artificial neurons (hereafter just ‘neuron’) stacked vertically in layers. Each neuron has the possibility to receive an input and pass along an output to another neuron. To decide whether it passes along an output, a neuron sums up all of its inputs (which are weighted by the value of the connection along which the output travels) and applies a function, called an activation function, to that sum. Depending on the result of the activation function, the neuron sends an output to one or more neurons depending on how many it connects to. Mathematically, passing data through a neuron is equivalent to applying the mathematical function f(∑i=0i=nwixi)f(\\sum_{i=0}^{i=n} w_i x_i), where ff is the activation function, wiw_i is the weight along an input path, and xix_i the actual value being sent along that input path.\nNeural networks are equivalent to matrix multiplication. Why is this so? Suppose there are two layers in our neural network. The first layer has 3 neurons, and the second layer has two neurons. Let’s name the latter two n1n_1 and n2n_2. Also, let’s assume that each neuron in the first layer sends is connected to each neuron in the second layer. Therefore, the outputs of the first layer are x1,x2,x3x_1,x_2,x_3 and the weights of the paths along which they are sent are w11,w12,w21,w22,w31,w32w_{11}, w_{12}, w_{21}, w_{22}, w_{31}, w_{32} for each neuron-neuron path.\nThe input to n1n_1 is w11x1+w21x2+w31x3w_{11}x_1+w_{21}x_2+w_{31}x_3, and the input to n2n_2 is w12x1+w22x1+w32x3w_{12}x_1+w_{22}x_1+w_{32}x_3. Writing these out in the form of a system of linear expressions:\n$$w_{11}x_1+w_{21}x_2+w_{31}x_3\\\\w_{12}x_1+w_{22}x_1+w_{32}x_3$$\nit is easy to see that this is in fact a matrix multiplication:WXWX, where W=(w11w21w31w21w22w32),X=(x1x2x3)W=\\begin{pmatrix}w_{11}&w_{21}&w_{31}\\\\w_{21}&w_{22}&w_{32}\\end{pmatrix}, X=\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix}. Each layer also tends to have a bias term, so the input to a layer can be represented as the equation WX+BWX+B, where BB is the bias matrix (usually a column vector containing the same value). The activation function is applied to this resultant vector. This means that the output of a layer of a neural network can be represented by a vector.\n#### Properties of neural networks\n\nNeural networks are universal function approximators. This means that given a large-enough network with nonlinear activation functions, neural networks can model any mapping between elements of a domain XX and a codomain YY. However, this does NOT tell us how many neurons and layers we need or what the activation function is.\nNeural networks can model nonlinear relationships between elements. While the discussion of linear decision boundaries is beyond the scope of this tutorial, it is enough to know that ff, the activation function, is usually chosen to be something like ReLU or the sigmoid function.\nSince neural networks are just matrix multiplication, they are extremely fast to train on computers16\n\n#### Activation functions\nIf the activation function is linear (such as a simple multiplier f(x)=2xf(x)=2x), the neural network cannot learn linear relationships, no matter how big you make it and how long you train it for. Nonlinear activation functions are necessary to learn nonlinear relationships i.e. relationships between two variables that cannot be explained by a matrix multiplication (attention is linear!). Activation functions like ReLU and the sigmoid function are chosen not only because they are easy to compute, but because of a certain requirement explained below.\n\n\n\nSince each layer of a neural network can be expressed as a function ff, a neural network can be thought of as a large composite function f1(W1f2(...fn(WnXn+BN)...+B1)+B0)f_1(W_1f_2(...f_n(W_nX_n+B_N)...+B_1)+B_0). Given a training set of (xi,yi){(x_i,y_i)} pairs, the loss of the model is a cost function C(yi,g(xi))C(y_i,g(x_i)) where g(xi)g(x_i) is the prediction of the neural network (the large composite function defined above) for the input variable xix_i. We want to minimize this cost function, as it means our neural network has learned the relationship between the input and output variables. This is done with an algorithm called backpropagation.\nBackpropagation is an algorithm that utilizes the technique of gradient descent - given a cost function, we calculate its gradient with respect to the weights and biases of the neural network. According to the learning rate aa, gradient descent updates the weights and biases of the neural network according to the rule W/Bnext=W/Bcurrent−α∂C(X,W/Bcurrent)∂W/B\nW/B_{\\text{next}}=W/B_{\\text{current}}-\\alpha\\frac{\\partial C(X,W/B_{\\text{current}})}{\\partial W/B}\n\nwhere // is read as ‘OR’. We subtract the gradient because the gradient denotes the direction of maximum increase, so the direction of maximum decrease would be the direction opposite to it. We apply this many times (this is therefore a greedy algorithm) to find the local minimum of the function i.e. the values of all WW and all BB where the cost function is minimized. After each step of updating the gradients, we have to compute the prediction of the network again, in order to prepare for the next step. This is called the forward pass or forward step through the network, and must be computed repeatedly, making the process a back-and-forth.\n\n\n\nLet us consider a neural network set up in the following way. We have 3 input neurons (that is, 3 input variables) and 1 hidden layer with 4 neurons, and one output neuron. For the sake of this example, assume that every neuron in one layer is connected to every neuron in the next layer and every neuron in the previous layer. Such a neural network is called a fully-connected neural network.\nWe have already seen how matrices can represent the input to a layer. Let’s represent the output of a layer by a vector after an activation function is applied to each neuron17. We will now define several variables that mathematically represent each layer. For each layer ll, we have:\n$$ n_l \\\nw_l n_{l} n{l-1} \\\nb_l n_l \\\na_l \\\nz_l a_l b_l \\\ng_l $$\nWe can write down some straightforward formluae after these definitions.\n$$ z_l=w_la_l+b_l\\\na_l=g_l(z_l)\\ $$\nThe next question is choosing an appropriate cost function for our task. Let us think about our task for a moment. Since we have been using probabilities all along to predict the next word in our sequence, it is appropriate to use a cost function that tells us how good our probability prediction is. The classical cost function that is used to explain backpropagation is the squared error function, (real value−predicted value)2(\\text{real value}-\\text{predicted value})^2. This function is natural because it is simply the difference between what we predict and what the truth is, and it is squared for many reasons such as being the variance of the unbiased estimator (if used in its mean-square form) and also being easily differentiable18. But we effectively want to measure the difference between a predicted probability distribution and the real probability distribution, as we are predicting the next word in a sentence. This requires having a maximum likelihood estimate of the parameters, and when working with Bernoulli-distributed variables (such as one-hot encoded vectors) the cross-entropy loss function −(ylnŷ+(1−y)ln(1−ŷ))-(yln\\hat{y}+(1-y)ln(1-\\hat{y})) minimizes the maximum likelihood estimate19.\nLet’s see how the derivative is calculated.\n$$ = \\[5pt]\n= $$\nby a simple application of the chain rule. This can easily be extended by observing that z3z_3 is a function of a2a_2, which is a function of z2z_2, and z2z_2 is a function of w2,b2,a1w_2, b_2, a_1.\n$$ = \\[5pt]\n= $$\nand similarly for the first (input) layer, named layer 0.\nThe next task is setting up a way to recursively calculate the derivative of the cost function for any arbitrary layer’s weight and bias. The general equation for this is\n$$\n\\frac{\\partial C}{\\partial w_l}=\\frac{\\partial C}{\\partial z_l}\\frac{\\partial z_l}{\\partial w_l}\\\\[5pt]\n\\frac{\\partial C}{\\partial b_l}=\\frac{\\partial C}{\\partial z_l}\\frac{\\partial z_l}{\\partial b_l}\n$$\nThere are two observations we can make from this. The first is that it is straightfoward to numerically calculate the partial derivative for the last/output layer, and we can store this value in order to avoid repeated computation wherever possible. The second is that you need to calculate the change in the gradient for the last layer, then use that changed gradient for the layer before that one, and so on, ‘back-propagating’ the errors.\nLet’s choose a nice activation function such as the sigmoid function σ(x)=11+e−x\\sigma(x)=\\frac{1}{1+e^{-x}} for this. Let us precompute the partial derivative of the output layer, since we’ll be needing it. Note that ⊙\\odot denotes the Hadamard, or element-wise product.\n$$ == ’(z_o) \\[5pt]\n=-(-) \\[5pt]\n’(z_o)=a_l(1-a_l) \\[5pt]\ny \\[5pt]\n=a_o-y $$\nThis is a very nice result. It is now easy to see that for any inner layer, we can repeatedly apply the chain rule to derive the partial derivatives. If you go about doing this you end up with the following results:\n$$ =w_{l+1}^T ’(z_l) \\[5pt]\n=a_{l-1} \\[5pt]\na_{l-1}^T \\[5pt]\n=1 \\[5pt]\n= $$\nA more complete derivation can be found here20, but the fundamental idea is the same.\nWe can now implement this in Python and train the neural network from scratch.\n\"\"\"\nNeural Network Implementation in NumPy\nInputs:\n    None\nOutputs:\n    Fully functional neural network trained on synthetic data\n\"\"\"\n\nimport numpy as np\n\ndef sigmoid(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Apply the sigmoid activation function element-wise\n    Inputs:\n        x: a NumPy ndarray, the input array\n    Outputs:\n        a NumPy ndarray with sigmoid applied element-wise\n    \"\"\"\n    return 1/(1+np.exp(-x))#sigmoid formula\n\ndef sigmoid_prime(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute the derivative of the sigmoid function element-wise\n    Inputs:\n        x: a NumPy ndarray, the input array\n    Outputs:\n        a NumPy ndarray with the derivative of sigmoid applied element-wise\n    \"\"\"\n    return sigmoid(x)*(1.0-sigmoid(x))#sigmoid derivative formula\n\nclass NeuralNetwork:\n    \"\"\"\n    Define a simple feedforward neural network\n    \"\"\"\n\n    def __init__(self,architecture: np.ndarray):\n        \"\"\"\n        initializer for the neural network class\n        Inputs:\n            architecture: a NumPy array representing the number of neurons in each layer\n        \"\"\"\n        self.L=architecture.size-1#number of layers (excluding input layer)\n        self.n=architecture#number of neurons in each layer\n        self.parameters={}#dictionary to store weights, biases, and activations\n\n        #initialize weights and biases for each layer\n        for i in range(1,self.L+1):\n            self.parameters['W'+str(i)]=np.random.randn(self.n[i],self.n[i-1])*0.01#small random weights\n            self.parameters['b'+str(i)]=np.ones((self.n[i],1))#biases initialized to 1\n            self.parameters['z'+str(i)]=np.ones((self.n[i],1))#pre-activation values initialized to 1\n            self.parameters['a'+str(i)]=np.ones((self.n[i],1))#activations initialized to 1\n        \n        self.parameters['a0']=np.ones((self.n[0],1))#input layer activation\n        self.parameters['C']=1#placeholder for cost value\n        self.derivatives={}#dictionary to store derivatives\n\n    def forward_propagate(self,X: np.ndarray):\n        \"\"\"\n        Perform forward propagation\n        Inputs:\n            X: a column vector representing one training example\n        Outputs:\n            None\n        \"\"\"\n        self.parameters['a0']=X#set input layer activation\n        for l in range(1,self.L+1):\n            self.parameters['z'+str(l)]=np.dot(self.parameters['W'+str(l)],self.parameters['a'+str(l-1)])+self.parameters['b'+str(l)]#W*a+b\n            self.parameters['a'+str(l)]=sigmoid(self.parameters['z'+str(l)])#apply sigmoid activation\n\n    def compute_cost(self,y: np.ndarray):\n        \"\"\"\n        function to compute the cost for one training example\n        Inputs:\n            y: the true label for the input sample\n        Outputs:\n            None\n        \"\"\"\n        self.parameters['C']=-(y*np.log(self.parameters['a'+str(self.L)])+(1-y)*np.log(1-self.parameters['a'+str(self.L)]))#binary cross-entropy loss\n\n    def compute_derivatives(self,y: np.ndarray):\n        \"\"\"\n        function to compute gradients for all parameters\n        Inputs:\n            y: the true label for the input sample\n        Outputs:\n            None\n        \"\"\"\n        self.derivatives['dz'+str(self.L)]=self.parameters['a'+str(self.L)]-y#last layer gradient\n        self.derivatives['dW'+str(self.L)]=np.dot(self.derivatives['dz'+str(self.L)],self.parameters['a'+str(self.L-1)].T)#last layer weights gradient\n        self.derivatives['db'+str(self.L)]=self.derivatives['dz'+str(self.L)]#last layer bias gradient\n\n        for l in range(self.L-1,0,-1):\n            self.derivatives['dz'+str(l)]=np.dot(self.parameters['W'+str(l+1)].T,self.derivatives['dz'+str(l+1)])*sigmoid_prime(self.parameters['z'+str(l)])#hidden layer gradient\n            self.derivatives['dW'+str(l)]=np.dot(self.derivatives['dz'+str(l)],self.parameters['a'+str(l-1)].T)#hidden layer weights gradient\n            self.derivatives['db'+str(l)]=self.derivatives['dz'+str(l)]#hidden layer bias gradient\n\n    def update_parameters(self,alpha: float):\n        \"\"\"\n        function to update network parameters using gradient descent\n        Inputs:\n            alpha: learning rate\n        Outputs:\n            None\n        \"\"\"\n        for l in range(1,self.L+1):\n            self.parameters['W'+str(l)]-=alpha*self.derivatives['dW'+str(l)]#update weights\n            self.parameters['b'+str(l)]-=alpha*self.derivatives['db'+str(l)]#update biases\n\n    def predict(self,x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        function to predict the output for a given input\n        Inputs:\n            x: a column vector representing one input sample\n        Outputs:\n            a NumPy array representing the predicted output\n        \"\"\"\n        self.forward_propagate(x)#perform forward propagation\n        return self.parameters['a'+str(self.L)]#return output layer activation\n\n    def fit(self,X: np.ndarray,Y: np.ndarray,num_iter: int,alpha: float=0.01):\n        \"\"\"\n        function to train the neural network\n        Inputs:\n            X: a NumPy array where each row is a training example\n            Y: a NumPy array of true labels\n            num_iter: number of iterations\n            alpha: learning rate\n        Outputs:\n            None\n        \"\"\"\n        for iter in range(num_iter):\n            c=0#cumulative cost\n            n_c=0#correct predictions count\n\n            for i in range(X.shape[0]):\n                x=X[i].reshape((X[i].size,1))#reshape input to column vector\n                y=Y[i]#true label\n                self.forward_propagate(x)#forward propagation\n                self.compute_cost(y)#compute cost\n                self.compute_derivatives(y)#compute gradients\n                self.update_parameters(alpha)#update parameters\n                c+=self.parameters['C']#accumulate cost\n                y_pred=self.predict(x)#make prediction\n                y_pred=(y_pred&gt;0.5)#convert probability to binary\n                if y_pred==y:\n                    n_c+=1#correct prediction count\n\n            c=c/X.shape[0]#average cost\n            print('Iteration:',iter)\n            print(\"Cost:\",c)\n            print(\"Accuracy:\",(n_c/X.shape[0])*100)\n\n#generate synthetic data\nnp.random.seed(42)#reproducibility\nX=np.random.rand(200,7)#200 samples, 7 features\ny=(np.sum(X,axis=1)&gt;3.5).astype(int).reshape(200,1)#labels based on sum of features\n\n#split data into training and testing sets\nsplit_ratio=0.7#70% training data\nsplit_index=int(X.shape[0]*split_ratio)\nindices=np.arange(X.shape[0])\nnp.random.shuffle(indices)\n\nX_train,X_test=X[indices[:split_index]],X[indices[split_index:]]\ny_train,y_test=y[indices[:split_index]],y[indices[split_index:]]\n\n#define architecture\narchitecture=np.array([7,5,1])#7 input features, 5 hidden neurons, 1 output\n\n#initialize and train the neural network\nnn=NeuralNetwork(architecture)\nnn.fit(X_train,y_train,num_iter=50,alpha=0.1)\n\n#evaluate the model\ncorrect_predictions=0\nfor i in range(X_test.shape[0]):\n    x=X_test[i].reshape((X_test[i].size,1))#reshape to column vector\n    y_true=y_test[i]#true label\n    y_pred=nn.predict(x)#prediction\n    y_pred=(y_pred&gt;0.5).astype(int)#convert to binary\n    if y_pred==y_true:\n        correct_predictions+=1#count correct predictions\n\n#calculate test accuracy\ntest_accuracy=(correct_predictions/X_test.shape[0])*100\nprint(\"Test Accuracy:\",test_accuracy)\nIteration: 0\nCost: [[0.72566273]]\nAccuracy: 64.28571428571429\nIteration: 1\nCost: [[0.71523798]]\nAccuracy: 66.42857142857143\nIteration: 2\nCost: [[0.70509586]]\nAccuracy: 66.42857142857143\nIteration: 3\nCost: [[0.6864599]]\nAccuracy: 67.85714285714286\nIteration: 4\nCost: [[0.65347682]]\nAccuracy: 75.71428571428571\nIteration: 5\nCost: [[0.60664836]]\nAccuracy: 82.14285714285714\nIteration: 6\nCost: [[0.55558826]]\nAccuracy: 91.42857142857143\nIteration: 7\nCost: [[0.5072171]]\nAccuracy: 93.57142857142857\nIteration: 8\nCost: [[0.46296521]]\nAccuracy: 95.0\nIteration: 9\nCost: [[0.42292781]]\nAccuracy: 96.42857142857143\nIteration: 10\nCost: [[0.38716204]]\nAccuracy: 97.85714285714285\nIteration: 11\nCost: [[0.35557885]]\nAccuracy: 97.85714285714285\nIteration: 12\nCost: [[0.32788988]]\nAccuracy: 97.85714285714285\nIteration: 13\nCost: [[0.30368962]]\nAccuracy: 97.85714285714285\nIteration: 14\nCost: [[0.28254309]]\nAccuracy: 97.85714285714285\nIteration: 15\nCost: [[0.26403564]]\nAccuracy: 97.85714285714285\nIteration: 16\nCost: [[0.24779326]]\nAccuracy: 97.85714285714285\nIteration: 17\nCost: [[0.23348834]]\nAccuracy: 98.57142857142858\nIteration: 18\nCost: [[0.22083889]]\nAccuracy: 98.57142857142858\nIteration: 19\nCost: [[0.20960497]]\nAccuracy: 98.57142857142858\nIteration: 20\nCost: [[0.19958386]]\nAccuracy: 98.57142857142858\nIteration: 21\nCost: [[0.190605]]\nAccuracy: 98.57142857142858\nIteration: 22\nCost: [[0.18252507]]\nAccuracy: 98.57142857142858\nIteration: 23\nCost: [[0.17522359]]\nAccuracy: 98.57142857142858\nIteration: 24\nCost: [[0.16859908]]\nAccuracy: 98.57142857142858\nIteration: 25\nCost: [[0.16256584]]\nAccuracy: 98.57142857142858\nIteration: 26\nCost: [[0.15705123]]\nAccuracy: 98.57142857142858\nIteration: 27\nCost: [[0.15199344]]\nAccuracy: 98.57142857142858\nIteration: 28\nCost: [[0.14733967]]\nAccuracy: 98.57142857142858\nIteration: 29\nCost: [[0.14304459]]\nAccuracy: 98.57142857142858\nIteration: 30\nCost: [[0.13906911]]\nAccuracy: 98.57142857142858\nIteration: 31\nCost: [[0.13537942]]\nAccuracy: 98.57142857142858\nIteration: 32\nCost: [[0.1319461]]\nAccuracy: 98.57142857142858\nIteration: 33\nCost: [[0.12874349]]\nAccuracy: 98.57142857142858\nIteration: 34\nCost: [[0.12574908]]\nAccuracy: 98.57142857142858\nIteration: 35\nCost: [[0.12294312]]\nAccuracy: 99.28571428571429\nIteration: 36\nCost: [[0.12030816]]\nAccuracy: 99.28571428571429\nIteration: 37\nCost: [[0.11782877]]\nAccuracy: 99.28571428571429\nIteration: 38\nCost: [[0.11549126]]\nAccuracy: 99.28571428571429\nIteration: 39\nCost: [[0.11328345]]\nAccuracy: 100.0\nIteration: 40\nCost: [[0.11119447]]\nAccuracy: 100.0\nIteration: 41\nCost: [[0.1092146]]\nAccuracy: 100.0\nIteration: 42\nCost: [[0.10733513]]\nAccuracy: 100.0\nIteration: 43\nCost: [[0.10554823]]\nAccuracy: 100.0\nIteration: 44\nCost: [[0.10384686]]\nAccuracy: 100.0\nIteration: 45\nCost: [[0.10222465]]\nAccuracy: 100.0\nIteration: 46\nCost: [[0.10067586]]\nAccuracy: 100.0\nIteration: 47\nCost: [[0.09919528]]\nAccuracy: 100.0\nIteration: 48\nCost: [[0.09777818]]\nAccuracy: 100.0\nIteration: 49\nCost: [[0.09642027]]\nAccuracy: 100.0\nTest Accuracy: 93.33333333333333\nHere’s a neat observation. If a neural network is simple a two-layer network with non-linear activation, it is simply a matrix multiplication to new inputs. Therefore, we now have a way to learn that second-order probability matrix!\nGiven a neural network which can learn vector-vector relationships, it is easy to see that we can reconstruct our word-pair combinations from a vector that is the result of an attention step. Suppose we have a vector corresponding to the words ‘My’, ‘rabbit’, and ‘likes’, (111)\\begin{pmatrix}1\\\\1\\\\1 \\end{pmatrix}, the result of making it non-future-dependent is (110)\\begin{pmatrix}1\\\\1\\\\0 \\end{pmatrix}, and our attention step results in (100)\\begin{pmatrix}1\\\\0\\\\0 \\end{pmatrix}. We now pass this as input to a neural network and compute the output. The output will simply map our input vector to an output vector. The insight is that you can train the network to accurately map the result of our attention step to word pairs! The next obvious question is how to generate these word pairs. But before that, we need to notice that our neural network gets trained quickly when the input-output training sets have a small number of elements. It quickly becomes unwieldy when you think about practical languages, like the 260,000 Italian words mentioned above. This moves us on to our next topic, embeddings.\n\n\n\n\nTo make our neural network work well, we need a large amount of input-output data. This is impractical at the scale of even small, real languages - there are simply too many words. Generating one-hot encoding matrices by vertically stacking the vectors, even with techniques to store sparse matrices, is still impractical once we think about storing word pairs and triplets. We need some way to reduce these one-hot matrices in size so storing them becomes more efficient. This is the same problem we tried to solve with neural networks: converting an input vector into another vector. In our case, we want to convert a large vector to a smaller vector such that enough information is retained. This smaller vector will be called the embedding vector.\nBased on all that we’ve seen so far, it is obvious that this conversion will be done with matrix multiplication. The question is how to make this new matrix. We can do the same thing we did before (training a neural network) or we can do something completely different. Here is an example. Suppose we want to embed ‘My’, ‘rabbit’, ‘likes’, and ‘bananas’ into 2 dimensions. We know that their one-hot encoding is an identity matrix, possibly with its columns shuffled. We can arbitrarily define a 4×24 \\times 2 matrix that will project this matrix into a smaller matrix. We can then say that column 1 (representing, say, ‘rabbit’) of the initial matrix is now replaced by column 1 of the new matrix. This is perfectly fine. But is this meaningful?\nWhat do we want from a ‘good’ embedding? Broadly, a good embedding should be useful for practical tasks. There is no use in embedding words and making the transformer’s neural network harder to train. We might want to apply clustering algorithms to word embeddings to find out, for example, how many nouns there are in a large corpus (plural corpora) of text. We might also want to know what words are related in an unknown language. For someone trying to embed English, making sure that the embeddings for ‘rabbit’ and ‘hare’ are closer(i.e. their difference is closer to 0→\\vec{0}) than the embeddings for ‘rabbit’ and ‘desk’ is important if training a model to explain what it says in pictures of lagomorphs in office environments. Embeddings should probably also capture context awareness - ‘hot dog’ must have a different embedding compared to both ‘hot’ and ‘dog’. They should also not be too low-dimensional; we might lose important information.\nIt is beyond the scope of this tutorial to discuss good embedding algorithms. Fortunately, there is a straightforward algorithm we can use to embed our four-word language. Let’s map them on the unit circle with a randomly generated matrix, as we have been doing so far.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#define words\nwords=['My','rabbit','likes','bananas']\n\n#ensure that the words are sufficiently apart for better visibility-say 30 degrees\nangles=np.sort(np.random.choice(np.linspace(0,2*np.pi,360,endpoint=False),len(words),replace=False))\n\n#compute unit circle coordinates\nunit_circle_vectors=np.array([[np.cos(angle),np.sin(angle)] for angle in angles])\n\n#define initial one-hot vectors\none_hot_vectors=np.eye(len(words), dtype=int)\n\n#plot embeddings\nplt.figure(figsize=(6,6))\nfor i,word in enumerate(words):\n    x,y=unit_circle_vectors[i]\n    plt.scatter(x,y,label=word)\n    plt.text(x+0.05,y+0.05,word,fontsize=12)\n    #draw the arrow\n    plt.arrow(0,0,x,y,head_width=0.05,head_length=0.1,fc='blue',ec='red',alpha=0.7)\n\n# Draw the unit circle\ntheta=np.linspace(0,2*np.pi,100)\ncircle_x=np.cos(theta)\ncircle_y=np.sin(theta)\nplt.plot(circle_x,circle_y,color='gray',linestyle='--')\n\nplt.title(\"2D Embedding of a 4-Word Language on Unit Circle\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.axhline(0,color='gray',linewidth=0.5)\nplt.axvline(0,color='gray',linewidth=0.5)\nplt.grid(True)\nplt.legend()\nplt.axis('equal')\nplt.show()\n\n# Print initial one-hot vectors and unit circle embeddings\nprint(\"Initial One-Hot Vectors:\")\nfor i,word in enumerate(words):\n    print(f\"{word}:{one_hot_vectors[i]}\")\n\nprint(\"\\nEmbedded Vectors on Unit Circle:\")\nfor i,word in enumerate(words):\n    print(f\"{word}:{unit_circle_vectors[i]}\")\n\n\n\npng\n\n\nInitial One-Hot Vectors:\nMy:[1 0 0 0]\nrabbit:[0 1 0 0]\nlikes:[0 0 1 0]\nbananas:[0 0 0 1]\n\nEmbedded Vectors on Unit Circle:\nMy:[0.9961947  0.08715574]\nrabbit:[-0.54463904 -0.83867057]\nlikes:[ 0.60181502 -0.79863551]\nbananas:[ 0.99254615 -0.12186934]\n\n\nLet’s think a bit about our two-word non-future-dependent vectors. The only condition we have applied so far is that the value in the column matrix row wherever a word appears ahead of our current word should be 0. If a word appeared 1364 places before our current word but its value was deemed ‘important’ by attention, its corresponding row value would be 1. This is impractical. We know that it is unlikely that a word appearing 1364 places before the current word affects it. How can we quantify this?\nThe solution is to do it heuristically. Let’s figure out what exactly the task is. We have to add some additional information in a word’s embedding that denotes the position of the word in a sentence. This additional information is called the positional encoding. We want to satisfy a few criteria:\n\nThe encoding must be unique for each word in the sequence even if that word appears again. The sentence ‘My rabbit likes bananas but my friend’s rabbit doesn’t.’ should have different encoding values for the first and second occurrences of ‘rabbit’.\nIf we have to add positional encoding to sentences of different lengths, the ‘distance’ between two pieces of information should remain constant. This means that ‘My rabbit likes bananas. My friend’s rabbit does not like bananas.’ should encode the first and second occurrences of ‘rabbit’ in a way that the difference between the additional information added should be the same as the difference between ‘likes’ and ‘does’. This ensures that the two sentences are recognized as part of a ‘speech’.\nWe should be able to generalize to any sentence length easily with bounded and deterministic values (i.e. do not train a neural network).\n\nEssentially, we need to find a function whose codomain is a vector of the same size of the embedding that is:\n\nEasy to compute\nPeriodic\nHas bounded values.\n\nand iterate through the sentence, computing the function at the index of every word. To add the information to the word embedding, we can literally add the two vectors. This encodes positional information in the embedding.\nA function that satisfies these criteria is f:ℕ→ℝdf: \\mathbb{N} \\rightarrow \\mathbb{R}^d\nfi(t)={sin(ωk⋅t),if i=2kcos(ωk⋅t),if i=2k+1\n\\\nf_i(t) = \n\\begin{cases} \n\\sin(\\omega_k \\cdot t), & \\text{if } i = 2k \\\\ \n\\cos(\\omega_k \\cdot t), & \\text{if } i = 2k + 1 \n\\end{cases}\n\\\n\nwhere dd is the number of rows in the column vector representation of the embedding vector, ii and kk are simply ways to denote even and odd positions (i.e. the first row of the encoding vector is a sine, the second row is a cosine, the third row is a sine, and so on) and w=1100002kdw=\\frac{1}{10000^{\\frac{2k}{d}}}. ww has been chosen completely by guesswork. tt simply denotes the row number. Another good property is that since the encoding are periodic functions, you have also put in some information saying ‘the encoding of a word mm places away from the current word is so-and-so’. Their periodicity implies that they can be represented as a linear combination of earlier encodings. I want to reiterate that this is a heuristic that works and theoretical justifications for this do not really exist. It works because you have differentiated between sentences such as ‘I just computed five times three plus two’ and ‘I just computed five plus three times two’ which have different underlying meanings.\n\n\n\nWe finally discuss actually choosing the next word in the sequence. Suppose that have taken a sentence, tokenized it, converted to one-hot encoding, embedded these encodings, added position embeddings, and then trained a neural network to predict an output vector. The final step is to convert this output vector, which is also an embedding, back into a vector that represents the target vocabulary. We do not want to convert it back into a one-hot vector. How will you choose the next word?\nLet’s do what is straightforward - multiply it with a matrix. This time, we are taking a smaller vector and making it a larger one. This comes with some caveats. We are making a column matrix bigger. If we want to make (123)\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix} bigger, say to twice its size, how can we do it? Should we do (102030)\\begin{pmatrix}1\\\\0\\\\2\\\\0\\\\3\\\\0\\end{pmatrix}, (012030)\\begin{pmatrix}0\\\\1\\\\2\\\\0\\\\3\\\\0\\end{pmatrix}, or (001023)\\begin{pmatrix}0\\\\0\\\\1\\\\0\\\\2\\\\3\\end{pmatrix}? We can set up a matrix to do any one of these transformations, but we cannot set up a matrix that does this for all possible input vectors. This is because you will be solving an overdetermined system of equations. We have no choice but to accept this, so we have to assume that even if we find a matrix that makes the values as close to zero as possible, they will never all be 0 for any practical case. Going back to our initial task (English to Italian), we will end up with a vector that looks something like this\n$\n\nDiosmiomangiatounaand so on...[0.10.000050.250...]\\begin{aligned}\n&\\text{Dios}\\\\\n&\\text{mio}\\\\\n&\\text{mangiato}\\\\\n&\\text{una}\\\\\n&\\text{and so on...}\n\\end{aligned}\n\\quad\n\\begin{bmatrix}\n0.1 \\\\ 0.00005 \\\\ 0.25 \\\\ 0 \\\\ \\text{...}\n\\end{bmatrix}\n\n$\nHow do we select the next word? We can certainly pick the one with the largest value, but this is not so good. Fortunately, we have already looked at a way to emphasize the right word - softmaxing! Softmaxing and picking the highest probability allows us to enhance the probability of the right word (and it will be high because we train the neural network in this way - remember that matrix multiplications are just two-layer neural networks) being picked. An added bonus is that the softmax function is differentiable.\nimport numpy as np\n\n#define words\nwords=['My','rabbit','likes','bananas']  #ensure vocabulary is 4 words long\n\nprint(\"Tokenized sentence:\", words)\n\n#ensure that the words are sufficiently apart for better visibility-say 30 degrees\nangles=np.sort(np.random.choice(np.linspace(0,2*np.pi,360,endpoint=False),len(words),replace=False))\n\n#compute unit circle coordinates (2D embeddings)\nembedding_dim=2  #set embedding dimension to 2\nunit_circle_vectors=np.array([[np.cos(angle),np.sin(angle)] for angle in angles])\n\n#define initial one-hot vectors\none_hot_vectors=np.eye(len(words),dtype=int)\n\n#function for positional encoding as defined in \"Attention is all you need\"\ndef positional_encoding(seq_len,d_model):\n    #initialize positional encoding matrix\n    pos_enc=np.zeros((seq_len,d_model))\n    for pos in range(seq_len):\n        for i in range(0,d_model,2):\n            pos_enc[pos,i]=np.sin(pos/(10000**(2*i/d_model)))\n            if i+1&lt;d_model:  #check to prevent index out of range\n                pos_enc[pos,i+1]=np.cos(pos/(10000**(2*i/d_model)))\n    return pos_enc\n\n#calculate positional encodings for the sentence\nseq_len=len(words)\npositional_encodings=positional_encoding(seq_len,embedding_dim)\n\n#calculate the sum of position encoding and embedding vectors\ncombined_vectors=unit_circle_vectors+positional_encodings\n\n#decoder matrix to map combined vectors back to one-hot-like representations\ndecoder_matrix=np.linalg.pinv(unit_circle_vectors)  #pseudo-inverse to decode\n\n#decode the combined vectors\ndecoded_vectors=np.dot(combined_vectors,decoder_matrix)\n\n#map decoded vectors to words by finding the closest match\ndef decode_to_words(decoded_vectors,word_embeddings,word_list):\n    result=[]\n    for vec in decoded_vectors:\n        #project vec back into the original embedding space\n        reconstructed_vec=np.dot(vec,word_embeddings)\n        #compute distances and find the closest match\n        distances=np.linalg.norm(word_embeddings-reconstructed_vec,axis=1)\n        closest_word_index=np.argmin(distances)\n        result.append(word_list[closest_word_index])\n    return result\n\ndecoded_words=decode_to_words(decoded_vectors,unit_circle_vectors,words)\n\n#print initial one-hot vectors\nprint(\"Initial One-Hot Vectors:\")\nfor i,word in enumerate(words):\n    print(f\"{word}:{one_hot_vectors[i]}\")\n\n#print embedded vectors on unit circle\nprint(\"\\nEmbedded Vectors on Unit Circle:\")\nfor i,word in enumerate(words):\n    print(f\"{word}:{unit_circle_vectors[i]}\")\n\n#print positional encodings\nprint(\"\\nPositional Encodings:\")\nfor i,word in enumerate(words):\n    print(f\"{word}:{positional_encodings[i]}\")\n\n#print combined vectors\nprint(\"\\nCombined Vectors (Embedding+Positional Encoding):\")\nfor i,word in enumerate(words):\n    print(f\"{word}:{combined_vectors[i]}\")\n\n#print decoded words\nprint(\"\\nDecoded tokenized sentence from Combined Vectors:\")\nprint(decoded_words)\n\nTokenized sentence: ['My', 'rabbit', 'likes', 'bananas']\nInitial One-Hot Vectors:\nMy:[1 0 0 0]\nrabbit:[0 1 0 0]\nlikes:[0 0 1 0]\nbananas:[0 0 0 1]\n\nEmbedded Vectors on Unit Circle:\nMy:[0.1391731  0.99026807]\nrabbit:[-0.5591929  -0.82903757]\nlikes:[-0.34202014 -0.93969262]\nbananas:[ 0.89100652 -0.4539905 ]\n\nPositional Encodings:\nMy:[0. 1.]\nrabbit:[0.84147098 0.54030231]\nlikes:[ 0.90929743 -0.41614684]\nbananas:[ 0.14112001 -0.9899925 ]\n\nCombined Vectors (Embedding+Positional Encoding):\nMy:[0.1391731  1.99026807]\nrabbit:[ 0.28227808 -0.28873527]\nlikes:[ 0.56727728 -1.35583946]\nbananas:[ 1.03212653 -1.443983  ]\n\nDecoded tokenized sentence from Combined Vectors:\n['My', 'bananas', 'bananas', 'bananas']\n\n\n\n\nWe have technically made an encoder-decoder model at this point. As you can see in the code above, even without attention, it is somewhat difficult to get consistent sequence reconstruction. Let’s think about attention again. So far, we have simply made an attention matrix and used it to suppress unimportant parts of the code. Our attention matrix was created by stacking one-hot encoded vectors on top of each other. This doesn’t make much sense. Some parts of a sentence may be important - maybe not as important as other parts - but important nonetheless. Multiplication with an attention matrix should result in a vector with parts that are suppressed but not necessarily zero. We can give importance to different parts by making our attention matrix elements fractions instead of ones and zeros.\nWe have so far not defined precisely what attention does. We have so far said that given a matrix of attention masks, we can pull out specific rows given our one-hot encoded non-future-dependent vectors, and suppress things even more. Now we focus on the big question. How do we generate this attention matrix? We’re in a completely different domain now - our words are no longer one-hot encoded, they’re embeddings, we’re adding positional information to them, and now we want to suppress irrelevant information.\nLet’s get an intuitive explanation of attention first. We can naturally ask whether the attention in machine learning is the same as attention in human beings. What does this mean? For example, our brain is flooded with many different sensory inputs every second. There’s the internal sensory information from the body (such as level of hunger, blood pressure, pain, our balance), and there’s external sensory information from the environment (such as me hearing the distant hum of cars outside the window while typing this, but choosing to ignore it). How do we not get overwhelmed? We ‘tune out’ (suppress) irrelevant information and only focus on the one that matters. Only a small subset of the sensory input data is considered relevant enough to be perceived - this is what we mean when we say we are paying attention to something.\nSimply put, we are assigning importance to items by filtering out the irrelevant ones. We also have a finite amount of attention. For example, watching a group of ants move around is significantly easier than tracking the path of more than a few ants in that group. You can either have a general idea of how the group is moving, or a specific idea of how some finite amount of ants in that group are moving, but not both.\nWe can specify this mathematically: Given a set of input items i1,i2,...ini_1,i_2,...i_n, we assign nonzero weights to them, w1,w2,...,wnw_1,w_2,...,w_n such that ∑k=0nwk=1\\sum_{k=0}^{n}w_k=1. Interpreting the weights as importances, they satisfy the two properties of human attention. We can than make a judgment about the items based on this attention. Since our inputs to the attention matrix are a collection of items (embedded word vectors), we can then assign a weight to them.\nWhere do these weights come from? This is where the learning in machine learning happens. We want to learn a function ff to compute these weights. This function is typically one that first computes some ‘relevance’ score for each word in the sequence and then softmaxes the weights in order to make the weights nonzero. We have already computed this relevance score. This is simply the sum of the embedding vector and positional encoding vector. What do we do now?\nSo far we have talked about second-order models. Throughout the tutorial, we have constructed our one-hot encoded vectors assuming second-order relationships in the past. We briefly said that we could extend the word-pair vector construction to word triplets, but as the discussion so far should show, this is very impractical. We now try to answer the question: How important is every word to every other word in the sentence, and it is possible to calculate this in one go? Positional encodings don’t really answer this question, as they show how ‘relevant’ each word is in the overall sentence, but not in relation to the actual other words. We have to figure out three main things:\n\nHow do you tell a word to ‘ask’ other words in the sentence the following question: ‘How important are you to me?’?\nIf this question is asked for every possible word pair, how do you calculate the response of every other word?\nIf you are able to answer questions 1 and 2, how do you actually construct the vector that will be fed into the neural network?\n\nLet explicitly define what this means. Each word has some sort of intrinsic value, which we have said is the sum of its embedding and positional encoding. For each word to ask each other word how important they are to it, we have to compare its intrinsic value to the other words’ values. But what values? In the compound sentence ‘I went out to buy fruit and my sister answered some emails.’, ‘fruit’ and ‘emails’ are the objects of the subjects ‘I’ and ‘my sister’ in the two coordinate clauses. Even though they are very important within their own clauses, they are more or less irrelevant for the other clause. If there was an earlier relation such as ‘my sister likes fruit’ much earlier (say 1000 words behind ‘buy fruit’) in the corpus, it would make sense to compare the intrinsic values of the embeddings. But since we practically do not want to look 1000 words behind for all the reasons outlined above, it makes sense to assign each word a response value i.e. if asked a question by another word, the word being asked will return a response value that may be different from its intrinsic value. Based on what the response value is, the initial word will decide what information value it ‘passes on’ to the neural network so that it can reconstruct this efficiently. Also, it does not make much sense for ‘I’ and ‘emails’ to be compared, so you also have to figure out how much ‘I’ will pay attention to the other words. This triplet of intrinsic value, response value, and information value are what defines the attention process.\nHow do we make a word get a vector response from every other word? Like we have been doing all along, we define an attention matrix that is not one-hot encoded this time around. The attention matrix prepares each word for asking questions. Multiplying a word vector by this attention matrix results in a vector that contains the information ‘how much attention should this word pay to other words?’. To make this process fast, you can stack each intrinsic value into a matrix and multiply it with the attention matrix. This results in a matrix (let’s call this matrix AA) that contains information about ‘how much should every word pay attention to every other word’? Note that this is not pair-specific; we are not saying that ‘I’ should pay lesser attention to ‘emails’ than to ‘went’. This is the behavior that is learned by the transformer.\nNext, we multiply AA with a matrix that contains a matrix of responses of the other words. Like before, we are not explicitly telling ‘I’ to provide a worse response if asked ‘How important are you to me?’ by ‘emails’, but we are learning this behavior. Let’s call the result of this matrix multiplication BB. You can now normalize the weights and softmax them in order for the matrix to have stable values. We then finally multiply this matrix by the matrix of information values to get CC the result of the attention step. This is the idea behind attention. BB contains information about how much weight each word should give to every other word, and CC contains information that makes our two-layer neural network make a judgment. These matrices are learned by the transformer by backpropagation.\n\n\nSince we are now no longer dealing with nonnegative zero-one matrices, the question of how to do this calculation efficiently for large corpora still remains. Unfortunately we cannot. We can only rely on specialized and accelerated hardware. We are also still not sure whether the attention process actually captures meanings in a way that is human-understandable. The solution is straightforward: instead of having one set of matrices to perform attention, have more than one set. Apply each matrix set’s attention independently, and for every newly returned information vector, combine them together in a specified way (usually, just concatenate, then use a neural network to predict outputs). The hope is that each set of matrices will learn something different about the text - sentence structure, word meanings, subject-object relationships. Each such set of matrices is called an attention head. To make this process somewhat computationally palatable, the matrices in multihead attention have smaller output dimensions (usually what the size would be for a single head for the whole corpus divided by the number of heads). In practice, this does work.\nThe final thing is dealing with practical languages. We still have to stack these attention blocks and two-layer neural networks many times in order to actually encode and decode things. This is the main reason it takes so long to train transformers on real text items. There is still of course the problem of getting correct datasets, but this is fine.\nimport numpy as np\n\n\ntokenized_sentence=['My','rabbit','likes','bananas']\n\n#combined_vectors is a np.ndarray of shape (4,2) that has been initialized in a different cell\n\nintrinsic_value_matrix=combined_vectors.T\n\nprint(\"The intrinsic value matrix is:\\n\", intrinsic_value_matrix)\n\nl=len(tokenized_sentence) #this is completely arbitrary - i want a machine learning model whose final dimension is 4\n\nattention_matrix=np.random.rand(l,l)\n\nprint(\"The initial attention matrix is:\\n\", attention_matrix)\n\nA=np.matmul(intrinsic_value_matrix, attention_matrix)\nprint(\"Preparing the words for attention, we get:\\n\", A)\n\nresponse_matrix=np.random.rand(l,l)\n\nprint(\"The initial response matrix is:\\n\", response_matrix)\n\nB=np.matmul(A,response_matrix.T)\n\nprint(\"The responses given by each word to each other word is:\\n\", B, \"\\n and after normalizing, we get:\\n\", B/np.sqrt(l))\nB=B/np.sqrt(l) #explicitly rewrite the matrix\n\nprint(\"We will softmax the attention matrix in order to boost closer words. The result of doing this is:\\n\", np.exp(B)/np.sum(np.exp(B),axis=1,keepdims=True))\nB=np.exp(B)/np.sum(np.exp(B),axis=1,keepdims=True) #store it again\n\ninformation_matrix=np.random.rand(l,l)\n\nprint(\"The information matrix is:\\n\", information_matrix)\n\nC=np.matmul(B, information_matrix)\n\nprint(\"The actual information passed on to the two-layer neural network for decoding is:\\n\", C)\nThe intrinsic value matrix is:\n [[ 0.1391731   0.28227808  0.56727728  1.03212653]\n [ 1.99026807 -0.28873527 -1.35583946 -1.443983  ]]\nThe initial attention matrix is:\n [[0.96775588 0.85687394 0.11764399 0.73973033]\n [0.31966683 0.98841599 0.48535898 0.1224339 ]\n [0.72518555 0.3230112  0.99707816 0.97978246]\n [0.65613402 0.12982645 0.686861   0.09303049]]\nPreparing the words for attention, we get:\n [[ 1.31351514  0.71549621  1.42792633  0.78933853]\n [-0.09688701  0.79459977 -2.24969069 -0.02585148]]\nThe initial response matrix is:\n [[0.02936416 0.82295845 0.20975295 0.37902713]\n [0.97345739 0.3368405  0.29648978 0.918208  ]\n [0.11074845 0.75958241 0.98806557 0.90129601]\n [0.65166792 0.31406079 0.25432198 0.8192551 ]]\nThe responses given by each word to each other word is:\n [[ 1.22608639  2.66780165  2.81126061  2.09050766]\n [ 0.16939992 -0.51740933 -1.65330782 -0.40691028]] \n and after normalizing, we get:\n [[ 0.61304319  1.33390082  1.4056303   1.04525383]\n [ 0.08469996 -0.25870467 -0.82665391 -0.20345514]]\nWe will softmax the attention matrix in order to boost closer words. The result of doing this is:\n [[0.14693005 0.30211697 0.32458379 0.22636919]\n [0.34953106 0.24794025 0.14050437 0.26202432]]\nThe information matrix is:\n [[0.87982191 0.52543491 0.24433994 0.11294321]\n [0.08717947 0.71907745 0.42011873 0.90535987]\n [0.7396713  0.19126042 0.61377742 0.43550545]\n [0.57724973 0.73806912 0.80619588 0.84373733]]\nThe actual information passed on to the two-layer neural network for decoding is:\n [[0.52636754 0.52360382 0.54454599 0.62247348]\n [0.5843209  0.58220905 0.48705008 0.54622243]]\nprint(\"This is a demonstration of multihead attention.\\n\")\n\n\nprint(\"Let's have two attention heads. The idea is to make our single-head attention model smaller.\")\n\nintrinsic_value_matrix=combined_vectors\n\nl2=int(l/2) #two heads\nmatrix_sets=[]\nl3=int(l2/2) #two heads, so the underlying projection dimesion will be smaller\nfor i in range(l2):\n    matrix_sets.append([np.random.rand(l2,l3),np.random.rand(l2,l3),np.random.rand(l2,l3)])\n\nprint(\"The multihead attention matrices are now:\")\nprint([i for i in matrix_sets])\n\n\npassed_on=[]\nprint(\"The attention process is now applied to the intrinsic value matrix for each head.\")\nfor i in range(l2):\n    A=np.matmul(intrinsic_value_matrix,matrix_sets[i][0])\n    B=np.matmul(A,matrix_sets[i][1].T)/np.sqrt(l2)\n    B=np.exp(B)/np.sum(np.exp(B),axis=1,keepdims=True)\n    C=np.matmul(B,matrix_sets[i][2])\n    print(\"For head\",i+1,\"the information passed on is:\\n\", C)\n    passed_on.append(C)\n\nprint(\"Therefore, the total information passed on is:\\n\",np.hstack((passed_on[0],passed_on[1])))\nThis is a demonstration of multihead attention.\n\nLet's have two attention heads. The idea is to make our single-head attention model smaller.\nThe multihead attention matrices are now:\n[[array([[0.25397487],\n       [0.0311156 ]]), array([[0.73929474],\n       [0.10634373]]), array([[0.31491959],\n       [0.00381713]])], [array([[0.68166948],\n       [0.16713201]]), array([[0.65528241],\n       [0.99174262]]), array([[0.07267087],\n       [0.07707801]])]]\nThe attention process is now applied to the intrinsic value matrix for each head.\nFor head 1 the information passed on is:\n [[0.16275392]\n [0.16155103]\n [0.16291437]\n [0.16692318]]\nFor head 2 the information passed on is:\n [[0.07498641]\n [0.07491223]\n [0.0749164 ]\n [0.07499548]]\nTherefore, the total information passed on is:\n [[0.16275392 0.07498641]\n [0.16155103 0.07491223]\n [0.16291437 0.0749164 ]\n [0.16692318 0.07499548]]\n\n\n\nWe have not exactly implemented future-proofing here. Masked attention is simply a term that makes BB an upper triangular matrix, as we can easily now see why it is a future proofed matrix.\n\n\n\nSometimes the result of the total information being passed on is small, so you can always add a skip connection to make it so that your vector is a result of embedding+position encoding AND attention. This is all empirical stuff that ‘just works’. Skip connections also serve the dual purpose of being good for gradient descent. One thing we have not spoken about is gradient descent in practice. It is easy to see that gradient descent may be bad in practice while sound in theory, when the error function’s plot in parameter space is just too hilly. Skip connections, things like layer normalization, and Xavier initialization for all parameters make ‘the gradients flow smoother’ in backpropagation. In practice, this means that the gradient calculation actually updates the values fairly frequently, instead of the gradient being calculated as 0 (because of a loss of numerical precision in computers - they have finite precision!).\nFor backpropagation through layer normalization, this is the mathematics: $$ x_{}=f_{} \\[5pt] f_{}=\\[5pt]\n= _{i=1}^N x_i \\[5pt]\n^2 = _{i=1}^N (x_i - )^2 \\[5pt]\n\\[5pt]\n d_{out}=\\[5pt]\n = {i=1}^N d{out_{i}} (x_i - ) (- (^2 + )^{-3/2})\\[5pt]\n = {i=1}^N d{out_{i}} (-) + _{i=1}^N (-2 (x_i - ) / N )\\[5pt]\n = + + $$\n\n\n\nSo far we have described attention as a standalone procedure. The idea is now to encode text with attention (for example, our neural network converts to a lower dimensional space) and pass the result of that into a decoder matrix. What does that mean? It means that we have converted text into an arbitrary dimension space after performing all of these procedures. But wait. These can simply be interpreted as the results of a separate instance of attention results! While generating text, we can simply pull those results in and feed them into the decoder’s decoding stage, providing the attention we need.\n\n\n\nNow we do our actual task. How do we actually choose the next word? The idea is to predict the next word and generate a probability distribution with softmaxing, and greedily picking the one with the highest probability. Then, take the text that has been generated so far along with the output and use it as an input to the decoder, apply attention, then use multihead to combine things with the encoded transformer. Transformers generate text one word at a time. The way you put the text in as input is by inserting a special word that translates to ‘SEQUENCE STARTS HERE’. This allows the decoder to train and learn to predict the next word.\n\n\n\n\nAfter all of this, we can finally implement a transformer in numpy. Note that when you backpropagate errors through the model, the updates are applied to all matrices as single neurons as well. Let us lay out what the overall task is:\n\nGive an input sentence, tokenize it, embed the words, add positional encoding. Keep a copy of the output.\nInitialize random attention, response, and value matrices, and run the matrix of inputs through the multi-head attention step.\nAdd the output copy and attention step results, and use that as input to a 2 layer neural network (whose neurons are randomly initialized) to convert the data into some weird representation that only the machine understands.\nPut this weird or latent representation into another neural network to decode it, then project it to the target space vocabulary size, then softmax. Select the word with the largest probability as the output and compare which word in the other vocabulary’s embeddings is the closest. Once the next word is found, use that as the input to the model (that is, put it through its own set of attention steps, but this time they’re masked) and use that to predict the next token. Once the entire sequence has been predicted, compute the cross-entropy loss and apply backpropagation, updating each matrix.\n\nimport numpy as np\nfrom typing import List, Tuple, Optional, Dict\n\n###############################################################################\n# Utilities\n###############################################################################\n\ndef whitespace_tokenizer(sentence: str) -&gt; List[str]:\n    \"\"\"\n        Splits a latin-character sentence into individual words assuming that each word is separated by a whitespace, and there's no punctuation\n    \n        Input:\n            sentence: a string \n        Output:\n            List of strings containing each individual words\n    \"\"\"\n\n    #we now start using python's inbuilt functions in order to make the code look more impressive\n    return sentence.strip().split()\n\ndef build_vocab(list_of_tokenized_sentences: List[List[str]]) -&gt; Tuple[dict, dict]:\n    \"\"\"\n        Builds a vocabulary of all words in the a given corpus of languages.\n        This means that if you have more than one sentence, it creates a list of words.\n        Input:\n            list_of_tokenized_sentences: a list of list of strings, AKA a whitespace tokenized sentence\n        Output:\n            A tuple of Python dictionaries, containing all words and their index. This index is random, because of the use of set()\n    \"\"\"\n    #the idea is to store the vocabulary in a Python set() object, which randomly stores elements for faster access (there's more to it but oh well)\n    vocabulary=set()\n    for tokenized_sentence in list_of_tokenized_sentences: \n        vocabulary.update(tokenized_sentence) #when you update a python set with a list, the elements of the list are added to the vocabulary\n    vocabulary=list(vocabulary) #convert the vocabulary set() into a list, so you can enumerate through it\n    word_index_pairs={w:i for i,w in enumerate(vocabulary)} #create a word:index pairing dictionary\n    index_word_pairs={i:w for i,w in enumerate(vocabulary)} #create an index:word pairing dictionary\n    return word_index_pairs,index_word_pairs\n\ndef pad_sequences(sequences: List[List[int]], max_length: Optional[int]=None, pad_value: int=0) -&gt; np.ndarray:\n    \"\"\"\n        Pads sequences. This is done because it is easier to process sequences with the same length, so we artifically add numbers to smaller ones.\n        Inputs:\n            sequences: a list of list of integers\n            max_length: Optional. Allows you to set the maximum length for a sequence instead of computing it dynamically.\n                     For example, in a corpus of sentences each under 20 words long, you can set the max length as 294\n            pad_value: What number you want to use to \n        Output:\n            A numpy.ndarray of a padded sequence\n\n    \"\"\"\n    #dynamically figure out the max length, as you have to pad to this\n    if max_length is None:\n        max_length=max(len(sequence) for sequence in sequences)\n    padded=[]\n    for sequence in sequences:\n        padded_sequence=sequence+[pad_value]*(max_length-len(sequence)) #simply append a list containing pad_value\n        padded.append(padded_sequence)\n    return np.array(padded, dtype=np.int32) #return an ndarray\n\ndef create_mask_for_removing_future_dependency(sequence: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n        Creates a non-future-dependent mask for the decoder, so that the decoder doesn't use the future for generation. This is also called autoregressive behavior.\n        Inputs:\n            sequence: An input ndarray whose shape is (batch_size, batch_size) - note that this assumes you are feeding it into the decoder at scale\n        Outputs:\n            An ndarray of the same shape as the input sequence which is upper triangular. The data type is Bool for faster processing. The upper triangular part has 'True'.\n    \"\"\"\n    sequence_length=sequence.shape[1]\n    autoregressive_mask=np.triu(np.ones((sequence_length,sequence_length)),k=1).astype(bool) #again, using the inbuilt functions\n    return autoregressive_mask\n\ndef one_hot(indices: np.ndarray, vocabulary_size: int) -&gt; np.ndarray:\n    \"\"\"\n        Converts input sequences into one-hot encoding in one go. \n        Since we are feeding sequences in one go to the encoder in batches, we need a 3D ndarray. \n        Inputs:\n            indices: An input ndarray whose shape is (batch_size, sequence_length), where each element has an integer index. This is why we created indices above\n            vocabulary_size: The number of words in the vocabulary of the language\n    \"\"\"\n    batched_tokens=np.zeros((indices.shape[0], indices.shape[1], vocabulary_size)) #create your zero ndarray, and populate it\n    #use a nested loop to set one-hot encodings\n    for batch_index in range(indices.shape[0]):\n        for token_index in range(indices.shape[1]):\n            batched_tokens[b,t,indices[batch_index,token_index]]=1\n    return batched_tokens\n\n###############################################################################\n# Normalization, activation, and loss\n###############################################################################\n\ndef layer_norm(x: np.ndarray, eps=1e-6) -&gt; Tuple[np.ndarray,np.ndarray,np.ndarray]:\n    \"\"\"\n        Normalizes the input ndarray. This is done to make backpropagation not run into numerical errors.\n        Inputs:\n            x: ndarray of inputs. usually just a vector\n            eps: this is done to prevent division by zero during normalization\n        Outputs:\n            Tuple of ndarrays containing the normalized input ndarray, the mean, and the variance\n    \"\"\"\n    mean=np.mean(x, axis=-1, keepdims=True)\n    var=np.var(x, axis=-1, keepdims=True)\n    x_norm=(x-mean)/np.sqrt(var+eps)\n    return x_norm,mean,var\n\ndef layer_norm_backprop(dout: np.ndarray, x: np.ndarray, mean: np.ndarray, var: np.ndarray, eps: float=1e-6) -&gt;np.ndarray:\n\n    \"\"\"\n        Compute the gradient of the loss with respect to the input x of a layer normalization operation, implementing backpropagation as defined in the tutorial\n        Inputs: \n            dout: an ndarray containing the gradient with respect to the normalized input\n            x: an ndarray containing the original input\n            mean: mean of the input ndarray x along the last axis, as computed above\n            var: exactly like the mean\n            eps: an optional value for numerical stabillity\n        Outputs:\n            an ndarray containing the gradient with respect to the layer normalization procedure\n    \"\"\"\n\n    #numerically backpropagate by calculating the derivatives. unfortunately, this just requires knowing the formula as shown above\n    N=x.shape[-1]\n    dx_norm=dout/np.sqrt(var+eps)\n    dvar=np.sum(dout*(x-mean)*-0.5*(var+eps)**(-1.5), axis=-1, keepdims=True)\n    dmean=(np.sum(dout*-1/np.sqrt(var+eps), axis=-1, keepdims=True)+dvar*np.sum(-2*(x-mean), axis=-1, keepdims=True)/N)\n    dx=dx_norm+dvar*2*(x-mean)/N+dmean/N\n    return dx\n\ndef softmax(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n        Compute the softmax of an input ndarray and generate a probability distribution\n        Inputs:\n            x: an ndarray that you want to softmax\n        Outputs:\n            an ndarray containing the softmaxed version along the last axis\n    \"\"\"\n    x_shifted=x-np.max(x, axis=-1, keepdims=True) #this is done to improve numerical stability. softmaxing is invariant to shifts by a constant value\n    exp_x=np.exp(x_shifted)\n    return exp_x/np.sum(exp_x, axis=-1, keepdims=True)\n\ndef cross_entropy_loss(predictions: np.ndarray, targets: np.ndarray) -&gt; float:\n    \"\"\"\n        Compute the cross-entropy loss as defined above, measuring the difference between two probability predicted distributions\n        Inputs:\n            predictions: an ndarray containing whatever you have predicted\n            targets: an ndarray containing whatever the real targets are\n        Outputs:\n            a float of computed cross-entropy loss, averaged over all elements in the batch\n    \"\"\"\n    epsilon=1e-12 #more numerical precision constants\n    predictions=np.clip(predictions, epsilon, 1-epsilon) #values smaller than epsilon become epsilon, values larger than 1-epsilon become 1-epsilon\n    flat_targets=targets.flatten() #maybe the arrays are not 1d, but the cross-entropy loss needs the 1d\n    flat_preds=predictions.reshape(-1, predictions.shape[-1]) #same reason here for flattening\n    loss=-np.mean(np.log(flat_preds[np.arange(flat_targets.shape[0]), flat_targets])) #compute the crossentropy loss\n    return loss\n\ndef cross_entropy_derivative(predictions: np.ndarray, targets: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n        Compute the backpropagation for cross-entropy loss\n        Inputs:\n            predictions: an ndarray containing whatever you have predicted\n            targets: an ndarray containing whatever the real targets are\n        Outputs:\n            an ndarray of the gradient of the cross-entropy derivative required for backpropagation\n    \"\"\"\n    #the gradient was also defined above so this is just a way to get batches of data in and publish it\n    batch,length,vocab_size=predictions.shape\n    grad=predictions.copy()\n    for b in range(batch):\n        for t in range(length):\n            grad[b, t, targets[b,t]]-=1\n    grad/=(batch*length)\n    return grad\n\n###############################################################################\n# Multi-Head Attention\n###############################################################################\n\ndef split_heads(x: np.ndarray, num_heads: int) -&gt; np.ndarray:\n    \"\"\"\n        Attention heads attend to different parts of the data, so this is a function to simply split the data \n        Inputs:\n            x: an ndarray of input data\n            num_heads: the number of attention instances you want\n        Outputs:\n            a reshaped x split into the number of heads\n    \"\"\"\n    #at this point it should be familiar, split the data into batches and make it work\n    batch,length,d_model=x.shape\n    head_dim=d_model//num_heads #the heads attend to different parts of the model\n    return x.reshape(batch, length, num_heads, head_dim).transpose(0,2,1,3) #reshape x so that every ndarray is for heads\n\ndef merge_heads(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n        After the result of multihead attention, you need to recombine them, so this function does it\n        Inputs:\n            x: an ndarray containing information of shape (batch, num_heads, length, head_dim)\n        Outputs:\n            a combined x after (presumably) multihead attention has been done\n    \"\"\"\n    batch,num_heads,length,head_dim=x.shape\n    return x.transpose(0, 2, 1, 3).reshape(batch, length, num_heads*head_dim)\n\ndef scaled_dot_product_attention(attention_matrix: np.ndarray, response_matrix: np.ndarray, information_matrix: np.ndarray, mask: Optional[np.ndarray]=None) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n        This is the official formula of the attention. As we have seen, we use an attention matrix to make each word ask each other word a question.\n        The response of each other word is the response matrix and the information passed on is the information matrix. \n        inputs:\n            attention_matrix: An ndarray of attention values\n            response_matrix: An ndarray of response values\n            information_matrix: An ndarray of information values that is passed on\n            mask: An ndarray of masks, but with Bool values\n        Outputs:\n            a tuple of the information passed on and the attention weights\n    \"\"\" \n    normalization_factor=attention_matrix.shape[-1]\n    scores=np.matmul(attention_matrix, response_matrix.transpose(0,1,3,2))/np.sqrt(normalization_factor) #we are now using 'official' terminology\n    if mask is not None:\n        scores=np.where(mask[np.newaxis, np.newaxis,:,:], -1e9, scores) #mask if required\n    attn_weights=softmax(scores) #softmax the product\n    output=np.matmul(attn_weights, information_matrix) #send information on\n    return output, attn_weights\n\ndef multi_head_attention(batched_input_intrinsic_value_matrix: np.ndarray, batched_input_response_matrix: np.ndarray, batched_input_information_matrix: np.ndarray, batched_input_intrinsic_value_projection_matrix: np.ndarray, batched_input_response_projection_matrix: np.ndarray, batched_input_information_projection_matrix: np.ndarray, final_reshaper_matrix: np.ndarray, num_heads: int, mask: Optional[np.ndarray]=None) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n        Implementation of multihead attention. Simply put, apply attention on smaller parts of the sequence, and then recombine them by concatenation.\n        Because we are splitting the sequence, we need to generate different attention, response, and information matrices for each attention instance.\n        This means we have to project the input into different matrices every time, and the matrix that does this projection is also learned.\n        This is the trick behind multihead attention.\n        Inputs:\n            batched_input_intrinsic_value_matrix: an ndarray of your batched input\n            batched_input_response_matrix: an ndarray that is essentially the same as batched_input_intrinsic_value\n            batched_input_information_matrix: same as above. you have to declare that these matrices exist \n            batched_input_intrinsic_value_projection_matrix: an ndarray that projects the intrinsic value to the size for multihead attention\n            batched_input_response_projection_matrix: same thing for response_matrix\n            batched_input_information_projection_matrix: same thing as above\n        Returns:\n            A tuple of ndarrays that have the concatenated result of multihead attention and also the attention weights respectively. \n    \"\"\"\n    attention_matrix=batched_input_intrinsic_value_matrix @ batched_input_intrinsic_value_projection_matrix #we now use the inbuilt @ operator to do matrix multiplication quickly\n    response_matrix=batched_input_response_matrix @ batched_input_intrinsic_value_projection_matrix\n    information_matrix=batched_input_information_matrix @ batched_input_information_projection_matrix\n\n    #extract dimensions from the attention matrix (query tensor)\n    #attention_matrix` is the query tensor with shape (batch_size, seq_len_q, d_model)\n    batch, lq, d_model=attention_matrix.shape\n\n    #extract the length of the key tensor from the response matrix\n    #response_matrix is the key tensor with shape (batch_size, seq_len_k, d_model)\n    #lk represents seq_len_k (sequence length of the response_matrix), which may differ from lq (seq_len_q)\n    lk=response_matrix.shape[1]\n\n    #we don't explicitly compute `lv` (seq_len_v, length of values) because information_matrix is expected to have the same sequence length as response_matrix\n\n    #reshape and transpose attention_matrix to prepare for multi-head attention\n    #step 1: reshape attention_matrix from (batch_size, seq_len_q, d_model) to (batch_size, seq_len_q, num_heads, head_dim), where head_dim = d_model//num_heads\n    #step 2: transpose to (batch_size, num_heads, seq_len_q, head_dim) for easier computation per head\n    A=attention_matrix.reshape(batch, lq, num_heads, d_model//num_heads).transpose(0,2,1,3)\n\n    #reshape and transpose response_matrix similarly\n    B=response_matrix.reshape(batch, lk, num_heads, d_model//num_heads).transpose(0,2,1,3)\n\n    #reshape and transpose information_matrix similarly\n    C=information_matrix.reshape(batch, lk, num_heads, d_model//num_heads).transpose(0,2,1,3)\n\n    #compute the attention each head\n    #input ndarrays (A, B, B) are now split into individual heads for parallel processing\n    #'mask` is optional and is used to block certain positions (e.g., future positions in autoregressive decoding)\n    out_heads, attn_weights=scaled_dot_product_attention(A, B, C, mask)\n\n    #concatenate the output\n    out=(merge_heads(out_heads))@final_reshaper_matrix  # Apply a linear projection to combine the head outputs into `d_model` dimensions.\n\n    return out, attn_weights\n\n\ndef mha_backprop(\n        dout: np.ndarray,\n        batched_input_intrinsic_value_matrix: np.ndarray,\n        batched_input_response_matrix: np.ndarray,\n        batched_input_information_matrix: np.ndarray, \n        batched_input_intrinsic_value_projection_matrix: np.ndarray,\n        batched_input_response_projection_matrix: np.ndarray, \n        batched_input_information_projection_matrix: np.ndarray, \n        final_projection_matrix: np.ndarray,\n        attention_weights: np.ndarray, \n        num_heads: int, \n        mask: Optional[np.ndarray]=None\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n        This is the big one. This is the function that backpropagates through multihead attention. \n        It computes the gradients of the loss with respect to the sequence matrices and the weight matrices used in the multi-head attention mechanism.\n        Like before, we are in fact feeding it the loss.\n        Inputs:\n            dout: an ndarray of the gradient of the loss with respect to the output of multihead attention\n            batched_input_intrinsic_value_matrix: an ndarray as defined in the multihead attention function above,\n            batched_input_response_matrix: an ndarray as defined in the multihead attention function above ,\n            batched_input_information_matrix: an ndarray as defined in the multihead attention function above,   \n            batched_input_intrinsic_value_projection_matrix: an ndarray as defined in the multihead attention function above,\n            batched_input_response_projection_matrix: an ndarray as defined in the multihead attention function above, \n            batched_input_information_projection_matrix: an ndarray as defined in the multihead attention function above, \n            final_projection_matrix: an ndarray as defined in the multihead attention function above,\n            attention_weights: an ndarray that is the output of multihead attention function, required for backpropagation\n            mask: an optional ndarray of masks (Bool data type)\n        Outputs:\n            the following tuple: Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n            - differential_intrinsic_value_matrix (np.ndarray): Gradient w.r.t. the query input `batched_input_intrinsic_value_matrix`, shape (batch_size, seq_len_q, d_model).\n            - differential_response_matrix (np.ndarray): Gradient w.r.t. the key input `batched_input_response_matrix`, shape (batch_size, seq_len_k, d_model).\n            - differential_information_matrix (np.ndarray): Gradient w.r.t. the value input `batched_input_information_matrix`, shape (batch_size, seq_len_k, d_model).\n            - original_shape_intrinsic_value_differential (np.ndarray): Gradient w.r.t. the query weight matrix `batched_input_intrinsic_value_projection_matrix`, shape (d_model, d_model).\n            - original_shape_response_differential (np.ndarray): Gradient w.r.t. the key weight matrix `batched_input_response_projection_matrix`, shape (d_model, d_model).\n            - original_shape_information_differential (np.ndarray): Gradient w.r.t. the value weight batched_input_information_projection_matrix `W_v`, shape (d_model, d_model).\n            - original_shape_attention_differential (np.ndarray): Gradient w.r.t. the output weight matrix `final_projection_matrix`, shape (d_model, d_model).\n    \"\"\"\n    #throughout the code, a variable starting with 'd' denotes a derivative/gradient, aside from d_model, which is the model dimension (user-defined really)\n    batch, attention_sequence_length, d_model=batched_input_intrinsic_value_matrix.shape #compute the gradient matrix\n    sequence_length=batched_input_response_matrix.shape[1] #length of the sequence, obtained from really any matrix\n    head_dim=d_model//num_heads #just the model head size\n\n    #recompute the forward step\n    attention_matrix=batched_input_intrinsic_value_matrix @ batched_input_intrinsic_value_projection_matrix #standard stuff\n    response_matrix=batched_input_response_matrix @ batched_input_response_projection_matrix\n    information_matrix=batched_input_information_matrix @ batched_input_information_projection_matrix\n\n    #similarly reshaping stuff as done before\n    head_attention_matrix=attention_matrix.reshape(batch, attention_sequence_length, num_heads, head_dim).transpose(0,2,1,3)\n    head_response_matrix=response_matrix.reshape(batch, sequence_length, num_heads, head_dim).transpose(0,2,1,3)\n    head_information_matrix=information_matrix.reshape(batch, sequence_length, num_heads, head_dim).transpose(0,2,1,3)\n\n    d_merged_heads=dout.reshape(batch, attention_sequence_length, d_model) #reshape dout into the correct shape\n    d_out_heads=d_merged_heads.reshape(batch, attention_sequence_length, num_heads, head_dim).transpose(0,2,1,3)\n\n    #check shape consistency, because it makes for easier debugging\n    assert d_out_heads.shape==(batch, num_heads, attention_sequence_length, head_dim), f\"Shape mismatch in d_out_heads: {d_out_heads.shape}\" #first time we've used 'assert'\n\n    #attention weights and value gradients\n    d_attention_weights=np.matmul(d_out_heads, head_information_matrix.transpose(0,1,3,2)) #this is pretty standard fare, start getting the stuff out from backprop\n    d_vh=np.matmul(attention_weights.transpose(0,1,3,2), d_out_heads) #this is also the same thing\n\n    #calculate the differential element of the attention scores, for backpropagation\n    sum_over_j=np.sum(attention_weights*d_attention_weights, axis=-1, keepdims=True)\n    d_scores=attention_weights*(d_attention_weights-sum_over_j)\n\n    #calculate the differential elements for the attention and response matrices, per head\n    d_attention_head=np.matmul(d_scores, head_response_matrix)/np.sqrt(head_dim)\n    d_response_head=np.matmul(d_scores.transpose(0,1,3,2), head_attention_matrix)/np.sqrt(head_dim)\n\n    #combine the elements back for concatenation\n    total_attention_differential=d_attention_head.transpose(0,2,1,3).reshape(batch, attention_sequence_length, d_model)\n    total_response_differential=d_response_head.transpose(0,2,1,3).reshape(batch, sequence_length, d_model)\n    total_information_differential=d_vh.transpose(0,2,1,3).reshape(batch, sequence_length, d_model)\n\n    #now calculate the total gradients for all elements\n    original_shape_intrinsic_value_differential=np.matmul(batched_input_intrinsic_value_matrix.reshape(-1, d_model).T, total_attention_differential.reshape(-1, d_model))\n    original_shape_response_differential=np.matmul(batched_input_response_matrix.reshape(-1, d_model).T, total_response_differential.reshape(-1, d_model))\n    original_shape_information_differential=np.matmul(batched_input_information_matrix.reshape(-1, d_model).T, total_information_differential.reshape(-1, d_model))\n    #original_shape_attention_differential=np.matmul(merge_heads(attention_weights @ information_matrix).reshape(batch*attention_sequence_length, d_model).T, dout.reshape(batch*attention_sequence_length, d_model))\n    C = information_matrix.reshape(batch, sequence_length, num_heads, head_dim).transpose(0, 2, 1, 3)\n\n    original_shape_attention_differential = np.matmul(\n    merge_heads(np.matmul(attention_weights, C)).reshape(batch * attention_sequence_length, d_model).T,\n    dout.reshape(batch * attention_sequence_length, d_model)\n        )\n    differential_intrinsic_value_matrix = total_attention_differential @ batched_input_intrinsic_value_projection_matrix.T\n    differential_response_matrix = total_response_differential @ batched_input_response_projection_matrix.T\n    differential_information_matrix = total_information_differential @ batched_input_information_projection_matrix.T\n\n    return differential_intrinsic_value_matrix, differential_response_matrix, differential_information_matrix, original_shape_intrinsic_value_differential, original_shape_response_differential, original_shape_information_differential, original_shape_attention_differential\n\n\n###############################################################################\n# Feed Forward Network\n###############################################################################\n\n#now we define the feedforward neural network (2 layers) with backprop. we use the ReLU activation function for easy differentials\n#note the different output in the definition of the feedforward network\ndef feed_forward(x: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray) -&gt; Tuple[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\" \n        This is a simple two layer feedforward neural network. The reason a cache is returned at all is because it is immensely helpful in backpropagation\n        Inputs:\n            x: input ndarray of data\n            W1: weight matrix for the first linear transformation\n            b1: bias vector for first layer\n            W2, b2: same as above for second layer\n        Outputs:\n            Tuple[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n            - z2 (np.ndarray): the output tensor after the feed-forward computation of shape (batch_size, seq_len, d_model).\n            - cache (Tuple[np.ndarray, np.ndarray]): a tuple containing:\n                - z1 (np.ndarray): the output of the first linear transformation before the ReLU activation, of shape (batch_size, seq_len, d_ff).\n                - relu (np.ndarray): The output of the ReLU activation, of shape (batch_size, seq_len, d_ff).\n    \"\"\"\n    #this is just straighforward - two matrix multiplications with a ReLU in between\n    z1=x@W1+b1\n    relu=np.maximum(0, z1)\n    z2=relu@W2+b2\n    return z2, (z1, relu)\n\ndef feed_forward_backprop(dz2: np.ndarray, x: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray, cache: Tuple[np.ndarray, np.ndarray]) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n        The backpropagation for a feedforward layer. Note that we are working backwards, so the input variables are defined that way\n        Inputs:\n            dz2: ndarray containing gradient of the loss function with respect to output of the second layer\n            x: input ndarray of data\n            W1-cache: same as above\n        Outputs:\n            Tuple of losses (ndarrays) with respect to x,W1,b1,W2,b2 respectively\n    \"\"\"\n    #backpropagation, step by step, is the exact formula outlines in the tutorial\n    (z1, relu)=cache #now you see why we utilized the cache at all\n    batch, length, d_model=x.shape\n    dW2=np.matmul(relu.reshape(-1, d_model).T, dz2.reshape(-1, d_model))\n    db2=dz2.sum(axis=(0,1))\n    d_relu=dz2@W2.T\n    d_z1=d_relu*(z1&gt;0)\n    dW1=np.matmul(x.reshape(-1, d_model).T, d_z1.reshape(-1, d_model))\n    db1=d_z1.sum(axis=(0,1))\n    dX=d_z1@W1.T\n    return dX, dW1, db1, dW2, db2\n\n###############################################################################\n# Encoder and Decoder Blocks\n###############################################################################\n\n#we now build the actual encoder-decoder layer.\n\ndef encoder_layer(x: np.ndarray, intrinsic_value_projector: np.ndarray, response_projector: np.ndarray, information_projector: np.ndarray, attention_projector: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray, num_heads: int=2) -&gt; Tuple[np.ndarray, Tuple]:\n    \"\"\"\n        Now we build the transformer in earnest from all of our classes. This implements the encoder layer using the structure outlined in the tutorial\n        Inputs:\n            x: ndarray of input data\n            intrinsic_value_projector, response_projector, information_projector, attention_projector: ndarrays containing projection matrices for input data\n            W1, b1, W2, b2: ndarrays of the two-layer neural network\n            num_heads: number of attention heads\n        Outputs:\n            out: output ndarray of the entire encoder\n            cache: a Tuple of all cached values for backpropagation\n\n    \"\"\"\n    #at this point i start using smaller variable names because the program becomes tedious to read. anyway, this code is self-explanatory \n    x_norm, mean1, var1=layer_norm(x)\n    attn_out, attn_w=multi_head_attention(x_norm, x_norm, x_norm, intrinsic_value_projector, response_projector, information_projector, attention_projector, num_heads=num_heads)\n    x2=x+attn_out\n\n    x2_norm, mean2, var2=layer_norm(x2)\n    ff_out, ff_cache=feed_forward(x2_norm,W1,b1,W2,b2)\n    out=x2+ff_out\n    cache=(x, x2, x_norm, x2_norm, mean1, var1, mean2, var2, attn_w, ff_cache)\n    return out, cache\n\ndef decoder_layer(x, encoder_out,\n                  intrinsic_weight_masked, response_weight_masked, information_weight_masked, attention_weight_masked,\n                  intrinsic_crossattention_weight, response_crossattention_weight, information_crossattention_weight, attention_crossattention_weight, W1, b1, W2, b2, mask, num_heads=2):\n    \"\"\"\n        We implement the decoder layer with masked attention.\n        Inputs: \n            x: ndarray of input data\n            intrinsic_weight_masked: projection matrix for masked self-attention for the intrinsic value weight\n            response_weight_masked: projection matrix for masked self-attention for the response weight\n            information_weight_masked: projection matrix for masked self-attention for the information weight\n            attention_weight_masked: projection matrix for masked self-attention for the attention weights\n            intrinsic_crossattention_weight: projection matrix for the intrinsic value weight during cross-attention\n            response_crossattention_weight: projection matrix for the response weight during cross-attention\n            information_crossattention_weight: projection matrix for the information weight during cross-attention\n            attention_crossattention_weight: projection matrix for the attention weights during cross-attention\n            W1,b1,W2,b2, mask: all defined above\n            num_heads: number of attention instances\n        Outputs:\n            Tuple[np.ndarray, Tuple]:\n            - out: output ndarray of the decoder layer, shape (batch_size, seq_len, d_model)\n            - cache (Tuple): cached values for backpropagation, including:\n                - x, x2, x3 : intermediate ndarrays at various stages\n                - x_norm_dec1, x2_norm, x3_norm : layer-normalized ndarrays\n                - mean_dec1, var_dec1, mean_dec2, var_dec2, mean_dec3, var_dec3: Means and variances (in ndarrays) from layer normalization\n                - masked_attn_w, cross_attn_w: attention weight ndarrays from masked self-attention and cross-attention\n                - ff_cache_dec (Tuple): cached values from the feed-forward network\n    \"\"\"\n    #this just implements the decoder layer\n\n    #first, apply layer normalization, then compute multihead attention\n    x_norm_dec1,mean_dec1,var_dec1=layer_norm(x)\n    masked_attn_out, masked_attn_w=multi_head_attention(x_norm_dec1, x_norm_dec1, x_norm_dec1,\n                                                          intrinsic_weight_masked, response_weight_masked, information_weight_masked, attention_weight_masked, \n                                                          num_heads=num_heads, mask=mask)\n    x2=x+masked_attn_out\n    #then layernorm again\n    x2_norm, mean_dec2, var_dec2=layer_norm(x2)\n    cross_attn_out, cross_attn_w=multi_head_attention(x2_norm, encoder_out, encoder_out,\n                                                        intrinsic_crossattention_weight, response_crossattention_weight, information_crossattention_weight, attention_crossattention_weight,\n                                                        num_heads=num_heads, mask=None)\n    x3=x2+cross_attn_out\n    #layer norm thrice\n    x3_norm, mean_dec3, var_dec3=layer_norm(x3)\n    ff_out, ff_cache_dec=feed_forward(x3_norm, W1, b1, W2, b2)\n    out=x3+ff_out\n    #prepare the cache for backprop\n    cache=(x, x2, x3, x_norm_dec1, x2_norm, x3_norm, mean_dec1, var_dec1, mean_dec2, var_dec2, mean_dec3, var_dec3, masked_attn_w, cross_attn_w, ff_cache_dec)\n    return out, cache\n\n###############################################################################\n# Forward and Backprop Through Model (Single Layer Encoder-Decoder)\n###############################################################################\n\n#at this point \n\ndef forward_transformer(enc_in: np.ndarray, dec_in: np.ndarray,\n                        intrinsic_value_weight_enc: np.ndarray, response_weight_enc: np.ndarray, information_weight_enc: np.ndarray, attention_weight_enc: np.ndarray, W1_enc: np.ndarray, b1_enc: np.ndarray, W2_enc: np.ndarray, b2_enc: np.ndarray,\n                        intrinsic_value_weight_dec_masked: np.ndarray, response_weight_dec_masked: np.ndarray, information_weight_dec_masked: np.ndarray, attention_weight_dec_masked: np.ndarray,\n                        intrinsic_value_weight_dec_cross: np.ndarray, response_weight_dec_cross: np.ndarray, information_weight_dec_cross: np.ndarray, attention_weight_dec_cross: np.ndarray,\n                        W1_dec: np.ndarray, b1_dec: np.ndarray, W2_dec: np.ndarray, b2_dec: np.ndarray,\n                        W_embed_out: np.ndarray, b_embed_out: np.ndarray,\n                        src_mask: np.ndarray, tgt_mask: np.ndarray) -&gt; Tuple[np.ndarray, Tuple]:\n    \"\"\"\n        We implement the forward pass for the transformer model. This is fairly straightforward and how we've defined it\n\n        Inputs:\n            enc_in: ndarray of input data to the encoder, shape (batch_size, src_len, d_model)\n            dec_in: ndarray of input data to the decoder, shape (batch_size, tgt_len, d_model)\n            intrinsic_value_weight_enc: projection matrix for the query in the encoder's self-attention\n            response_value_weight_enc: projection matrix for the key in the encoder's self-attention\n            information_value_weight_enc: projection matrix for the value in the encoder's self-attention\n            attention_value_weight_enc: projection matrix for the output in the encoder's self-attention\n            W1_enc, b1_enc, W2_enc, b2_enc: feed-forward network weights and biases for the encoder\n            intrinsic_value_weight_dec_masked: projection matrix for the query in the decoder's masked self-attention\n            response_value_weight_dec_masked: projection matrix for the key in the decoder's masked self-attention\n            information_value_weight_dec_masked: projection matrix for the value in the decoder's masked self-attention\n            attention_value_weight_dec_masked: projection matrix for the output in the decoder's masked self-attention\n            intrinsic_value_weight_dec_cross: projection matrix for the query in the decoder's cross-attention\n            response_value_weight_dec_cross: projection matrix for the key in the decoder's cross-attention\n            information_value_weight_dec_cross: projection matrix for the value in the decoder's cross-attention\n            attention_value_weight_dec_cross: projection matrix for the output in the decoder's cross-attention\n            W1_dec, b1_dec, W2_dec, b2_dec: feed-forward network weights and biases for the decoder\n            W_embed_out: projection matrix for mapping decoder output to the vocabulary space, shape (d_model, vocab_size)\n            b_embed_out: bias vector for mapping decoder output to the vocabulary space, shape (vocab_size,)\n            src_mask: ndarray mask for the encoder, shape (src_len, src_len)\n            tgt_mask: ndarray mask for the decoder, shape (tgt_len, tgt_len)\n\n        Outputs:\n            Tuple[np.ndarray, Tuple]:\n            - probs: output probabilities over the vocabulary, shape (batch_size, tgt_len, vocab_size)\n            - cache (Tuple): cached values for backpropagation, including:\n                - enc_out: ndarray of the encoder output, shape (batch_size, src_len, d_model)\n                - enc_cache: cached intermediate values from the encoder\n                - dec_out: ndarray of the decoder output, shape (batch_size, tgt_len, d_model)\n                - dec_cache: cached intermediate values from the decoder\n    \"\"\"\n\n    #this should be fairly straighforward by now, how it's implemented. just feed your weights in and go through the entire layer\n    enc_out, enc_cache=encoder_layer(enc_in, intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc, W1_enc, b1_enc, W2_enc, b2_enc)\n    dec_out, dec_cache=decoder_layer(dec_in, enc_out, intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,\n                                       intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,\n                                       W1_dec, b1_dec, W2_dec, b2_dec,\n                                       mask=tgt_mask)\n    logits=dec_out@W_embed_out+b_embed_out\n    probs=softmax(logits)\n    return probs, (enc_out, enc_cache, dec_out, dec_cache)\n\ndef backward_transformer(dprobs: np.ndarray,\n    enc_in: np.ndarray,\n    dec_in: np.ndarray,\n    enc_out: np.ndarray,\n    enc_cache: Tuple,\n    dec_out: np.ndarray,\n    dec_cache: Tuple,\n    intrinsic_value_weight_enc: np.ndarray,\n    response_weight_enc: np.ndarray,\n    information_weight_enc: np.ndarray,\n    attention_weight_enc: np.ndarray,\n    W1_enc: np.ndarray,\n    b1_enc: np.ndarray,\n    W2_enc: np.ndarray,\n    b2_enc: np.ndarray,\n    intrinsic_value_weight_dec_masked: np.ndarray,\n    response_weight_dec_masked: np.ndarray,\n    information_weight_dec_masked: np.ndarray,\n    attention_weight_dec_masked: np.ndarray,\n    intrinsic_value_weight_dec_cross: np.ndarray,\n    response_weight_dec_cross: np.ndarray,\n    information_weight_dec_cross: np.ndarray,\n    attention_weight_dec_cross: np.ndarray,\n    W1_dec: np.ndarray,\n    b1_dec: np.ndarray,\n    W2_dec: np.ndarray,\n    b2_dec: np.ndarray,\n    W_embed_out: np.ndarray,\n    b_embed_out: np.ndarray,\n    src_mask: np.ndarray,\n    tgt_mask: np.ndarray) -&gt; Tuple[Dict[str, np.ndarray], np.ndarray]:\n    \"\"\"\n        This is possibly the hardest part in the code. This is complete backpropagation for the transformer, through all layers.\n\n        Inputs:\n            dprobs: ndarray of gradients with respect to output probabilities, shape (batch_size, tgt_len, vocab_size)\n            enc_in: ndarray of input data to the encoder, shape (batch_size, src_len, d_model)\n            dec_in: ndarray of input data to the decoder, shape (batch_size, tgt_len, d_model)\n            enc_out: ndarray of encoder outputs, shape (batch_size, src_len, d_model)\n            enc_cache: cached values from the encoder forward pass\n            dec_out: ndarray of decoder outputs, shape (batch_size, tgt_len, d_model)\n            dec_cache: cached values from the decoder forward pass\n            intrinsic_value_weight_enc: projection matrix for the query in the encoder's self-attention\n            response_weight_enc: projection matrix for the key in the encoder's self-attention\n            information_weight_enc: projection matrix for the value in the encoder's self-attention\n            attention_weight_enc: projection matrix for the output in the encoder's self-attention\n            W1_enc, b1_enc, W2_enc, b2_enc: feed-forward network weights and biases for the encoder\n            intrinsic_value_weight_dec_masked: projection matrix for the query in the decoder's masked self-attention\n            response_weight_dec_masked: projection matrix for the key in the decoder's masked self-attention\n            information_weight_dec_masked: projection matrix for the value in the decoder's masked self-attention\n            attention_weight_dec_masked: projection matrix for the output in the decoder's masked self-attention\n            intrinsic_value_weight_dec_cross: projection matrix for the query in the decoder's cross-attention\n            response_weight_dec_cross: projection matrix for the key in the decoder's cross-attention\n            information_weight_dec_cross: projection matrix for the value in the decoder's cross-attention\n            attention_weight_dec_cross: projection matrix for the output in the decoder's cross-attention\n            W1_dec, b1_dec, W2_dec, b2_dec: feed-forward network weights and biases for the decoder\n            W_embed_out: projection matrix for mapping decoder outputs to the vocabulary, shape (d_model, vocab_size)\n            b_embed_out: bias vector for mapping decoder outputs to the vocabulary, shape (vocab_size,)\n            src_mask: mask for the source sequence, shape (src_len, src_len)\n            tgt_mask: mask for the target sequence, shape (tgt_len, tgt_len)\n\n        Outputs:\n            Tuple[Dict[str, np.ndarray], np.ndarray]:\n            - grads: dictionary containing gradients for all trainable weights and biases, including:\n                - intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc: gradients for encoder self-attention weights\n                - W1_enc, b1_enc, W2_enc, b2_enc: gradients for encoder feed-forward network\n                - intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked: gradients for decoder masked self-attention weights\n                - intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross: gradients for decoder cross-attention weights\n                - W1_dec, b1_dec, W2_dec, b2_dec: gradients for decoder feed-forward network\n                - W_embed_out, b_embed_out: gradients for output projection layer\n            - dx_enc1: gradient with respect to the encoder input, shape (batch_size, src_len, d_model)\n    \"\"\"\n\n    #this is how backprop is implemented\n    batch, length, d_model=dec_out.shape  #first, extract the output of the decoder. what we are really interested in is the model dimension\n    vocab_size=W_embed_out.shape[1]  #and get the vocabulary size\n\n    #start: backprop through the final layer\n    \n    dW_embed_out=dec_out.reshape(-1, d_model).T@dprobs.reshape(-1, vocab_size)  # Shape: (d_model, vocab_size)\n    db_embed_out=dprobs.sum(axis=(0,1))  # Shape: (vocab_size,)\n    d_dec_out=dprobs @ W_embed_out.T  # Gradient w.r.t decoder output, shape (batch, tgt_len, d_model)\n\n    #get all values from the decoder - this is called 'unpacking'\n    (x, x2, x3,\n     x_norm_dec1, x2_norm_dec, x3_norm_dec,\n     mean_dec1, var_dec1, mean_dec2, var_dec2, mean_dec3, var_dec3,\n     masked_attn_w, cross_attn_w, ff_cache_dec)=dec_cache\n\n    #backprop through the feedforward neural network for the decoder\n    d_x3_ff=d_dec_out\n    d_x3_ff, dW1_dec, db1_dec, dW2_dec, db2_dec=feed_forward_backprop(d_x3_ff, x3_norm_dec, W1_dec, b1_dec, W2_dec, b2_dec, ff_cache_dec)\n    dx3_norm=d_x3_ff\n    dx3=layer_norm_backprop(dx3_norm, x3_norm_dec, mean_dec3, var_dec3)\n    d_x3_skip=dx3\n\n    #backprop through crossattention\n    d_x2_cross=d_x3_skip\n    dx_cross_Q, dx_cross_K, dx_cross_V, dintrinsic_value_weight_dec_cross_, dresponse_weight_dec_cross_, dinformation_weight_dec_cross_, dattention_weight_dec_cross_ = mha_backprop(\n        d_x2_cross, x2_norm_dec, enc_out, enc_out,\n        intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,\n        cross_attn_w, num_heads=2\n    )\n    d_x2_skip=dx_cross_Q\n    d_enc_out=dx_cross_K+dx_cross_V\n\n    #backprop through masked selfattention\n    d_x2_masked=d_x2_skip\n    dx_masked_Q, dx_masked_K, dx_masked_V, dintrinsic_value_weight_dec_masked_, dresponse_weight_dec_masked_, dinformation_weight_dec_masked_, dattention_weight_dec_masked_ = mha_backprop(\n        d_x2_masked, x_norm_dec1, x_norm_dec1, x_norm_dec1,\n        intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,\n        masked_attn_w, num_heads=2, mask=tgt_mask\n    )\n    dx_norm_dec1=dx_masked_Q+dx_masked_K+dx_masked_V\n    dx_dec1=layer_norm_backprop(dx_norm_dec1, x_norm_dec1, mean_dec1, var_dec1)\n\n    dx2_norm=d_x2_cross\n    dx2=layer_norm_backprop(dx2_norm, x2_norm_dec, mean_dec2, var_dec2)\n\n    #combine different layers' gradients\n    dx=dx_dec1+dx2\n\n    #unpack encoder values\n    (enc_x, enc_x2, enc_x_norm, enc_x2_norm, enc_mean1, enc_var1, enc_mean2, enc_var2, enc_attn_w, enc_ff_cache)=enc_cache\n\n    #backprop through the encoder\n    d_enc=d_enc_out\n    d_enc_ff, dW1_enc, db1_enc, dW2_enc, db2_enc=feed_forward_backprop(d_enc, enc_x2_norm, W1_enc, b1_enc, W2_enc, b2_enc, enc_ff_cache)\n    d_enc2_norm=d_enc_ff\n    d_enc2=layer_norm_backprop(d_enc2_norm, enc_x2_norm, enc_mean2, enc_var2)\n\n    #backprop through encoder's attention\n    dx_enc_Q, dx_enc_K, dx_enc_V, dintrinsic_value_weight_enc_, dresponse_weight_enc_, dinformation_weight_enc_, dattention_weight_enc_=mha_backprop(\n        d_enc2, enc_x_norm, enc_x_norm, enc_x_norm,\n        intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc,\n        enc_attn_w, num_heads=2\n    )\n    d_enc_norm1=dx_enc_Q+dx_enc_K+dx_enc_V\n    dx_enc1=layer_norm_backprop(d_enc_norm1, enc_x_norm, enc_mean1, enc_var1)\n\n    #combine all gradients in a dictionary \n    grads = {\n        'intrinsic_value_weight_enc': dintrinsic_value_weight_enc_, 'response_weight_enc': dresponse_weight_enc_, 'information_weight_enc': dinformation_weight_enc_, 'attention_weight_enc': dattention_weight_enc_,\n        'W1_enc': dW1_enc, 'b1_enc': db1_enc, 'W2_enc': dW2_enc, 'b2_enc': db2_enc,\n        'intrinsic_value_weight_dec_masked': dintrinsic_value_weight_dec_masked_, 'response_weight_dec_masked': dresponse_weight_dec_masked_, 'information_weight_dec_masked': dinformation_weight_dec_masked_, 'attention_weight_dec_masked': dattention_weight_dec_masked_,\n        'intrinsic_value_weight_dec_cross': dintrinsic_value_weight_dec_cross_, 'response_weight_dec_cross': dresponse_weight_dec_cross_, 'information_weight_dec_cross': dinformation_weight_dec_cross_, 'attention_weight_dec_cross': dattention_weight_dec_cross_,\n        'W1_dec': dW1_dec, 'b1_dec': db1_dec, 'W2_dec': dW2_dec, 'b2_dec': db2_dec,\n        'W_embed_out': dW_embed_out, 'b_embed_out': db_embed_out\n    }\n\n    return grads, dx_enc1\n\n###############################################################################\n# Main Training Setup\n###############################################################################\n\n\n#we are finally done. let's now train the actual transformer\nenglish_sentences=[\n    \"My rabbit likes bananas\"\n]\n\nitalian_sentences=[\n    \"Al mio coniglio piacciono le banane\",\n]\n\neng_tokens=[whitespace_tokenizer(s) for s in english_sentences]\nfor_tokens=[whitespace_tokenizer(s) for s in italian_sentences]\n\neng_word2idx, eng_idx2word=build_vocab(eng_tokens)\nfor_word2idx, for_idx2word=build_vocab(for_tokens)\n\ndef encode(tokens, w2i):\n    return [w2i[t] for t in tokens]\n\neng_encoded=[encode(t, eng_word2idx) for t in eng_tokens]\nfor_encoded=[encode(t, for_word2idx) for t in for_tokens]\n\nmax_eng_len=max(len(x) for x in eng_encoded)\nmax_for_len=max(len(x) for x in for_encoded)\n\n# Add start/end tokens\nstart_token=len(for_word2idx)\nend_token=len(for_word2idx)+1\nfor_word2idx[\"&lt;start&gt;\"]=start_token\nfor_word2idx[\"&lt;end&gt;\"]=end_token\nfor_idx2word[start_token]=\"&lt;start&gt;\"\nfor_idx2word[end_token]=\"&lt;end&gt;\"\n\nfor_idx2word={idx: token for token, idx in for_word2idx.items()}\n\nfor_encoded_input=[[start_token]+seq for seq in for_encoded]\nfor_encoded_target=[seq+[end_token] for seq in for_encoded]\n\nmax_for_len_inp=max(len(s) for s in for_encoded_input)\nmax_for_len_tgt=max(len(s) for s in for_encoded_target)\n\neng_padded=pad_sequences(eng_encoded, max_length=max_eng_len, pad_value=0)\nfor_inp_padded=pad_sequences(for_encoded_input, max_length=max_for_len_inp, pad_value=0)\nfor_tgt_padded=pad_sequences(for_encoded_target, max_length=max_for_len_tgt, pad_value=0)\n\nbatch_size=len(eng_padded)\nsrc_len=eng_padded.shape[1]\ntgt_len=for_inp_padded.shape[1]\nvocab_size_src=len(eng_word2idx)\nvocab_size_tgt=len(for_word2idx)\nd_model=16 #this is arbitrary. many AI companies sell access to their embeddings and dimensions\n\nsrc_embeddings=np.random.randn(vocab_size_src, d_model)*0.01 #i chose 0.01 for a balance between numerical stability and demonstrating the power of transformers\ntgt_embeddings=np.random.randn(vocab_size_tgt, d_model)*0.01\n\n#we haven't actually defined the embedding function- let's embed our vocabulary\ndef embed(x, emb):\n    return emb[x] \n\n#this next section is just defining random matrices\n\nW_embed_out=np.random.randn(d_model, vocab_size_tgt)*0.01 #define this random matrix for embedding\nb_embed_out=np.zeros(vocab_size_tgt) #and the bias of the weights as well\n\nintrinsic_value_weight_enc=np.random.randn(d_model, d_model)*0.01 \nresponse_weight_enc=np.random.randn(d_model, d_model)*0.01\ninformation_weight_enc=np.random.randn(d_model, d_model)*0.01\nattention_weight_enc=np.random.randn(d_model, d_model)*0.01\nW1_enc=np.random.randn(d_model, d_model)*0.01\nb1_enc=np.zeros(d_model)\nW2_enc=np.random.randn(d_model, d_model)*0.01\nb2_enc=np.zeros(d_model)\n\nintrinsic_value_weight_dec_masked=np.random.randn(d_model, d_model)*0.01\nresponse_weight_dec_masked=np.random.randn(d_model, d_model)*0.01\ninformation_weight_dec_masked=np.random.randn(d_model, d_model)*0.01\nattention_weight_dec_masked=np.random.randn(d_model, d_model)*0.01\n\nintrinsic_value_weight_dec_cross=np.random.randn(d_model, d_model)*0.01\nresponse_weight_dec_cross=np.random.randn(d_model, d_model)*0.01\ninformation_weight_dec_cross=np.random.randn(d_model, d_model)*0.01\nattention_weight_dec_cross=np.random.randn(d_model, d_model)*0.01\n\nW1_dec=np.random.randn(d_model, d_model)*0.01\nb1_dec=np.zeros(d_model)\nW2_dec=np.random.randn(d_model, d_model)*0.01\nb2_dec=np.zeros(d_model)\n\n#here we define the learning rate and epochs\nlearning_rate=0.01\nepochs=20\n\nsrc_mask=None\ntgt_mask=create_mask_for_removing_future_dependency(for_inp_padded)\n\n#and implement the generic neural network training\n\nfor epoch in range(epochs):\n    enc_inp=embed(eng_padded, src_embeddings)\n    dec_inp=embed(for_inp_padded, tgt_embeddings)\n\n    #this is the forward pass\n    probs, cache=forward_transformer(enc_inp, dec_inp,\n                                       intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc, W1_enc, b1_enc, W2_enc, b2_enc,\n                                       intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,\n                                       intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,\n                                       W1_dec, b1_dec, W2_dec, b2_dec,\n                                       W_embed_out, b_embed_out,\n                                       src_mask, tgt_mask)\n\n    loss=cross_entropy_loss(probs, for_tgt_padded)\n    print(f\"Epoch {epoch+1}, Loss: {loss}\")\n\n    pred_indices=np.argmax(probs, axis=-1)\n    for b in range(batch_size):\n        predicted_tokens=[for_idx2word[idx] for idx in pred_indices[b]]\n        print(\"Predicted:\", \" \".join(predicted_tokens))\n    \n    dprobs=cross_entropy_derivative(probs, for_tgt_padded)\n\n    #this is the backward pass\n    grads, dx_enc=backward_transformer(dprobs, enc_inp, dec_inp, *cache,\n                                         intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc, W1_enc, b1_enc, W2_enc, b2_enc,\n                                         intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,\n                                         intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,\n                                         W1_dec, b1_dec, W2_dec, b2_dec,\n                                         W_embed_out, b_embed_out,\n                                         src_mask, tgt_mask)\n\n    #update all parameters after backprop\n    intrinsic_value_weight_enc-=learning_rate*grads['intrinsic_value_weight_enc']\n    response_weight_enc-=learning_rate*grads['response_weight_enc']\n    information_weight_enc-=learning_rate*grads['information_weight_enc']\n    attention_weight_enc-=learning_rate*grads['attention_weight_enc']\n    W1_enc-=learning_rate*grads['W1_enc']\n    b1_enc-=learning_rate*grads['b1_enc']\n    W2_enc-=learning_rate*grads['W2_enc']\n    b2_enc-=learning_rate*grads['b2_enc']\n\n    intrinsic_value_weight_dec_masked-=learning_rate*grads['intrinsic_value_weight_dec_masked']\n    response_weight_dec_masked-=learning_rate*grads['response_weight_dec_masked']\n    information_weight_dec_masked-=learning_rate*grads['information_weight_dec_masked']\n    attention_weight_dec_masked-=learning_rate*grads['attention_weight_dec_masked']\n\n    intrinsic_value_weight_dec_cross-=learning_rate*grads['intrinsic_value_weight_dec_cross']\n    response_weight_dec_cross-=learning_rate*grads['response_weight_dec_cross']\n    information_weight_dec_cross-=learning_rate*grads['information_weight_dec_cross']\n    attention_weight_dec_cross-=learning_rate*grads['attention_weight_dec_cross']\n\n    W1_dec-=learning_rate*grads['W1_dec']\n    b1_dec-=learning_rate*grads['b1_dec']\n    W2_dec-=learning_rate*grads['W2_dec']\n    b2_dec-=learning_rate*grads['b2_dec']\n\n    W_embed_out-=learning_rate*grads['W_embed_out']\n    b_embed_out-=learning_rate*grads['b_embed_out']\n\n# After training, you can use the decoder in inference mode by feeding\n# previously generated tokens (shifted) as input to the decoder and applying\n# the masked multi-head attention to predict the next token\nEpoch 1, Loss: 2.0795187664172397\nPredicted: banane coniglio banane banane &lt;start&gt; piacciono &lt;end&gt;\nEpoch 2, Loss: 2.0793373726502367\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 3, Loss: 2.0791564355830636\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 4, Loss: 2.078974718625546\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 5, Loss: 2.0787933552602227\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 6, Loss: 2.0786123118403346\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 7, Loss: 2.078431551323341\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 8, Loss: 2.078251032354273\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 9, Loss: 2.0780707083355354\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 10, Loss: 2.0778905181441116\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 11, Loss: 2.077710390742451\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 12, Loss: 2.077530273264538\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 13, Loss: 2.0773500940542484\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 14, Loss: 2.0771697955319044\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 15, Loss: 2.0769892772510743\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 16, Loss: 2.07680845041442\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 17, Loss: 2.0766272061537157\nPredicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;\nEpoch 18, Loss: 2.0764454984062057\nPredicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;\nEpoch 19, Loss: 2.076263321722837\nPredicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;\nEpoch 20, Loss: 2.0760807185682095\nPredicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;\n\n\n\nWell, after all of that, this poor of a performance is what we got? This is true. We were translating only one sequence, after all. Where transformers really excel is at scale, because of the differentiable attention matrix which can be parallelized. But you can see how it gets stuck depending on different learning rates and initialization. One of the difficulties is initialization to make gradient descent helpful. Xavier initialization could have worked, but it is much better to do it with numpy to get real understanding.\n\n\n\nThis aims to be an implementation of transformers that works, end-to-end, in numpy. No pointing to paper that blew up - Attention is All You Need (no, not it isn’t). No awkward discussions of ‘keys’, ‘values’, ‘queries’ (replaced with fundamental names instead). No list of mathematical formulas for different attention mechanisms with ‘choose what you want’. No describing the transformer architecture in a way that is essentially ‘just look at the figure’. No code snippets without explanation for the basics. For the last point, the final implementation is, in fact, annoying. The assumption is that you read this point from start to end. However there is something I have not mentioned at all. That something is automatic differentiation.\n\n\nYou may have noticed just how obnoxious implementing backpropagation was. Every single function had to be done manually. Was there a way we could have done it recursively? The answer is yes. This is called automatic differentiation. Automatic differentiation relies on the technique that all mathematical operations done by a computer are at the end pulled from basic arithmetic functions, since that’s what’s implemented on their circuit boards. Therefore, all computations of functions are inherently limited by this. Even though multiplication is not repeated addition, to a computer it is. Libraries exist entirely to implement backpropagation and automatic differentiation for user-defined functions. One such library is PyTorch. It implements automatic differentiation and abstracts it away, allowing users to define new functions and not worry about how backpropagation deals with that object. How would the transformer from scratch look like in PyTorch? :)"
  },
  {
    "objectID": "blog/2024-12-07-transformers/index.html#introduction",
    "href": "blog/2024-12-07-transformers/index.html#introduction",
    "title": "Transformers from scratch in numpy",
    "section": "",
    "text": "We address the following questions:\n\nWhat is the fundamental principle of a transformer?\nIn very general terms, what does a transformer do?\nWhat specific tasks do we need to do in order to implement a transformer?\n\n\n\nThe fundamental principle of a transformer is that one-hot vectors can be used to look up particular rows of a matrix, and you can exploit this to selectively extract, combine, and mask information from your input to produce better outputs1. A lot of readers, especially NLP enthusiasts, may immediately have a problem with this statement. After all, we have not used the most famous term associated with a transformer (attention) while stating this fundamental principle. We also did not say anything about feature embeddings, long-range dependencies, contextual relationships, and encodings - all terms that are used when talking/reading about transformers. This is done for two reasons. First, I believe that it is extremely important to understand what exactly is going on in terms of as many elementary operations as possible. I believe that this necessarily precludes using domain-specific jargon. Second, I am tired of reading innumerable blogs2, code comments on GitHub3, and slides that fail to give you understanding4.\n\n\n\nA transformer takes in a sequence of elements (this sequence is often long), figures out how different elements in that sequence are related, then squeezes that sequence into a list of numbers that capture any inherent ‘meaning’5. It then takes that list of numbers and unsqueezes it into a different sequence that tries to preserve the ‘meaning’ of the original sequence. As shown in the welcome section, transformers can be used for language translation.\n\n\n\n\nFigure out a way to split individual elements in a sequence (hereafter ‘words and punctuation in a sentence’) and find a way to feed them to the computer.\nMake the computer squeeze these words into a list of numbers it can understand.\nMake the computer unsqueeze the list of numbers into words and form sentences in another language."
  },
  {
    "objectID": "blog/2024-12-07-transformers/index.html#preliminaries",
    "href": "blog/2024-12-07-transformers/index.html#preliminaries",
    "title": "Transformers from scratch in numpy",
    "section": "",
    "text": "Suppose our input language only has four words (‘My’, ‘rabbit’, ‘likes’, ‘bananas’), and no punctuation at all. Sentences from our language could be ‘rabbit likes bananas’ or ‘My rabbit likes bananas’ or ‘likes My bananas’ or ‘bananas rabbit My’. For the sake of sanity, let’s assume that our language only has the sentence ‘My rabbit likes bananas’. We want to translate this into Italian: ‘Al mio coniglio piacciono le banane’. How do we feed our initial sentence to a computer?\n\n\nWe first need to split the sentence into individual words. Because our language does not have any sort of punctuation, we can do what is called whitespace tokenization. This is the most natural way of splitting an English sentence - assume that individual words are separated by a blank space, read through the sentence, and store all characters between two whitespaces as a single word 6.\nfrom typing import List #for clean function annotation\n\ndef whitespace_tokenizer(sentence: str) -&gt; List[str]:\n    \"\"\"\n    Function that reads a string/sentence and outputs a list of strings, where each output string is a word in that sentence. Each word is considered to be delimited by whitespaces.\n\n    Input:\n        sentence: str - assumed nonempty for explanation purposes\n    Output:\n        list of strings\n    \"\"\"\n    tokenized_sentence=[] #final output, a list of words\n    current_word=[] #list to store the current word\n\n    #The technique to whitespace tokenize the sentence is to iterate through it, and store each non-whitespace character in current_word. \n    #Once a whitespace is encountered, append the contents of current_word to tokenized_sentence and clear current_word\n    for i in sentence:\n        if i==\" \":\n            if current_word:\n                tokenized_sentence.append(''.join(current_word)) #append to the list of tokens\n                current_word=[]#reset current_word\n        else:\n            current_word.append(i)\n    \n    #this still leaves the final word in, so add it last\n    if current_word:\n        tokenized_sentence.append(''.join(current_word))\n    \n    #delete the current word from memory explicitly (not required)\n    del current_word\n\n    return tokenized_sentence\n\nenglish_sentence=\"My rabbit likes bananas\"\nprint(\"Your list of tokens for the English sentence is:\", whitespace_tokenizer(english_sentence))\nenglish_tokenized_sentence=whitespace_tokenizer(english_sentence) #save the tokenization in a list\n\nitalian_sentence=\"Al mio coniglio piacciono le banane\"\nprint(\"Your list of tokens for the Italian sentence is:\", whitespace_tokenizer(italian_sentence))\nitalian_tokenized_sentence=whitespace_tokenizer(italian_sentence)\nYour list of tokens for the English sentence is: ['My', 'rabbit', 'likes', 'bananas']\nYour list of tokens for the Italian sentence is: ['Al', 'mio', 'coniglio', 'piacciono', 'le', 'banane']\n\n\n\nHow do we feed these words into a computer? One way of doing it would be by assigning each individual word to a real number: [‘My’, ‘rabbit’, ‘likes’, ‘bananas’] -&gt; [‘935.88’, ‘-28124.4483957’, ‘3’, ‘-2’]. This is inefficient, as the amount of precision you would need to implement would increase computatational costs and storage requirements. A better way of storing a word would be to store it in a vector. Here is how we can do this. Given a vector, stored as a column matrix with NN rows, where NN is the number of words in your vocabulary, replace one of the zeros with 1 such that the position of the 1 is unique for that particular word. Then, stack those vectors side by side to form a matrix where each row and column has only one 1 and all other elements are zero. Such vectors are called ‘one-hot’ vectors and this is a type of encoding called one-hot encoding.\nThis is illustrated below:\n‘My’=$\n(0010)\\begin{pmatrix}\n0\\\\\n0\\\\\n1\\\\\n0\n\\end{pmatrix}\n,′rabbit′=, 'rabbit'=\n(0100)\\begin{pmatrix}\n0\\\\\n1\\\\\n0\\\\\n0\n\\end{pmatrix}\n,′likes′=, 'likes'=\n(1000)\\begin{pmatrix}\n1\\\\\n0\\\\\n0\\\\\n0\n\\end{pmatrix}\n,′bananas′=, 'bananas'=\n(0001)\\begin{pmatrix}\n0\\\\\n0\\\\\n0\\\\\n1\n\\end{pmatrix}\n,′Myrabbitlikesbananas′=,\n'My rabbit likes bananas'=\n(0010010010000001)\\begin{pmatrix}\n0 & 0 & 1 & 0\\\\\n0 & 1 & 0 & 0\\\\\n1 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n$\nLet’s call the last matrix WW, for ‘word matrix’. Observe that when any one-hot encoded is multiplied with another matrix, by the rules of matrix multiplication, the column of the second matrix corresponding to the position of the 1 in the column vector is ‘pulled out’7. This is illustrated below:\n$\n(0010)\\begin{pmatrix}\n0 & 0 & 1 & 0\n\\end{pmatrix}\n$ $\n(0.20.4−91+i291245.32853902i3223212430.4820.212241eπTREE(3)−TREE(3))\\begin{pmatrix}\n0.2 & 0.4 & -9 & 1+i \\\\\n29 & 12 & 45.328539 & 0 \\\\\n2i & 32 & 2 & 32 \\\\\n12 & 43 & 0.482 & 0.212 \\\\\n241 & e^{\\pi} & TREE(3) & -TREE(3)\n\\end{pmatrix}\n$ =(−945.32853920.482TREE(3))=\\begin{pmatrix}\n-9 \\\\\n45.328539 \\\\\n2 \\\\\n0.482\\\\\nTREE(3)\n\\end{pmatrix}\nIf you’re able to construct this second matrix, then it can potentially lead to something interesting8. There are of course other ways to encode words, and for practical language tasks you take someone else’s encoding and use it, but it is important to understand the core principle.\nimport numpy as np\nfrom typing import List, Dict, Tuple\nimport random\n\ndef create_concatenated_matrix_from_tokens(tokens: List[str]) -&gt; np.ndarray:\n    \"\"\"\n    Function that creates a concatenated one-hot encoded matrix from a tokenized sentence.\n    \n    Input:\n        tokens: List containing tokens\n    Output:\n        tokenized_matrix: A 2-D one-hot encoded np.ndarray of tokens. Each row and column contains only one 1. Always square.\n    \"\"\"\n\n    #The idea is to simply generate a diagonal matrix which will be one-hot encoded by definition\n    #Create a dictionary to map each token to a unique index\n    token_to_index={token: idx for idx, token in enumerate(tokens)} #get index-token pair from the input list\n    #Initialize the matrix with zeros\n    tokenized_matrix=np.zeros((len(tokens), len(tokens)), dtype=int)\n    #Populate the one-hot encoded matrix\n    for token in tokens:\n        index=token_to_index[token]\n        tokenized_matrix[index][index]=1  #Set the diagonal element to 1\n\n    return tokenized_matrix, token_to_index #this second variable is returned to pull out a random token and corresponding vector later\n\nenglish_onehot_matrix, english_token_to_index=create_concatenated_matrix_from_tokens(english_tokenized_sentence)\nprint(\"The English one-hot encoded matrix is:\\n\", english_onehot_matrix)\n\n#let's pull out a random token and a one-hot encoded vector to see how it pulls out specific features\ndef get_random_token_and_vector(token_to_index: Dict[str, int], one_hot_matrix: np.ndarray) -&gt; Tuple[str, np.ndarray]:\n    \"\"\"\n    Function that pulls a random token and its corresponding one-hot vector.\n\n    Input:\n        token_to_index: Dictionary mapping tokens to their indices\n        one_hot_matrix: 2D NumPy array containing one-hot vectors\n    Output:\n        A tuple containing the random token and its corresponding one-hot vector\n    \"\"\"\n    #Randomly select a token\n    random_token=random.choice(list(token_to_index.keys()))\n    \n    #Get the corresponding one-hot vector\n    one_hot_vector=one_hot_matrix[token_to_index[random_token]]\n    \n    return random_token, one_hot_vector\n\nrandom_token, corresponding_one_hot_vector=get_random_token_and_vector(english_token_to_index, english_onehot_matrix)\nprint(\"Let's pick a random token:\", random_token, \"\\nThe corresponding one-hot vector is:\", corresponding_one_hot_vector)\n\n#generate a random matrix to demonstrate pulling out certain columns/rows\nrandom_matrix=np.random.rand(len(english_tokenized_sentence), len(english_tokenized_sentence))\nprint(\"Multiplying an example random matrix\\n\", random_matrix, \"\\nby\", random_token+\"'s one-hot vector\", corresponding_one_hot_vector, \"\\npulls out the row\", np.matmul(corresponding_one_hot_vector, random_matrix), \"\\nand multiplying by the tranpose of that vector pulls out the column:\\n\", np.matmul(random_matrix, corresponding_one_hot_vector[:, np.newaxis]))\nThe English one-hot encoded matrix is:\n [[1 0 0 0]\n [0 1 0 0]\n [0 0 1 0]\n [0 0 0 1]]\nLet's pick a random token: bananas \nThe corresponding one-hot vector is: [0 0 0 1]\nMultiplying an example random matrix\n [[0.48871169 0.52574402 0.78833029 0.7045616 ]\n [0.76188795 0.13720883 0.39406852 0.02774654]\n [0.27090269 0.35964049 0.20715361 0.3064574 ]\n [0.23701048 0.67718606 0.87441259 0.05116356]] \nby bananas's one-hot vector [0 0 0 1] \npulls out the row [0.23701048 0.67718606 0.87441259 0.05116356] \nand multiplying by the tranpose of that vector pulls out the column:\n [[0.7045616 ]\n [0.02774654]\n [0.3064574 ]\n [0.05116356]]\n\n\n\nAn immediate application of this specific kind of matrix multiplication is as follows. Suppose we have the following sentence in our four-word language: ‘My rabbit’. Our task is to predict the next word that comes after it9. One easy way of doing this is by observing that we only have four options. We can construct the following four sentences: | Next Word | Potential next (possible incomplete) sentence | |—-|—-| |My|My rabbit My| |rabbit|My rabbit rabbit| |likes|My rabbit likes| |bananas|My rabbit bananas|\nHow do we decide what word comes next? Well, we can’t decide on our own. Perhaps, to an alien whose language consists of only four words that sound exactly like English words, the sentence ‘My rabbit My’ would translate to English as ‘I am in need of two oranges and a deck of playing cards.’ The sentence ‘My rabbit rabbit’ would translate to ‘I am on fire’. The sentence ‘My rabbit rabbit rabbit rabbit bananas bananas rabbit bananas My likes likes bananas’ would translate to ‘Yes’ (remember, I have not put any limits on the length of the sentences!). The point of these examples is to show you that there is no way for us to predict the next word unless we have some idea of what it is going to be. One way to solve this problem is for a third party (say a talking dog) to step in and say, “I’ve been around these aliens, and I’ve observed that whenever they begin a sentence with ‘My rabbit’, the next word is ‘bananas’ 10% of the time, ‘likes’ 85% of the time, ‘rabbit’ 5% of the time, but ‘My’ never comes after ‘rabbit’. Is there some way for you to use this information? Also, whatever I say is always true.”\nSince we have no better option, let’s trust the talking dog. We can in fact use its information in the following way. We can construct the following vector that shows the probability of predicting the next word after ‘rabbit’, if spoken by an alien.\n$\n\nbananaslikesrabbitMy[0.10.850.050]\\begin{aligned}\n&\\text{bananas}\\\\\n&\\text{likes}\\\\\n&\\text{rabbit}\\\\\n&\\text{My}\\\\\n\\end{aligned}\n\\quad\n\\begin{bmatrix}\n0.1 \\\\ 0.85 \\\\ 0.05 \\\\ 0\n\\end{bmatrix}\n\n$\nThis is somewhat useful. We know that there is a high chance that the next word in the sentence will be ‘likes’, so the possible incomplete sentence will now probably be ‘My rabbit likes’. But wait a minute. This vector of probabilities is like the column we pulled out of the matrix above. Is it possible to reconstruct this matrix? We can certainly do so - just assume that the dog is always true and start interrogating the dog about the probabilities of the next word after each word in the language, regardless of the context. Let’s assume the dog is happy to tell us this, so we now have the following matrix:\n$\n\nbananaslikesrabbitMybananas0.1000likes0.900.070.03rabbit0.10.850.050My0.20.010.790\\begin{array}{r|cccc}\n\\text{} & \\text{bananas} & \\text{likes} & \\text{rabbit} & \\text{My} \\\\\n\\hline\n\\text{bananas} & 0.1 & 0 & 0 & 0 \\\\\n\\text{likes}   & 0.9 & 0 & 0.07 & 0.03  \\\\\n\\text{rabbit}  & 0.1 & 0.85 & 0.05 & 0 \\\\\n\\text{My}      & 0.2 & 0.01 & 0.79 & 0\\\\\n\\end{array}\n\n$\nThe matrix is read row-first, column-second i.e. the probability that the word ‘likes’ occurs after ‘rabbit’ is 0.85.\nThis tells us something about how the language is constructed. We know that if we hear an alien say ‘rabbit’, there is a very high chance that it will say ‘likes’ next. If we hear it say ‘likes’, there is a very high chance it will say ‘bananas’ next. There is also a very small chance it will say ‘My’ after ‘likes’, but it will never say ‘likes’ after ‘likes’. Therefore, to a first order, we can construct this matrix of probabilities that tells us what the next word in the language is going to be. In more formal terms, this is the stochastic matrix of a first-order Markov chain. It is first-order because the next word in the language only depends on the current word of the language.\nBut wait. Languages tend to have meaning when several words are used together. For example, in English, the word ‘cold’ refers to something whose molecules have a lower average kinetic energy than a reference object. However, the phrase ‘cold call’ means unsolicited phone calls typically made for business purposes. If you only know that the current word in the sentence is ‘cold’ and your probability matrix says that the word ‘call’ appears after ‘cold’ 70% of the time, you may say that the sentence ‘The water is cold.’ is incomplete and would complete it by saying ‘The water is cold call.’, which makes no sense10. What do we do?\nThe natural approach is to say, “I know combinations of words tend to change the meaning of a phrase11, but I don’t have any idea what constitutes a phrase in my unknown language, nor do I know if the ‘meaning’ of the sentence itself changes if a two words are present in adjacent positions12. Let me do the same thing I did for my first-order Markov chain. Instead of asking the talking dog the probabilities of the next word after my current word, I will look at the probabilities of the next word after my current word if another word is present in the sentence.”\nSpecifically, you can ask the talking dog the questions “If ‘rabbit likes’ is present in the sentence, what is the probability that the next word is ‘bananas’? What about ‘rabbit’, ‘My’, and ‘likes’? If ‘My rabbit’ is present, what is are the probabilities for the next word?” and construct the same matrix as we did above. Since we are looking at every pair of words, the number of rows of the matrix quickly grows in size. If there are 5 words in the language, the number of two-word pairs is 20 (obtained from $ 5 $) since the order matters. If there are 100 words, there are 4950 pairs. If there are 260,000 words (a quick Google search tells me that this is roughly the number of words in Italian) then there are 33799870000 pairs. And this is just for consecutive word pairings! If we attempt to look even further back i.e. three-word pairs, there will be even more. It is easy to see that the amount of space required to store this prediction matrix grows exponentially13.\n$\n\nbananaslikesrabbitMylikes bananas0TREE(4)TREE(5)00.02My rabbit00.826TREE(3)TREE(4)0.03rabbit likes0.0246820.5π10e0My bananas0.00400.120.0018256151and so on..\\begin{array}{r|cccc}\n\\text{} & \\text{bananas} & \\text{likes} & \\text{rabbit} & \\text{My} \\\\\n\\hline\n\\text{likes bananas} & 0 & \\frac{TREE(4)}{TREE(5)} & 0 & 0.02 \\\\\n\\text{My rabbit}   & 0 & 0.826 & \\frac{TREE(3)}{TREE(4)} & 0.03  \\\\\n\\text{rabbit likes}  & 0.024682 & 0.5 & \\frac{\\pi}{10e} & 0 \\\\\n\\text{My bananas}      & 0.004 & 0 & 0.12 & 0.0018256151\\\\\n\\text{and so on..}\n\\end{array}\n\n$\nGiven a sufficiently large prediction matrix containing all possible words and combinations of all possible lengths, we are able to predict the next word. Note that we have not said anything about actually choosing the next word in this situation, as this leads to problems. One problem is that we still do not know how to deal with cases where there is an equal chance of two words appearing after our current word. Let’s ignore this for now and focus on the biggest one: We want to avoid actually constructing any such matrix. Let’s try another trick. Let’s say, “The next word in a sentence is easier to predict if another word appears before the current word, but not necessarily directly before it. It may happen sometimes, but there is no reason why it should be like this. Here is my hypothesis. I think that it is easier to predict the next word in the sentence given a probability matrix containing all possible combinations of words where the second word is the current word.” This would look something like:\n$\n\nbananaslikesrabbitMylikes bananas0000.01My bananas0000.03rabbit bananas0.10.00000023eπ0bananas bananas0.0040.0468260.120.5151and so on...\\begin{array}{r|cccc}\n\\text{} & \\text{bananas} & \\text{likes} & \\text{rabbit} & \\text{My} \\\\\n\\hline\n\\text{likes bananas} & 0 & 0 & 0 & 0.01 \\\\\n\\text{My bananas}   & 0 & 0 & 0 & 0.03  \\\\\n\\text{rabbit bananas}  & 0.1 & 0.00000023 & \\frac{e}{\\pi} & 0 \\\\\n\\text{bananas bananas}      & 0.004 & 0.046826 & 0.12 & 0.5151\\\\\n\\text{and so on...}\n\\end{array}\n\n$\nThis is much better to work with. Note that this is no longer a representation of a Markov chain, as we cannot simply look at the row corresponding to the current word and predict the next one. What can we do instead? We can say: “Okay, let’s say that these probabilities represent how much these pairs contribute to the next word in the sequence. We call these probabilities as votes and to predict the next word, we can sum over each column and compare these sums to determine the next word.” This is good, because now we are capturing long-range/skip dependencies in the language/sequence. Each row now represents one of many features that can describe our sequence at a particular point.\nThis is more clearly illustrated when you have, say, only two possible sentences in the language, but the main takeaway from actually doing this task for a set vocabulary and finite amount of sentences in the language is the observation that many elements in this probability matrix do not matter. They can either be so small that they are practically zero, or something like 0.5, which means that the next word is equally likely to appear regardless of the sentence, so it may not matter too much. What we are really interested in are elements we can distinguish. For example, suppose that the two sentences that were possible in our language were ‘My rabbit likes bananas’ and ‘My bananas likes rabbit’. If we had the incomplete sentence ‘My rabbit likes’, then we could ask the talking dog to give us this matrix, and what we would see is that the matrix has a large number of zeros but a 1 for ‘bananas’, enabling us to do this sum-over-columns technique to accurately predict the next word, even with a deep dependency. To be fair, this example is a bit contrived and longer sentences would illustrate the point much more easily.\nThis is still pretty bad. Real languages have a large number of words. Our talking dog could have only been around aliens who lived on a certain continent of the alien planet, which led to them developing their own dialect. If you think about it for just a little bit, it is easy to see that this sum-over-columns approach can end up telling us that the next word in the incomplete sentence ‘Japan is east of’ can be ‘China’, with a vote total of 2339, and ‘Mongolia’, with a vote total of 2340. Sure, we can still pick ‘Mongolia’ as the next word, but such a small difference can naturally be induced by statistical noise, unknowingly biased probability matrices, and other factors (such as us messing up the addition!). Are there ways to overcome this?\nOne approach is to modify the values in the columns before you sum them up, in a way that allows us to differentiate between them even more. One way to do this is to simply sum all the values and divide each value by the sum, to get a fractional representation. This is not very helpful - it preserves the same relation between the numbers in terms of scaling. Converting a column of [1,2,3] to [0.1666, 0.3334, 0.5] preserves the scaling. To overcome this, we utilize the independence from irrelevant alternatives axiom of decision theory, which states that irrelevant choices should not affect the relative probability of choosing between the things you really want to choose between. In mathematical terms, this means that if you have a set of numbers x∈Xx \\in X and you want to decide between x1x_1 and x2x_2 but x3...xnx_3...x_n are small values that are affecting your confidence, you can suppress x3...xnx_3...x_n by replacing each variable in the following way:\nxi→exi∑i=1i=nexix_i\\rightarrow\\frac{e^{x_i}}{\\sum_{i=1}^{i=n}e^{x_i}}\nThis is the famous softmax function which is more or less used to convert a probability distribution to another probability distribution14. The important thing is that the softmax function suppressed irrelevant values (as ek→1e^k \\rightarrow 1 as k→0k \\rightarrow 0).\nHowever, the softmax function is also not applicable to our scenario. Suppose we did actually end up converting the votes to a probability distribution and summing them. What would it actually look like? Let’s do an example below:\n$\n(00.510)→(0.157060.2589480.4269330.15706)\\begin{pmatrix}\n0 \\\\ 0.5 \\\\ 1 \\\\ 0\n\\end{pmatrix}\n\\rightarrow\n\\begin{pmatrix}\n0.15706 \\\\ 0.258948 \\\\ 0.426933 \\\\ 0.15706\n\\end{pmatrix}\n$\nThe sum of the softmaxed vector elements is 1. This is correct, because we did just convert it to a probability distribution. So this approach, while it did ‘suppress’ the smaller values, does not actually help us with voting. What can we do?\n“Okay,” we say. “Let’s do something else. Instead of attempting to modify every value, let’s just discard the values that aren’t important15. First, let’s look at how to extract specific features from our matrix. We know one-hot encoded vectors pull relevant rows/columns out of the matrix, so let’s make a one-hot encoded vector to pull out the relevant features in the matrix in the following way. We construct a vector initially filled with zeros featuring all possible pairs in sentence where the second word is the current word, and the first word has all other words (possibly including the current word, depending on the dimensions of our matrix). Then, if the first word appears before the current word in the sentence, set that element to 1. This vector allows us to pull out the features of our probability matrix that are ‘active’ until that current point.”\nThis would look like:\n$\n\nMy likesrabbit likesbananas likes(110)\\begin{aligned}\n&\\text{My likes}\\\\\n&\\text{rabbit likes}\\\\\n&\\text{bananas likes}\\\\\n\\end{aligned}\n\\quad\n\\begin{pmatrix}\n1 \\\\ 1 \\\\ 0\n\\end{pmatrix}\n^\n$ $\n\nbananaslikesrabbitMyMy likes1000rabbit likes1000.03bananas likes10.00000023eπ0and so on...\\begin{array}{r|cccc}\n\\text{} & \\text{bananas} & \\text{likes} & \\text{rabbit} & \\text{My} \\\\\n\\hline\n\\text{My likes} & 1 & 0 & 0 & 0 \\\\\n\\text{rabbit likes}   & 1 & 0 & 0 & 0.03  \\\\\n\\text{bananas likes}  & 1 & 0.00000023 & \\frac{e}{\\pi} & 0 \\\\\n\\text{and so on...}\n\\end{array}\n\n$\nNote the transpose sign. We can see that the matrix multiplication will suppress those elements in the pulled out feature vectors where pairs taking into account words appearing after the current word in the sequence will be suppressed i.e. we cannot use knowledge of the entire sentence to predict the next sentence. We have now ‘suppressed the future’, but we still need to figure out what feature elements in our sequence are important. This is still an unknown, but what we can do is use another one-hot encoded vector to multiply this suppressed vector, to suppress even more. That is: we can compute the pairwise product to return a vector after multiplying our two vectors. Where can we get this second one-hot encoded vector? Let’s assume that the talking dog gave this to us. The point is that if we manage to suppress information then our voting becomes much stronger, as a lot of elements will be 0. The trick is now to find out how to create this second vector so that we suppress irrelevant information.\nIncidentally, the second form of suppression is the idea behind attention.\nimport numpy as np\nfrom typing import List, Dict, Tuple\nimport random\n\n#remember that the english sentence is \"My rabbit likes bananas\"\ndef generate_biased_probability_matrix(size: int) -&gt; np.ndarray:\n    \"\"\"\n    Function to generate a square probability matrix where each row and column \n    has one value significantly higher than the others.\n\n    Input:\n        size: The number of rows and columns in the square matrix\n    Output:\n        biased_matrix: A 2-D np.ndarray where each row and column has one high-probability value\n    \"\"\"\n    #the technique is to generate a uniform matrix and randomly assign biased high probability values in each row and column\n    biased_matrix=np.random.uniform(0.01, 0.05, (size, size))\n    \n    #generate high probability values for each row and column\n    high_probabilities=np.random.uniform(0.7, 0.9, size=size)\n    \n    #ghuffle indices to randomly distribute the high probabilities across columns\n    indices=np.arange(size)\n    np.random.shuffle(indices)\n    \n    #assign one high probability per row and column\n    for i in range(size):\n        biased_matrix[i, indices[i]]=high_probabilities[i]\n    \n    #normalize each row to sum to 1\n    biased_matrix=biased_matrix/biased_matrix.sum(axis=1, keepdims=True)\n    \n    return biased_matrix\n\nsize=4\nexample_probability_matrix=generate_biased_probability_matrix(size)\nprint(\"As an example, a second-order probability matrix with skip dependencies given to us by the talking dog can be this:\")\nprint(example_probability_matrix)\n\n\ndef softmax(numbers: List[int])-&gt;List[int]:\n    \"\"\"\n    Function to softmax a set of numbers\n    Input:\n        numbers: a list of integers\n    Output:\n        The list, softmaxed\n    \"\"\"\n\n    exponential_list=np.exp(numbers)\n    softmaxed_numbers=[np.exp(number)/sum(exponential_list) for number in numbers]\n    return softmaxed_numbers\n\n\ndef digram_one_hot_encoding(sentence: str, tokens: List[str], index_of_word: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate one-hot encoding vectors for digrams based on a user-defined index.\n\n    Input:\n        sentence: Original sentence (used for context if needed).\n        tokens: List of words (tokens) in the sentence.\n        index_of_word: Index of the target word in the tokens list. Zero-indexed\n\n    Output:\n        A tuple containing:\n            - A NumPy array of digrams (other words paired with the target word).\n            - A 1D NumPy array where each element is 1 if the other word appears before the target word, 0 otherwise.\n    \"\"\"\n    if index_of_word&lt;0 or index_of_word&gt;=len(tokens):\n        raise ValueError(\"Invalid user-defined index. Must be within the range of the tokens list.\")\n\n    #extract the target word. This of course assumes that the tokenization is sequential, but for illustrative purposes, it is fine\n    target_word = tokens[index_of_word]\n\n    #generate digrams and the one-hot vector. The idea is that if the word appears before our word then set the index to 1, else 0\n    digrams=[f\"{token},{target_word}\" for idx,token in enumerate(tokens) if idx!=index_of_word]\n    one_hot_vector=np.array([1 if idx&lt;index_of_word else 0 for idx in range(len(tokens)) if idx!=index_of_word],dtype=int)\n\n    return np.array(digrams), one_hot_vector\n\nprint(\"Our sentence is:\", english_sentence)\n\n#lets take index 2 ('likes' in \"My rabbit likes bananas\")\nindex_of_word=2\nincomplete_sentence=\" \".join([word for index,word in enumerate(english_tokenized_sentence) if index&lt;index_of_word+1])\nprint(\"We want an incomplete sentence. Our generation task is to predict the next word in:\", incomplete_sentence)\n#generate the digrams (word pairs)\ndigrams,digram_onehot_vector=digram_one_hot_encoding(english_sentence,english_tokenized_sentence,index_of_word)\nprint(\"Digrams:\")\nprint(digrams)\nprint(\"One-hot vector (without any future dependency):\")\nprint(digram_onehot_vector)\n\n#next, we create the attention mask by hand. specifically, we generate a ones vector equal to the size of the number of words in our sentence\n#then we randomly pick 2\nattention_mask=np.ones((len(whitespace_tokenizer(incomplete_sentence))), dtype=int)\nzero_indices = np.random.choice(len(whitespace_tokenizer(incomplete_sentence)), size=random.randrange(0,len(whitespace_tokenizer(incomplete_sentence))), replace=False)\nattention_mask[zero_indices]=0\nprint(\"Example attention mask: \", attention_mask.T)\n\nprint(\"Attention applied to the non-future dependency capturing one-hot vector:\", attention_mask*digram_onehot_vector)\nAs an example, a second-order probability matrix with skip dependencies given to us by the talking dog can be this:\n[[0.03888508 0.03133871 0.88147493 0.04830127]\n [0.89819378 0.03108927 0.03605569 0.03466125]\n [0.02927141 0.88815681 0.02795252 0.05461926]\n [0.01763851 0.04167578 0.05967272 0.881013  ]]\nOur sentence is: My rabbit likes bananas\nWe want an incomplete sentence. Our generation task is to predict the next word in: My rabbit likes\nDigrams:\n['My,likes' 'rabbit,likes' 'bananas,likes']\nOne-hot vector (without any future dependency):\n[1 1 0]\nExample attention mask:  [0 0 1]\nAttention applied to the non-future dependency capturing one-hot vector: [0 0 0]\n\n\n\nWe have so far our non-future dependent feature vector. We have used it so far in conjunction with the probability matrix to predict the next step. If we want to suppress the feature vector with attention, does it make sense to use another matrix in the same way? Let’s assume that we have a bunch of attention masks/vectors. We can stack them either vertically or horizontally (depending on how exactly we want to implement our lookup) and generate a matrix of attention masks. We can then send our feature vector through the attention matrix and then send the result of that product into our probability matrix to predict the next word.\ndef send_vector_through_two_matrices(vector: np.ndarray, probability_matrix: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Function to send an input vector through an attention matrix and then a probability matrix\n    Inputs:\n        vector: a 1D NumPy ndarray\n    Output:\n        a 1D NumPy ndarray of the same length as the input after being sent through two matrices\n    \"\"\"\n\n    #the idea is to generate a 1d array of ones, replace half of the elements with 0, and shuffle and reshape it\n\n    print(\"Your input vector is:\\n\", vector)\n\n    total_elements=vector.shape[0]**2\n    half=total_elements//2\n    flat=np.ones(total_elements, dtype=int)\n    flat[:half]=0\n    np.random.shuffle(flat)\n    example_attention_matrix=flat.reshape((vector.shape[0],vector.shape[0]))\n\n    print(\"An example attention matrix is:\\n\", example_attention_matrix)\n\n    result_1=np.matmul(vector, example_attention_matrix)\n    print(\"After multiplying, you get:\\n\",result_1)\n\n    print(\"After multiplying the result with the probability matrix, you get\", np.matmul(result_1, probability_matrix))\n\nexample_probability_matrix=np.random.rand(len(digram_onehot_vector), len(digram_onehot_vector))\nprint(\"An example probability matrix is:\\n\", example_probability_matrix)\n\nsend_vector_through_two_matrices(digram_onehot_vector, example_probability_matrix)\nAn example probability matrix is:\n [[0.6600921  0.31488209 0.01236613]\n [0.51302175 0.04638934 0.92027961]\n [0.38042472 0.01756476 0.29279993]]\nYour input vector is:\n [1 1 0]\nAn example attention matrix is:\n [[1 1 0]\n [1 0 1]\n [0 0 1]]\nAfter multiplying, you get:\n [2 1 1]\nAfter multiplying the result with the probability matrix, you get [2.21363067 0.69371827 1.2378118 ]\n\n\n\nWhat do we do with the result of the attention step? Sure, we have a vector that has encoded word pairs (a second-order model), but we don’t yet have a way to deconstruct that vector back into a word pair. How do we do this? So far, matrix multiplication has enabled us to encode sentences into vectors and selectively mask the irrelevant word pairs. Can we apply matrix multiplication to decode a word pair? The answer is yes. Matrix multiplications are in fact what neural networks do.\n\n\nNeural networks are a deep learning architecture based on the neuron-synapse structure of the human brain. Neural networks consist of a series of blocks called artificial neurons (hereafter just ‘neuron’) stacked vertically in layers. Each neuron has the possibility to receive an input and pass along an output to another neuron. To decide whether it passes along an output, a neuron sums up all of its inputs (which are weighted by the value of the connection along which the output travels) and applies a function, called an activation function, to that sum. Depending on the result of the activation function, the neuron sends an output to one or more neurons depending on how many it connects to. Mathematically, passing data through a neuron is equivalent to applying the mathematical function f(∑i=0i=nwixi)f(\\sum_{i=0}^{i=n} w_i x_i), where ff is the activation function, wiw_i is the weight along an input path, and xix_i the actual value being sent along that input path.\nNeural networks are equivalent to matrix multiplication. Why is this so? Suppose there are two layers in our neural network. The first layer has 3 neurons, and the second layer has two neurons. Let’s name the latter two n1n_1 and n2n_2. Also, let’s assume that each neuron in the first layer sends is connected to each neuron in the second layer. Therefore, the outputs of the first layer are x1,x2,x3x_1,x_2,x_3 and the weights of the paths along which they are sent are w11,w12,w21,w22,w31,w32w_{11}, w_{12}, w_{21}, w_{22}, w_{31}, w_{32} for each neuron-neuron path.\nThe input to n1n_1 is w11x1+w21x2+w31x3w_{11}x_1+w_{21}x_2+w_{31}x_3, and the input to n2n_2 is w12x1+w22x1+w32x3w_{12}x_1+w_{22}x_1+w_{32}x_3. Writing these out in the form of a system of linear expressions:\n$$w_{11}x_1+w_{21}x_2+w_{31}x_3\\\\w_{12}x_1+w_{22}x_1+w_{32}x_3$$\nit is easy to see that this is in fact a matrix multiplication:WXWX, where W=(w11w21w31w21w22w32),X=(x1x2x3)W=\\begin{pmatrix}w_{11}&w_{21}&w_{31}\\\\w_{21}&w_{22}&w_{32}\\end{pmatrix}, X=\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix}. Each layer also tends to have a bias term, so the input to a layer can be represented as the equation WX+BWX+B, where BB is the bias matrix (usually a column vector containing the same value). The activation function is applied to this resultant vector. This means that the output of a layer of a neural network can be represented by a vector.\n#### Properties of neural networks\n\nNeural networks are universal function approximators. This means that given a large-enough network with nonlinear activation functions, neural networks can model any mapping between elements of a domain XX and a codomain YY. However, this does NOT tell us how many neurons and layers we need or what the activation function is.\nNeural networks can model nonlinear relationships between elements. While the discussion of linear decision boundaries is beyond the scope of this tutorial, it is enough to know that ff, the activation function, is usually chosen to be something like ReLU or the sigmoid function.\nSince neural networks are just matrix multiplication, they are extremely fast to train on computers16\n\n#### Activation functions\nIf the activation function is linear (such as a simple multiplier f(x)=2xf(x)=2x), the neural network cannot learn linear relationships, no matter how big you make it and how long you train it for. Nonlinear activation functions are necessary to learn nonlinear relationships i.e. relationships between two variables that cannot be explained by a matrix multiplication (attention is linear!). Activation functions like ReLU and the sigmoid function are chosen not only because they are easy to compute, but because of a certain requirement explained below.\n\n\n\nSince each layer of a neural network can be expressed as a function ff, a neural network can be thought of as a large composite function f1(W1f2(...fn(WnXn+BN)...+B1)+B0)f_1(W_1f_2(...f_n(W_nX_n+B_N)...+B_1)+B_0). Given a training set of (xi,yi){(x_i,y_i)} pairs, the loss of the model is a cost function C(yi,g(xi))C(y_i,g(x_i)) where g(xi)g(x_i) is the prediction of the neural network (the large composite function defined above) for the input variable xix_i. We want to minimize this cost function, as it means our neural network has learned the relationship between the input and output variables. This is done with an algorithm called backpropagation.\nBackpropagation is an algorithm that utilizes the technique of gradient descent - given a cost function, we calculate its gradient with respect to the weights and biases of the neural network. According to the learning rate aa, gradient descent updates the weights and biases of the neural network according to the rule W/Bnext=W/Bcurrent−α∂C(X,W/Bcurrent)∂W/B\nW/B_{\\text{next}}=W/B_{\\text{current}}-\\alpha\\frac{\\partial C(X,W/B_{\\text{current}})}{\\partial W/B}\n\nwhere // is read as ‘OR’. We subtract the gradient because the gradient denotes the direction of maximum increase, so the direction of maximum decrease would be the direction opposite to it. We apply this many times (this is therefore a greedy algorithm) to find the local minimum of the function i.e. the values of all WW and all BB where the cost function is minimized. After each step of updating the gradients, we have to compute the prediction of the network again, in order to prepare for the next step. This is called the forward pass or forward step through the network, and must be computed repeatedly, making the process a back-and-forth.\n\n\n\nLet us consider a neural network set up in the following way. We have 3 input neurons (that is, 3 input variables) and 1 hidden layer with 4 neurons, and one output neuron. For the sake of this example, assume that every neuron in one layer is connected to every neuron in the next layer and every neuron in the previous layer. Such a neural network is called a fully-connected neural network.\nWe have already seen how matrices can represent the input to a layer. Let’s represent the output of a layer by a vector after an activation function is applied to each neuron17. We will now define several variables that mathematically represent each layer. For each layer ll, we have:\n$$ n_l \\\nw_l n_{l} n{l-1} \\\nb_l n_l \\\na_l \\\nz_l a_l b_l \\\ng_l $$\nWe can write down some straightforward formluae after these definitions.\n$$ z_l=w_la_l+b_l\\\na_l=g_l(z_l)\\ $$\nThe next question is choosing an appropriate cost function for our task. Let us think about our task for a moment. Since we have been using probabilities all along to predict the next word in our sequence, it is appropriate to use a cost function that tells us how good our probability prediction is. The classical cost function that is used to explain backpropagation is the squared error function, (real value−predicted value)2(\\text{real value}-\\text{predicted value})^2. This function is natural because it is simply the difference between what we predict and what the truth is, and it is squared for many reasons such as being the variance of the unbiased estimator (if used in its mean-square form) and also being easily differentiable18. But we effectively want to measure the difference between a predicted probability distribution and the real probability distribution, as we are predicting the next word in a sentence. This requires having a maximum likelihood estimate of the parameters, and when working with Bernoulli-distributed variables (such as one-hot encoded vectors) the cross-entropy loss function −(ylnŷ+(1−y)ln(1−ŷ))-(yln\\hat{y}+(1-y)ln(1-\\hat{y})) minimizes the maximum likelihood estimate19.\nLet’s see how the derivative is calculated.\n$$ = \\[5pt]\n= $$\nby a simple application of the chain rule. This can easily be extended by observing that z3z_3 is a function of a2a_2, which is a function of z2z_2, and z2z_2 is a function of w2,b2,a1w_2, b_2, a_1.\n$$ = \\[5pt]\n= $$\nand similarly for the first (input) layer, named layer 0.\nThe next task is setting up a way to recursively calculate the derivative of the cost function for any arbitrary layer’s weight and bias. The general equation for this is\n$$\n\\frac{\\partial C}{\\partial w_l}=\\frac{\\partial C}{\\partial z_l}\\frac{\\partial z_l}{\\partial w_l}\\\\[5pt]\n\\frac{\\partial C}{\\partial b_l}=\\frac{\\partial C}{\\partial z_l}\\frac{\\partial z_l}{\\partial b_l}\n$$\nThere are two observations we can make from this. The first is that it is straightfoward to numerically calculate the partial derivative for the last/output layer, and we can store this value in order to avoid repeated computation wherever possible. The second is that you need to calculate the change in the gradient for the last layer, then use that changed gradient for the layer before that one, and so on, ‘back-propagating’ the errors.\nLet’s choose a nice activation function such as the sigmoid function σ(x)=11+e−x\\sigma(x)=\\frac{1}{1+e^{-x}} for this. Let us precompute the partial derivative of the output layer, since we’ll be needing it. Note that ⊙\\odot denotes the Hadamard, or element-wise product.\n$$ == ’(z_o) \\[5pt]\n=-(-) \\[5pt]\n’(z_o)=a_l(1-a_l) \\[5pt]\ny \\[5pt]\n=a_o-y $$\nThis is a very nice result. It is now easy to see that for any inner layer, we can repeatedly apply the chain rule to derive the partial derivatives. If you go about doing this you end up with the following results:\n$$ =w_{l+1}^T ’(z_l) \\[5pt]\n=a_{l-1} \\[5pt]\na_{l-1}^T \\[5pt]\n=1 \\[5pt]\n= $$\nA more complete derivation can be found here20, but the fundamental idea is the same.\nWe can now implement this in Python and train the neural network from scratch.\n\"\"\"\nNeural Network Implementation in NumPy\nInputs:\n    None\nOutputs:\n    Fully functional neural network trained on synthetic data\n\"\"\"\n\nimport numpy as np\n\ndef sigmoid(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Apply the sigmoid activation function element-wise\n    Inputs:\n        x: a NumPy ndarray, the input array\n    Outputs:\n        a NumPy ndarray with sigmoid applied element-wise\n    \"\"\"\n    return 1/(1+np.exp(-x))#sigmoid formula\n\ndef sigmoid_prime(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute the derivative of the sigmoid function element-wise\n    Inputs:\n        x: a NumPy ndarray, the input array\n    Outputs:\n        a NumPy ndarray with the derivative of sigmoid applied element-wise\n    \"\"\"\n    return sigmoid(x)*(1.0-sigmoid(x))#sigmoid derivative formula\n\nclass NeuralNetwork:\n    \"\"\"\n    Define a simple feedforward neural network\n    \"\"\"\n\n    def __init__(self,architecture: np.ndarray):\n        \"\"\"\n        initializer for the neural network class\n        Inputs:\n            architecture: a NumPy array representing the number of neurons in each layer\n        \"\"\"\n        self.L=architecture.size-1#number of layers (excluding input layer)\n        self.n=architecture#number of neurons in each layer\n        self.parameters={}#dictionary to store weights, biases, and activations\n\n        #initialize weights and biases for each layer\n        for i in range(1,self.L+1):\n            self.parameters['W'+str(i)]=np.random.randn(self.n[i],self.n[i-1])*0.01#small random weights\n            self.parameters['b'+str(i)]=np.ones((self.n[i],1))#biases initialized to 1\n            self.parameters['z'+str(i)]=np.ones((self.n[i],1))#pre-activation values initialized to 1\n            self.parameters['a'+str(i)]=np.ones((self.n[i],1))#activations initialized to 1\n        \n        self.parameters['a0']=np.ones((self.n[0],1))#input layer activation\n        self.parameters['C']=1#placeholder for cost value\n        self.derivatives={}#dictionary to store derivatives\n\n    def forward_propagate(self,X: np.ndarray):\n        \"\"\"\n        Perform forward propagation\n        Inputs:\n            X: a column vector representing one training example\n        Outputs:\n            None\n        \"\"\"\n        self.parameters['a0']=X#set input layer activation\n        for l in range(1,self.L+1):\n            self.parameters['z'+str(l)]=np.dot(self.parameters['W'+str(l)],self.parameters['a'+str(l-1)])+self.parameters['b'+str(l)]#W*a+b\n            self.parameters['a'+str(l)]=sigmoid(self.parameters['z'+str(l)])#apply sigmoid activation\n\n    def compute_cost(self,y: np.ndarray):\n        \"\"\"\n        function to compute the cost for one training example\n        Inputs:\n            y: the true label for the input sample\n        Outputs:\n            None\n        \"\"\"\n        self.parameters['C']=-(y*np.log(self.parameters['a'+str(self.L)])+(1-y)*np.log(1-self.parameters['a'+str(self.L)]))#binary cross-entropy loss\n\n    def compute_derivatives(self,y: np.ndarray):\n        \"\"\"\n        function to compute gradients for all parameters\n        Inputs:\n            y: the true label for the input sample\n        Outputs:\n            None\n        \"\"\"\n        self.derivatives['dz'+str(self.L)]=self.parameters['a'+str(self.L)]-y#last layer gradient\n        self.derivatives['dW'+str(self.L)]=np.dot(self.derivatives['dz'+str(self.L)],self.parameters['a'+str(self.L-1)].T)#last layer weights gradient\n        self.derivatives['db'+str(self.L)]=self.derivatives['dz'+str(self.L)]#last layer bias gradient\n\n        for l in range(self.L-1,0,-1):\n            self.derivatives['dz'+str(l)]=np.dot(self.parameters['W'+str(l+1)].T,self.derivatives['dz'+str(l+1)])*sigmoid_prime(self.parameters['z'+str(l)])#hidden layer gradient\n            self.derivatives['dW'+str(l)]=np.dot(self.derivatives['dz'+str(l)],self.parameters['a'+str(l-1)].T)#hidden layer weights gradient\n            self.derivatives['db'+str(l)]=self.derivatives['dz'+str(l)]#hidden layer bias gradient\n\n    def update_parameters(self,alpha: float):\n        \"\"\"\n        function to update network parameters using gradient descent\n        Inputs:\n            alpha: learning rate\n        Outputs:\n            None\n        \"\"\"\n        for l in range(1,self.L+1):\n            self.parameters['W'+str(l)]-=alpha*self.derivatives['dW'+str(l)]#update weights\n            self.parameters['b'+str(l)]-=alpha*self.derivatives['db'+str(l)]#update biases\n\n    def predict(self,x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        function to predict the output for a given input\n        Inputs:\n            x: a column vector representing one input sample\n        Outputs:\n            a NumPy array representing the predicted output\n        \"\"\"\n        self.forward_propagate(x)#perform forward propagation\n        return self.parameters['a'+str(self.L)]#return output layer activation\n\n    def fit(self,X: np.ndarray,Y: np.ndarray,num_iter: int,alpha: float=0.01):\n        \"\"\"\n        function to train the neural network\n        Inputs:\n            X: a NumPy array where each row is a training example\n            Y: a NumPy array of true labels\n            num_iter: number of iterations\n            alpha: learning rate\n        Outputs:\n            None\n        \"\"\"\n        for iter in range(num_iter):\n            c=0#cumulative cost\n            n_c=0#correct predictions count\n\n            for i in range(X.shape[0]):\n                x=X[i].reshape((X[i].size,1))#reshape input to column vector\n                y=Y[i]#true label\n                self.forward_propagate(x)#forward propagation\n                self.compute_cost(y)#compute cost\n                self.compute_derivatives(y)#compute gradients\n                self.update_parameters(alpha)#update parameters\n                c+=self.parameters['C']#accumulate cost\n                y_pred=self.predict(x)#make prediction\n                y_pred=(y_pred&gt;0.5)#convert probability to binary\n                if y_pred==y:\n                    n_c+=1#correct prediction count\n\n            c=c/X.shape[0]#average cost\n            print('Iteration:',iter)\n            print(\"Cost:\",c)\n            print(\"Accuracy:\",(n_c/X.shape[0])*100)\n\n#generate synthetic data\nnp.random.seed(42)#reproducibility\nX=np.random.rand(200,7)#200 samples, 7 features\ny=(np.sum(X,axis=1)&gt;3.5).astype(int).reshape(200,1)#labels based on sum of features\n\n#split data into training and testing sets\nsplit_ratio=0.7#70% training data\nsplit_index=int(X.shape[0]*split_ratio)\nindices=np.arange(X.shape[0])\nnp.random.shuffle(indices)\n\nX_train,X_test=X[indices[:split_index]],X[indices[split_index:]]\ny_train,y_test=y[indices[:split_index]],y[indices[split_index:]]\n\n#define architecture\narchitecture=np.array([7,5,1])#7 input features, 5 hidden neurons, 1 output\n\n#initialize and train the neural network\nnn=NeuralNetwork(architecture)\nnn.fit(X_train,y_train,num_iter=50,alpha=0.1)\n\n#evaluate the model\ncorrect_predictions=0\nfor i in range(X_test.shape[0]):\n    x=X_test[i].reshape((X_test[i].size,1))#reshape to column vector\n    y_true=y_test[i]#true label\n    y_pred=nn.predict(x)#prediction\n    y_pred=(y_pred&gt;0.5).astype(int)#convert to binary\n    if y_pred==y_true:\n        correct_predictions+=1#count correct predictions\n\n#calculate test accuracy\ntest_accuracy=(correct_predictions/X_test.shape[0])*100\nprint(\"Test Accuracy:\",test_accuracy)\nIteration: 0\nCost: [[0.72566273]]\nAccuracy: 64.28571428571429\nIteration: 1\nCost: [[0.71523798]]\nAccuracy: 66.42857142857143\nIteration: 2\nCost: [[0.70509586]]\nAccuracy: 66.42857142857143\nIteration: 3\nCost: [[0.6864599]]\nAccuracy: 67.85714285714286\nIteration: 4\nCost: [[0.65347682]]\nAccuracy: 75.71428571428571\nIteration: 5\nCost: [[0.60664836]]\nAccuracy: 82.14285714285714\nIteration: 6\nCost: [[0.55558826]]\nAccuracy: 91.42857142857143\nIteration: 7\nCost: [[0.5072171]]\nAccuracy: 93.57142857142857\nIteration: 8\nCost: [[0.46296521]]\nAccuracy: 95.0\nIteration: 9\nCost: [[0.42292781]]\nAccuracy: 96.42857142857143\nIteration: 10\nCost: [[0.38716204]]\nAccuracy: 97.85714285714285\nIteration: 11\nCost: [[0.35557885]]\nAccuracy: 97.85714285714285\nIteration: 12\nCost: [[0.32788988]]\nAccuracy: 97.85714285714285\nIteration: 13\nCost: [[0.30368962]]\nAccuracy: 97.85714285714285\nIteration: 14\nCost: [[0.28254309]]\nAccuracy: 97.85714285714285\nIteration: 15\nCost: [[0.26403564]]\nAccuracy: 97.85714285714285\nIteration: 16\nCost: [[0.24779326]]\nAccuracy: 97.85714285714285\nIteration: 17\nCost: [[0.23348834]]\nAccuracy: 98.57142857142858\nIteration: 18\nCost: [[0.22083889]]\nAccuracy: 98.57142857142858\nIteration: 19\nCost: [[0.20960497]]\nAccuracy: 98.57142857142858\nIteration: 20\nCost: [[0.19958386]]\nAccuracy: 98.57142857142858\nIteration: 21\nCost: [[0.190605]]\nAccuracy: 98.57142857142858\nIteration: 22\nCost: [[0.18252507]]\nAccuracy: 98.57142857142858\nIteration: 23\nCost: [[0.17522359]]\nAccuracy: 98.57142857142858\nIteration: 24\nCost: [[0.16859908]]\nAccuracy: 98.57142857142858\nIteration: 25\nCost: [[0.16256584]]\nAccuracy: 98.57142857142858\nIteration: 26\nCost: [[0.15705123]]\nAccuracy: 98.57142857142858\nIteration: 27\nCost: [[0.15199344]]\nAccuracy: 98.57142857142858\nIteration: 28\nCost: [[0.14733967]]\nAccuracy: 98.57142857142858\nIteration: 29\nCost: [[0.14304459]]\nAccuracy: 98.57142857142858\nIteration: 30\nCost: [[0.13906911]]\nAccuracy: 98.57142857142858\nIteration: 31\nCost: [[0.13537942]]\nAccuracy: 98.57142857142858\nIteration: 32\nCost: [[0.1319461]]\nAccuracy: 98.57142857142858\nIteration: 33\nCost: [[0.12874349]]\nAccuracy: 98.57142857142858\nIteration: 34\nCost: [[0.12574908]]\nAccuracy: 98.57142857142858\nIteration: 35\nCost: [[0.12294312]]\nAccuracy: 99.28571428571429\nIteration: 36\nCost: [[0.12030816]]\nAccuracy: 99.28571428571429\nIteration: 37\nCost: [[0.11782877]]\nAccuracy: 99.28571428571429\nIteration: 38\nCost: [[0.11549126]]\nAccuracy: 99.28571428571429\nIteration: 39\nCost: [[0.11328345]]\nAccuracy: 100.0\nIteration: 40\nCost: [[0.11119447]]\nAccuracy: 100.0\nIteration: 41\nCost: [[0.1092146]]\nAccuracy: 100.0\nIteration: 42\nCost: [[0.10733513]]\nAccuracy: 100.0\nIteration: 43\nCost: [[0.10554823]]\nAccuracy: 100.0\nIteration: 44\nCost: [[0.10384686]]\nAccuracy: 100.0\nIteration: 45\nCost: [[0.10222465]]\nAccuracy: 100.0\nIteration: 46\nCost: [[0.10067586]]\nAccuracy: 100.0\nIteration: 47\nCost: [[0.09919528]]\nAccuracy: 100.0\nIteration: 48\nCost: [[0.09777818]]\nAccuracy: 100.0\nIteration: 49\nCost: [[0.09642027]]\nAccuracy: 100.0\nTest Accuracy: 93.33333333333333\nHere’s a neat observation. If a neural network is simple a two-layer network with non-linear activation, it is simply a matrix multiplication to new inputs. Therefore, we now have a way to learn that second-order probability matrix!\nGiven a neural network which can learn vector-vector relationships, it is easy to see that we can reconstruct our word-pair combinations from a vector that is the result of an attention step. Suppose we have a vector corresponding to the words ‘My’, ‘rabbit’, and ‘likes’, (111)\\begin{pmatrix}1\\\\1\\\\1 \\end{pmatrix}, the result of making it non-future-dependent is (110)\\begin{pmatrix}1\\\\1\\\\0 \\end{pmatrix}, and our attention step results in (100)\\begin{pmatrix}1\\\\0\\\\0 \\end{pmatrix}. We now pass this as input to a neural network and compute the output. The output will simply map our input vector to an output vector. The insight is that you can train the network to accurately map the result of our attention step to word pairs! The next obvious question is how to generate these word pairs. But before that, we need to notice that our neural network gets trained quickly when the input-output training sets have a small number of elements. It quickly becomes unwieldy when you think about practical languages, like the 260,000 Italian words mentioned above. This moves us on to our next topic, embeddings.\n\n\n\n\nTo make our neural network work well, we need a large amount of input-output data. This is impractical at the scale of even small, real languages - there are simply too many words. Generating one-hot encoding matrices by vertically stacking the vectors, even with techniques to store sparse matrices, is still impractical once we think about storing word pairs and triplets. We need some way to reduce these one-hot matrices in size so storing them becomes more efficient. This is the same problem we tried to solve with neural networks: converting an input vector into another vector. In our case, we want to convert a large vector to a smaller vector such that enough information is retained. This smaller vector will be called the embedding vector.\nBased on all that we’ve seen so far, it is obvious that this conversion will be done with matrix multiplication. The question is how to make this new matrix. We can do the same thing we did before (training a neural network) or we can do something completely different. Here is an example. Suppose we want to embed ‘My’, ‘rabbit’, ‘likes’, and ‘bananas’ into 2 dimensions. We know that their one-hot encoding is an identity matrix, possibly with its columns shuffled. We can arbitrarily define a 4×24 \\times 2 matrix that will project this matrix into a smaller matrix. We can then say that column 1 (representing, say, ‘rabbit’) of the initial matrix is now replaced by column 1 of the new matrix. This is perfectly fine. But is this meaningful?\nWhat do we want from a ‘good’ embedding? Broadly, a good embedding should be useful for practical tasks. There is no use in embedding words and making the transformer’s neural network harder to train. We might want to apply clustering algorithms to word embeddings to find out, for example, how many nouns there are in a large corpus (plural corpora) of text. We might also want to know what words are related in an unknown language. For someone trying to embed English, making sure that the embeddings for ‘rabbit’ and ‘hare’ are closer(i.e. their difference is closer to 0→\\vec{0}) than the embeddings for ‘rabbit’ and ‘desk’ is important if training a model to explain what it says in pictures of lagomorphs in office environments. Embeddings should probably also capture context awareness - ‘hot dog’ must have a different embedding compared to both ‘hot’ and ‘dog’. They should also not be too low-dimensional; we might lose important information.\nIt is beyond the scope of this tutorial to discuss good embedding algorithms. Fortunately, there is a straightforward algorithm we can use to embed our four-word language. Let’s map them on the unit circle with a randomly generated matrix, as we have been doing so far.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#define words\nwords=['My','rabbit','likes','bananas']\n\n#ensure that the words are sufficiently apart for better visibility-say 30 degrees\nangles=np.sort(np.random.choice(np.linspace(0,2*np.pi,360,endpoint=False),len(words),replace=False))\n\n#compute unit circle coordinates\nunit_circle_vectors=np.array([[np.cos(angle),np.sin(angle)] for angle in angles])\n\n#define initial one-hot vectors\none_hot_vectors=np.eye(len(words), dtype=int)\n\n#plot embeddings\nplt.figure(figsize=(6,6))\nfor i,word in enumerate(words):\n    x,y=unit_circle_vectors[i]\n    plt.scatter(x,y,label=word)\n    plt.text(x+0.05,y+0.05,word,fontsize=12)\n    #draw the arrow\n    plt.arrow(0,0,x,y,head_width=0.05,head_length=0.1,fc='blue',ec='red',alpha=0.7)\n\n# Draw the unit circle\ntheta=np.linspace(0,2*np.pi,100)\ncircle_x=np.cos(theta)\ncircle_y=np.sin(theta)\nplt.plot(circle_x,circle_y,color='gray',linestyle='--')\n\nplt.title(\"2D Embedding of a 4-Word Language on Unit Circle\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.axhline(0,color='gray',linewidth=0.5)\nplt.axvline(0,color='gray',linewidth=0.5)\nplt.grid(True)\nplt.legend()\nplt.axis('equal')\nplt.show()\n\n# Print initial one-hot vectors and unit circle embeddings\nprint(\"Initial One-Hot Vectors:\")\nfor i,word in enumerate(words):\n    print(f\"{word}:{one_hot_vectors[i]}\")\n\nprint(\"\\nEmbedded Vectors on Unit Circle:\")\nfor i,word in enumerate(words):\n    print(f\"{word}:{unit_circle_vectors[i]}\")\n\n\n\npng\n\n\nInitial One-Hot Vectors:\nMy:[1 0 0 0]\nrabbit:[0 1 0 0]\nlikes:[0 0 1 0]\nbananas:[0 0 0 1]\n\nEmbedded Vectors on Unit Circle:\nMy:[0.9961947  0.08715574]\nrabbit:[-0.54463904 -0.83867057]\nlikes:[ 0.60181502 -0.79863551]\nbananas:[ 0.99254615 -0.12186934]\n\n\nLet’s think a bit about our two-word non-future-dependent vectors. The only condition we have applied so far is that the value in the column matrix row wherever a word appears ahead of our current word should be 0. If a word appeared 1364 places before our current word but its value was deemed ‘important’ by attention, its corresponding row value would be 1. This is impractical. We know that it is unlikely that a word appearing 1364 places before the current word affects it. How can we quantify this?\nThe solution is to do it heuristically. Let’s figure out what exactly the task is. We have to add some additional information in a word’s embedding that denotes the position of the word in a sentence. This additional information is called the positional encoding. We want to satisfy a few criteria:\n\nThe encoding must be unique for each word in the sequence even if that word appears again. The sentence ‘My rabbit likes bananas but my friend’s rabbit doesn’t.’ should have different encoding values for the first and second occurrences of ‘rabbit’.\nIf we have to add positional encoding to sentences of different lengths, the ‘distance’ between two pieces of information should remain constant. This means that ‘My rabbit likes bananas. My friend’s rabbit does not like bananas.’ should encode the first and second occurrences of ‘rabbit’ in a way that the difference between the additional information added should be the same as the difference between ‘likes’ and ‘does’. This ensures that the two sentences are recognized as part of a ‘speech’.\nWe should be able to generalize to any sentence length easily with bounded and deterministic values (i.e. do not train a neural network).\n\nEssentially, we need to find a function whose codomain is a vector of the same size of the embedding that is:\n\nEasy to compute\nPeriodic\nHas bounded values.\n\nand iterate through the sentence, computing the function at the index of every word. To add the information to the word embedding, we can literally add the two vectors. This encodes positional information in the embedding.\nA function that satisfies these criteria is f:ℕ→ℝdf: \\mathbb{N} \\rightarrow \\mathbb{R}^d\nfi(t)={sin(ωk⋅t),if i=2kcos(ωk⋅t),if i=2k+1\n\\\nf_i(t) = \n\\begin{cases} \n\\sin(\\omega_k \\cdot t), & \\text{if } i = 2k \\\\ \n\\cos(\\omega_k \\cdot t), & \\text{if } i = 2k + 1 \n\\end{cases}\n\\\n\nwhere dd is the number of rows in the column vector representation of the embedding vector, ii and kk are simply ways to denote even and odd positions (i.e. the first row of the encoding vector is a sine, the second row is a cosine, the third row is a sine, and so on) and w=1100002kdw=\\frac{1}{10000^{\\frac{2k}{d}}}. ww has been chosen completely by guesswork. tt simply denotes the row number. Another good property is that since the encoding are periodic functions, you have also put in some information saying ‘the encoding of a word mm places away from the current word is so-and-so’. Their periodicity implies that they can be represented as a linear combination of earlier encodings. I want to reiterate that this is a heuristic that works and theoretical justifications for this do not really exist. It works because you have differentiated between sentences such as ‘I just computed five times three plus two’ and ‘I just computed five plus three times two’ which have different underlying meanings.\n\n\n\nWe finally discuss actually choosing the next word in the sequence. Suppose that have taken a sentence, tokenized it, converted to one-hot encoding, embedded these encodings, added position embeddings, and then trained a neural network to predict an output vector. The final step is to convert this output vector, which is also an embedding, back into a vector that represents the target vocabulary. We do not want to convert it back into a one-hot vector. How will you choose the next word?\nLet’s do what is straightforward - multiply it with a matrix. This time, we are taking a smaller vector and making it a larger one. This comes with some caveats. We are making a column matrix bigger. If we want to make (123)\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix} bigger, say to twice its size, how can we do it? Should we do (102030)\\begin{pmatrix}1\\\\0\\\\2\\\\0\\\\3\\\\0\\end{pmatrix}, (012030)\\begin{pmatrix}0\\\\1\\\\2\\\\0\\\\3\\\\0\\end{pmatrix}, or (001023)\\begin{pmatrix}0\\\\0\\\\1\\\\0\\\\2\\\\3\\end{pmatrix}? We can set up a matrix to do any one of these transformations, but we cannot set up a matrix that does this for all possible input vectors. This is because you will be solving an overdetermined system of equations. We have no choice but to accept this, so we have to assume that even if we find a matrix that makes the values as close to zero as possible, they will never all be 0 for any practical case. Going back to our initial task (English to Italian), we will end up with a vector that looks something like this\n$\n\nDiosmiomangiatounaand so on...[0.10.000050.250...]\\begin{aligned}\n&\\text{Dios}\\\\\n&\\text{mio}\\\\\n&\\text{mangiato}\\\\\n&\\text{una}\\\\\n&\\text{and so on...}\n\\end{aligned}\n\\quad\n\\begin{bmatrix}\n0.1 \\\\ 0.00005 \\\\ 0.25 \\\\ 0 \\\\ \\text{...}\n\\end{bmatrix}\n\n$\nHow do we select the next word? We can certainly pick the one with the largest value, but this is not so good. Fortunately, we have already looked at a way to emphasize the right word - softmaxing! Softmaxing and picking the highest probability allows us to enhance the probability of the right word (and it will be high because we train the neural network in this way - remember that matrix multiplications are just two-layer neural networks) being picked. An added bonus is that the softmax function is differentiable.\nimport numpy as np\n\n#define words\nwords=['My','rabbit','likes','bananas']  #ensure vocabulary is 4 words long\n\nprint(\"Tokenized sentence:\", words)\n\n#ensure that the words are sufficiently apart for better visibility-say 30 degrees\nangles=np.sort(np.random.choice(np.linspace(0,2*np.pi,360,endpoint=False),len(words),replace=False))\n\n#compute unit circle coordinates (2D embeddings)\nembedding_dim=2  #set embedding dimension to 2\nunit_circle_vectors=np.array([[np.cos(angle),np.sin(angle)] for angle in angles])\n\n#define initial one-hot vectors\none_hot_vectors=np.eye(len(words),dtype=int)\n\n#function for positional encoding as defined in \"Attention is all you need\"\ndef positional_encoding(seq_len,d_model):\n    #initialize positional encoding matrix\n    pos_enc=np.zeros((seq_len,d_model))\n    for pos in range(seq_len):\n        for i in range(0,d_model,2):\n            pos_enc[pos,i]=np.sin(pos/(10000**(2*i/d_model)))\n            if i+1&lt;d_model:  #check to prevent index out of range\n                pos_enc[pos,i+1]=np.cos(pos/(10000**(2*i/d_model)))\n    return pos_enc\n\n#calculate positional encodings for the sentence\nseq_len=len(words)\npositional_encodings=positional_encoding(seq_len,embedding_dim)\n\n#calculate the sum of position encoding and embedding vectors\ncombined_vectors=unit_circle_vectors+positional_encodings\n\n#decoder matrix to map combined vectors back to one-hot-like representations\ndecoder_matrix=np.linalg.pinv(unit_circle_vectors)  #pseudo-inverse to decode\n\n#decode the combined vectors\ndecoded_vectors=np.dot(combined_vectors,decoder_matrix)\n\n#map decoded vectors to words by finding the closest match\ndef decode_to_words(decoded_vectors,word_embeddings,word_list):\n    result=[]\n    for vec in decoded_vectors:\n        #project vec back into the original embedding space\n        reconstructed_vec=np.dot(vec,word_embeddings)\n        #compute distances and find the closest match\n        distances=np.linalg.norm(word_embeddings-reconstructed_vec,axis=1)\n        closest_word_index=np.argmin(distances)\n        result.append(word_list[closest_word_index])\n    return result\n\ndecoded_words=decode_to_words(decoded_vectors,unit_circle_vectors,words)\n\n#print initial one-hot vectors\nprint(\"Initial One-Hot Vectors:\")\nfor i,word in enumerate(words):\n    print(f\"{word}:{one_hot_vectors[i]}\")\n\n#print embedded vectors on unit circle\nprint(\"\\nEmbedded Vectors on Unit Circle:\")\nfor i,word in enumerate(words):\n    print(f\"{word}:{unit_circle_vectors[i]}\")\n\n#print positional encodings\nprint(\"\\nPositional Encodings:\")\nfor i,word in enumerate(words):\n    print(f\"{word}:{positional_encodings[i]}\")\n\n#print combined vectors\nprint(\"\\nCombined Vectors (Embedding+Positional Encoding):\")\nfor i,word in enumerate(words):\n    print(f\"{word}:{combined_vectors[i]}\")\n\n#print decoded words\nprint(\"\\nDecoded tokenized sentence from Combined Vectors:\")\nprint(decoded_words)\n\nTokenized sentence: ['My', 'rabbit', 'likes', 'bananas']\nInitial One-Hot Vectors:\nMy:[1 0 0 0]\nrabbit:[0 1 0 0]\nlikes:[0 0 1 0]\nbananas:[0 0 0 1]\n\nEmbedded Vectors on Unit Circle:\nMy:[0.1391731  0.99026807]\nrabbit:[-0.5591929  -0.82903757]\nlikes:[-0.34202014 -0.93969262]\nbananas:[ 0.89100652 -0.4539905 ]\n\nPositional Encodings:\nMy:[0. 1.]\nrabbit:[0.84147098 0.54030231]\nlikes:[ 0.90929743 -0.41614684]\nbananas:[ 0.14112001 -0.9899925 ]\n\nCombined Vectors (Embedding+Positional Encoding):\nMy:[0.1391731  1.99026807]\nrabbit:[ 0.28227808 -0.28873527]\nlikes:[ 0.56727728 -1.35583946]\nbananas:[ 1.03212653 -1.443983  ]\n\nDecoded tokenized sentence from Combined Vectors:\n['My', 'bananas', 'bananas', 'bananas']\n\n\n\n\nWe have technically made an encoder-decoder model at this point. As you can see in the code above, even without attention, it is somewhat difficult to get consistent sequence reconstruction. Let’s think about attention again. So far, we have simply made an attention matrix and used it to suppress unimportant parts of the code. Our attention matrix was created by stacking one-hot encoded vectors on top of each other. This doesn’t make much sense. Some parts of a sentence may be important - maybe not as important as other parts - but important nonetheless. Multiplication with an attention matrix should result in a vector with parts that are suppressed but not necessarily zero. We can give importance to different parts by making our attention matrix elements fractions instead of ones and zeros.\nWe have so far not defined precisely what attention does. We have so far said that given a matrix of attention masks, we can pull out specific rows given our one-hot encoded non-future-dependent vectors, and suppress things even more. Now we focus on the big question. How do we generate this attention matrix? We’re in a completely different domain now - our words are no longer one-hot encoded, they’re embeddings, we’re adding positional information to them, and now we want to suppress irrelevant information.\nLet’s get an intuitive explanation of attention first. We can naturally ask whether the attention in machine learning is the same as attention in human beings. What does this mean? For example, our brain is flooded with many different sensory inputs every second. There’s the internal sensory information from the body (such as level of hunger, blood pressure, pain, our balance), and there’s external sensory information from the environment (such as me hearing the distant hum of cars outside the window while typing this, but choosing to ignore it). How do we not get overwhelmed? We ‘tune out’ (suppress) irrelevant information and only focus on the one that matters. Only a small subset of the sensory input data is considered relevant enough to be perceived - this is what we mean when we say we are paying attention to something.\nSimply put, we are assigning importance to items by filtering out the irrelevant ones. We also have a finite amount of attention. For example, watching a group of ants move around is significantly easier than tracking the path of more than a few ants in that group. You can either have a general idea of how the group is moving, or a specific idea of how some finite amount of ants in that group are moving, but not both.\nWe can specify this mathematically: Given a set of input items i1,i2,...ini_1,i_2,...i_n, we assign nonzero weights to them, w1,w2,...,wnw_1,w_2,...,w_n such that ∑k=0nwk=1\\sum_{k=0}^{n}w_k=1. Interpreting the weights as importances, they satisfy the two properties of human attention. We can than make a judgment about the items based on this attention. Since our inputs to the attention matrix are a collection of items (embedded word vectors), we can then assign a weight to them.\nWhere do these weights come from? This is where the learning in machine learning happens. We want to learn a function ff to compute these weights. This function is typically one that first computes some ‘relevance’ score for each word in the sequence and then softmaxes the weights in order to make the weights nonzero. We have already computed this relevance score. This is simply the sum of the embedding vector and positional encoding vector. What do we do now?\nSo far we have talked about second-order models. Throughout the tutorial, we have constructed our one-hot encoded vectors assuming second-order relationships in the past. We briefly said that we could extend the word-pair vector construction to word triplets, but as the discussion so far should show, this is very impractical. We now try to answer the question: How important is every word to every other word in the sentence, and it is possible to calculate this in one go? Positional encodings don’t really answer this question, as they show how ‘relevant’ each word is in the overall sentence, but not in relation to the actual other words. We have to figure out three main things:\n\nHow do you tell a word to ‘ask’ other words in the sentence the following question: ‘How important are you to me?’?\nIf this question is asked for every possible word pair, how do you calculate the response of every other word?\nIf you are able to answer questions 1 and 2, how do you actually construct the vector that will be fed into the neural network?\n\nLet explicitly define what this means. Each word has some sort of intrinsic value, which we have said is the sum of its embedding and positional encoding. For each word to ask each other word how important they are to it, we have to compare its intrinsic value to the other words’ values. But what values? In the compound sentence ‘I went out to buy fruit and my sister answered some emails.’, ‘fruit’ and ‘emails’ are the objects of the subjects ‘I’ and ‘my sister’ in the two coordinate clauses. Even though they are very important within their own clauses, they are more or less irrelevant for the other clause. If there was an earlier relation such as ‘my sister likes fruit’ much earlier (say 1000 words behind ‘buy fruit’) in the corpus, it would make sense to compare the intrinsic values of the embeddings. But since we practically do not want to look 1000 words behind for all the reasons outlined above, it makes sense to assign each word a response value i.e. if asked a question by another word, the word being asked will return a response value that may be different from its intrinsic value. Based on what the response value is, the initial word will decide what information value it ‘passes on’ to the neural network so that it can reconstruct this efficiently. Also, it does not make much sense for ‘I’ and ‘emails’ to be compared, so you also have to figure out how much ‘I’ will pay attention to the other words. This triplet of intrinsic value, response value, and information value are what defines the attention process.\nHow do we make a word get a vector response from every other word? Like we have been doing all along, we define an attention matrix that is not one-hot encoded this time around. The attention matrix prepares each word for asking questions. Multiplying a word vector by this attention matrix results in a vector that contains the information ‘how much attention should this word pay to other words?’. To make this process fast, you can stack each intrinsic value into a matrix and multiply it with the attention matrix. This results in a matrix (let’s call this matrix AA) that contains information about ‘how much should every word pay attention to every other word’? Note that this is not pair-specific; we are not saying that ‘I’ should pay lesser attention to ‘emails’ than to ‘went’. This is the behavior that is learned by the transformer.\nNext, we multiply AA with a matrix that contains a matrix of responses of the other words. Like before, we are not explicitly telling ‘I’ to provide a worse response if asked ‘How important are you to me?’ by ‘emails’, but we are learning this behavior. Let’s call the result of this matrix multiplication BB. You can now normalize the weights and softmax them in order for the matrix to have stable values. We then finally multiply this matrix by the matrix of information values to get CC the result of the attention step. This is the idea behind attention. BB contains information about how much weight each word should give to every other word, and CC contains information that makes our two-layer neural network make a judgment. These matrices are learned by the transformer by backpropagation.\n\n\nSince we are now no longer dealing with nonnegative zero-one matrices, the question of how to do this calculation efficiently for large corpora still remains. Unfortunately we cannot. We can only rely on specialized and accelerated hardware. We are also still not sure whether the attention process actually captures meanings in a way that is human-understandable. The solution is straightforward: instead of having one set of matrices to perform attention, have more than one set. Apply each matrix set’s attention independently, and for every newly returned information vector, combine them together in a specified way (usually, just concatenate, then use a neural network to predict outputs). The hope is that each set of matrices will learn something different about the text - sentence structure, word meanings, subject-object relationships. Each such set of matrices is called an attention head. To make this process somewhat computationally palatable, the matrices in multihead attention have smaller output dimensions (usually what the size would be for a single head for the whole corpus divided by the number of heads). In practice, this does work.\nThe final thing is dealing with practical languages. We still have to stack these attention blocks and two-layer neural networks many times in order to actually encode and decode things. This is the main reason it takes so long to train transformers on real text items. There is still of course the problem of getting correct datasets, but this is fine.\nimport numpy as np\n\n\ntokenized_sentence=['My','rabbit','likes','bananas']\n\n#combined_vectors is a np.ndarray of shape (4,2) that has been initialized in a different cell\n\nintrinsic_value_matrix=combined_vectors.T\n\nprint(\"The intrinsic value matrix is:\\n\", intrinsic_value_matrix)\n\nl=len(tokenized_sentence) #this is completely arbitrary - i want a machine learning model whose final dimension is 4\n\nattention_matrix=np.random.rand(l,l)\n\nprint(\"The initial attention matrix is:\\n\", attention_matrix)\n\nA=np.matmul(intrinsic_value_matrix, attention_matrix)\nprint(\"Preparing the words for attention, we get:\\n\", A)\n\nresponse_matrix=np.random.rand(l,l)\n\nprint(\"The initial response matrix is:\\n\", response_matrix)\n\nB=np.matmul(A,response_matrix.T)\n\nprint(\"The responses given by each word to each other word is:\\n\", B, \"\\n and after normalizing, we get:\\n\", B/np.sqrt(l))\nB=B/np.sqrt(l) #explicitly rewrite the matrix\n\nprint(\"We will softmax the attention matrix in order to boost closer words. The result of doing this is:\\n\", np.exp(B)/np.sum(np.exp(B),axis=1,keepdims=True))\nB=np.exp(B)/np.sum(np.exp(B),axis=1,keepdims=True) #store it again\n\ninformation_matrix=np.random.rand(l,l)\n\nprint(\"The information matrix is:\\n\", information_matrix)\n\nC=np.matmul(B, information_matrix)\n\nprint(\"The actual information passed on to the two-layer neural network for decoding is:\\n\", C)\nThe intrinsic value matrix is:\n [[ 0.1391731   0.28227808  0.56727728  1.03212653]\n [ 1.99026807 -0.28873527 -1.35583946 -1.443983  ]]\nThe initial attention matrix is:\n [[0.96775588 0.85687394 0.11764399 0.73973033]\n [0.31966683 0.98841599 0.48535898 0.1224339 ]\n [0.72518555 0.3230112  0.99707816 0.97978246]\n [0.65613402 0.12982645 0.686861   0.09303049]]\nPreparing the words for attention, we get:\n [[ 1.31351514  0.71549621  1.42792633  0.78933853]\n [-0.09688701  0.79459977 -2.24969069 -0.02585148]]\nThe initial response matrix is:\n [[0.02936416 0.82295845 0.20975295 0.37902713]\n [0.97345739 0.3368405  0.29648978 0.918208  ]\n [0.11074845 0.75958241 0.98806557 0.90129601]\n [0.65166792 0.31406079 0.25432198 0.8192551 ]]\nThe responses given by each word to each other word is:\n [[ 1.22608639  2.66780165  2.81126061  2.09050766]\n [ 0.16939992 -0.51740933 -1.65330782 -0.40691028]] \n and after normalizing, we get:\n [[ 0.61304319  1.33390082  1.4056303   1.04525383]\n [ 0.08469996 -0.25870467 -0.82665391 -0.20345514]]\nWe will softmax the attention matrix in order to boost closer words. The result of doing this is:\n [[0.14693005 0.30211697 0.32458379 0.22636919]\n [0.34953106 0.24794025 0.14050437 0.26202432]]\nThe information matrix is:\n [[0.87982191 0.52543491 0.24433994 0.11294321]\n [0.08717947 0.71907745 0.42011873 0.90535987]\n [0.7396713  0.19126042 0.61377742 0.43550545]\n [0.57724973 0.73806912 0.80619588 0.84373733]]\nThe actual information passed on to the two-layer neural network for decoding is:\n [[0.52636754 0.52360382 0.54454599 0.62247348]\n [0.5843209  0.58220905 0.48705008 0.54622243]]\nprint(\"This is a demonstration of multihead attention.\\n\")\n\n\nprint(\"Let's have two attention heads. The idea is to make our single-head attention model smaller.\")\n\nintrinsic_value_matrix=combined_vectors\n\nl2=int(l/2) #two heads\nmatrix_sets=[]\nl3=int(l2/2) #two heads, so the underlying projection dimesion will be smaller\nfor i in range(l2):\n    matrix_sets.append([np.random.rand(l2,l3),np.random.rand(l2,l3),np.random.rand(l2,l3)])\n\nprint(\"The multihead attention matrices are now:\")\nprint([i for i in matrix_sets])\n\n\npassed_on=[]\nprint(\"The attention process is now applied to the intrinsic value matrix for each head.\")\nfor i in range(l2):\n    A=np.matmul(intrinsic_value_matrix,matrix_sets[i][0])\n    B=np.matmul(A,matrix_sets[i][1].T)/np.sqrt(l2)\n    B=np.exp(B)/np.sum(np.exp(B),axis=1,keepdims=True)\n    C=np.matmul(B,matrix_sets[i][2])\n    print(\"For head\",i+1,\"the information passed on is:\\n\", C)\n    passed_on.append(C)\n\nprint(\"Therefore, the total information passed on is:\\n\",np.hstack((passed_on[0],passed_on[1])))\nThis is a demonstration of multihead attention.\n\nLet's have two attention heads. The idea is to make our single-head attention model smaller.\nThe multihead attention matrices are now:\n[[array([[0.25397487],\n       [0.0311156 ]]), array([[0.73929474],\n       [0.10634373]]), array([[0.31491959],\n       [0.00381713]])], [array([[0.68166948],\n       [0.16713201]]), array([[0.65528241],\n       [0.99174262]]), array([[0.07267087],\n       [0.07707801]])]]\nThe attention process is now applied to the intrinsic value matrix for each head.\nFor head 1 the information passed on is:\n [[0.16275392]\n [0.16155103]\n [0.16291437]\n [0.16692318]]\nFor head 2 the information passed on is:\n [[0.07498641]\n [0.07491223]\n [0.0749164 ]\n [0.07499548]]\nTherefore, the total information passed on is:\n [[0.16275392 0.07498641]\n [0.16155103 0.07491223]\n [0.16291437 0.0749164 ]\n [0.16692318 0.07499548]]\n\n\n\nWe have not exactly implemented future-proofing here. Masked attention is simply a term that makes BB an upper triangular matrix, as we can easily now see why it is a future proofed matrix.\n\n\n\nSometimes the result of the total information being passed on is small, so you can always add a skip connection to make it so that your vector is a result of embedding+position encoding AND attention. This is all empirical stuff that ‘just works’. Skip connections also serve the dual purpose of being good for gradient descent. One thing we have not spoken about is gradient descent in practice. It is easy to see that gradient descent may be bad in practice while sound in theory, when the error function’s plot in parameter space is just too hilly. Skip connections, things like layer normalization, and Xavier initialization for all parameters make ‘the gradients flow smoother’ in backpropagation. In practice, this means that the gradient calculation actually updates the values fairly frequently, instead of the gradient being calculated as 0 (because of a loss of numerical precision in computers - they have finite precision!).\nFor backpropagation through layer normalization, this is the mathematics: $$ x_{}=f_{} \\[5pt] f_{}=\\[5pt]\n= _{i=1}^N x_i \\[5pt]\n^2 = _{i=1}^N (x_i - )^2 \\[5pt]\n\\[5pt]\n d_{out}=\\[5pt]\n = {i=1}^N d{out_{i}} (x_i - ) (- (^2 + )^{-3/2})\\[5pt]\n = {i=1}^N d{out_{i}} (-) + _{i=1}^N (-2 (x_i - ) / N )\\[5pt]\n = + + $$\n\n\n\nSo far we have described attention as a standalone procedure. The idea is now to encode text with attention (for example, our neural network converts to a lower dimensional space) and pass the result of that into a decoder matrix. What does that mean? It means that we have converted text into an arbitrary dimension space after performing all of these procedures. But wait. These can simply be interpreted as the results of a separate instance of attention results! While generating text, we can simply pull those results in and feed them into the decoder’s decoding stage, providing the attention we need.\n\n\n\nNow we do our actual task. How do we actually choose the next word? The idea is to predict the next word and generate a probability distribution with softmaxing, and greedily picking the one with the highest probability. Then, take the text that has been generated so far along with the output and use it as an input to the decoder, apply attention, then use multihead to combine things with the encoded transformer. Transformers generate text one word at a time. The way you put the text in as input is by inserting a special word that translates to ‘SEQUENCE STARTS HERE’. This allows the decoder to train and learn to predict the next word.\n\n\n\n\nAfter all of this, we can finally implement a transformer in numpy. Note that when you backpropagate errors through the model, the updates are applied to all matrices as single neurons as well. Let us lay out what the overall task is:\n\nGive an input sentence, tokenize it, embed the words, add positional encoding. Keep a copy of the output.\nInitialize random attention, response, and value matrices, and run the matrix of inputs through the multi-head attention step.\nAdd the output copy and attention step results, and use that as input to a 2 layer neural network (whose neurons are randomly initialized) to convert the data into some weird representation that only the machine understands.\nPut this weird or latent representation into another neural network to decode it, then project it to the target space vocabulary size, then softmax. Select the word with the largest probability as the output and compare which word in the other vocabulary’s embeddings is the closest. Once the next word is found, use that as the input to the model (that is, put it through its own set of attention steps, but this time they’re masked) and use that to predict the next token. Once the entire sequence has been predicted, compute the cross-entropy loss and apply backpropagation, updating each matrix.\n\nimport numpy as np\nfrom typing import List, Tuple, Optional, Dict\n\n###############################################################################\n# Utilities\n###############################################################################\n\ndef whitespace_tokenizer(sentence: str) -&gt; List[str]:\n    \"\"\"\n        Splits a latin-character sentence into individual words assuming that each word is separated by a whitespace, and there's no punctuation\n    \n        Input:\n            sentence: a string \n        Output:\n            List of strings containing each individual words\n    \"\"\"\n\n    #we now start using python's inbuilt functions in order to make the code look more impressive\n    return sentence.strip().split()\n\ndef build_vocab(list_of_tokenized_sentences: List[List[str]]) -&gt; Tuple[dict, dict]:\n    \"\"\"\n        Builds a vocabulary of all words in the a given corpus of languages.\n        This means that if you have more than one sentence, it creates a list of words.\n        Input:\n            list_of_tokenized_sentences: a list of list of strings, AKA a whitespace tokenized sentence\n        Output:\n            A tuple of Python dictionaries, containing all words and their index. This index is random, because of the use of set()\n    \"\"\"\n    #the idea is to store the vocabulary in a Python set() object, which randomly stores elements for faster access (there's more to it but oh well)\n    vocabulary=set()\n    for tokenized_sentence in list_of_tokenized_sentences: \n        vocabulary.update(tokenized_sentence) #when you update a python set with a list, the elements of the list are added to the vocabulary\n    vocabulary=list(vocabulary) #convert the vocabulary set() into a list, so you can enumerate through it\n    word_index_pairs={w:i for i,w in enumerate(vocabulary)} #create a word:index pairing dictionary\n    index_word_pairs={i:w for i,w in enumerate(vocabulary)} #create an index:word pairing dictionary\n    return word_index_pairs,index_word_pairs\n\ndef pad_sequences(sequences: List[List[int]], max_length: Optional[int]=None, pad_value: int=0) -&gt; np.ndarray:\n    \"\"\"\n        Pads sequences. This is done because it is easier to process sequences with the same length, so we artifically add numbers to smaller ones.\n        Inputs:\n            sequences: a list of list of integers\n            max_length: Optional. Allows you to set the maximum length for a sequence instead of computing it dynamically.\n                     For example, in a corpus of sentences each under 20 words long, you can set the max length as 294\n            pad_value: What number you want to use to \n        Output:\n            A numpy.ndarray of a padded sequence\n\n    \"\"\"\n    #dynamically figure out the max length, as you have to pad to this\n    if max_length is None:\n        max_length=max(len(sequence) for sequence in sequences)\n    padded=[]\n    for sequence in sequences:\n        padded_sequence=sequence+[pad_value]*(max_length-len(sequence)) #simply append a list containing pad_value\n        padded.append(padded_sequence)\n    return np.array(padded, dtype=np.int32) #return an ndarray\n\ndef create_mask_for_removing_future_dependency(sequence: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n        Creates a non-future-dependent mask for the decoder, so that the decoder doesn't use the future for generation. This is also called autoregressive behavior.\n        Inputs:\n            sequence: An input ndarray whose shape is (batch_size, batch_size) - note that this assumes you are feeding it into the decoder at scale\n        Outputs:\n            An ndarray of the same shape as the input sequence which is upper triangular. The data type is Bool for faster processing. The upper triangular part has 'True'.\n    \"\"\"\n    sequence_length=sequence.shape[1]\n    autoregressive_mask=np.triu(np.ones((sequence_length,sequence_length)),k=1).astype(bool) #again, using the inbuilt functions\n    return autoregressive_mask\n\ndef one_hot(indices: np.ndarray, vocabulary_size: int) -&gt; np.ndarray:\n    \"\"\"\n        Converts input sequences into one-hot encoding in one go. \n        Since we are feeding sequences in one go to the encoder in batches, we need a 3D ndarray. \n        Inputs:\n            indices: An input ndarray whose shape is (batch_size, sequence_length), where each element has an integer index. This is why we created indices above\n            vocabulary_size: The number of words in the vocabulary of the language\n    \"\"\"\n    batched_tokens=np.zeros((indices.shape[0], indices.shape[1], vocabulary_size)) #create your zero ndarray, and populate it\n    #use a nested loop to set one-hot encodings\n    for batch_index in range(indices.shape[0]):\n        for token_index in range(indices.shape[1]):\n            batched_tokens[b,t,indices[batch_index,token_index]]=1\n    return batched_tokens\n\n###############################################################################\n# Normalization, activation, and loss\n###############################################################################\n\ndef layer_norm(x: np.ndarray, eps=1e-6) -&gt; Tuple[np.ndarray,np.ndarray,np.ndarray]:\n    \"\"\"\n        Normalizes the input ndarray. This is done to make backpropagation not run into numerical errors.\n        Inputs:\n            x: ndarray of inputs. usually just a vector\n            eps: this is done to prevent division by zero during normalization\n        Outputs:\n            Tuple of ndarrays containing the normalized input ndarray, the mean, and the variance\n    \"\"\"\n    mean=np.mean(x, axis=-1, keepdims=True)\n    var=np.var(x, axis=-1, keepdims=True)\n    x_norm=(x-mean)/np.sqrt(var+eps)\n    return x_norm,mean,var\n\ndef layer_norm_backprop(dout: np.ndarray, x: np.ndarray, mean: np.ndarray, var: np.ndarray, eps: float=1e-6) -&gt;np.ndarray:\n\n    \"\"\"\n        Compute the gradient of the loss with respect to the input x of a layer normalization operation, implementing backpropagation as defined in the tutorial\n        Inputs: \n            dout: an ndarray containing the gradient with respect to the normalized input\n            x: an ndarray containing the original input\n            mean: mean of the input ndarray x along the last axis, as computed above\n            var: exactly like the mean\n            eps: an optional value for numerical stabillity\n        Outputs:\n            an ndarray containing the gradient with respect to the layer normalization procedure\n    \"\"\"\n\n    #numerically backpropagate by calculating the derivatives. unfortunately, this just requires knowing the formula as shown above\n    N=x.shape[-1]\n    dx_norm=dout/np.sqrt(var+eps)\n    dvar=np.sum(dout*(x-mean)*-0.5*(var+eps)**(-1.5), axis=-1, keepdims=True)\n    dmean=(np.sum(dout*-1/np.sqrt(var+eps), axis=-1, keepdims=True)+dvar*np.sum(-2*(x-mean), axis=-1, keepdims=True)/N)\n    dx=dx_norm+dvar*2*(x-mean)/N+dmean/N\n    return dx\n\ndef softmax(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n        Compute the softmax of an input ndarray and generate a probability distribution\n        Inputs:\n            x: an ndarray that you want to softmax\n        Outputs:\n            an ndarray containing the softmaxed version along the last axis\n    \"\"\"\n    x_shifted=x-np.max(x, axis=-1, keepdims=True) #this is done to improve numerical stability. softmaxing is invariant to shifts by a constant value\n    exp_x=np.exp(x_shifted)\n    return exp_x/np.sum(exp_x, axis=-1, keepdims=True)\n\ndef cross_entropy_loss(predictions: np.ndarray, targets: np.ndarray) -&gt; float:\n    \"\"\"\n        Compute the cross-entropy loss as defined above, measuring the difference between two probability predicted distributions\n        Inputs:\n            predictions: an ndarray containing whatever you have predicted\n            targets: an ndarray containing whatever the real targets are\n        Outputs:\n            a float of computed cross-entropy loss, averaged over all elements in the batch\n    \"\"\"\n    epsilon=1e-12 #more numerical precision constants\n    predictions=np.clip(predictions, epsilon, 1-epsilon) #values smaller than epsilon become epsilon, values larger than 1-epsilon become 1-epsilon\n    flat_targets=targets.flatten() #maybe the arrays are not 1d, but the cross-entropy loss needs the 1d\n    flat_preds=predictions.reshape(-1, predictions.shape[-1]) #same reason here for flattening\n    loss=-np.mean(np.log(flat_preds[np.arange(flat_targets.shape[0]), flat_targets])) #compute the crossentropy loss\n    return loss\n\ndef cross_entropy_derivative(predictions: np.ndarray, targets: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n        Compute the backpropagation for cross-entropy loss\n        Inputs:\n            predictions: an ndarray containing whatever you have predicted\n            targets: an ndarray containing whatever the real targets are\n        Outputs:\n            an ndarray of the gradient of the cross-entropy derivative required for backpropagation\n    \"\"\"\n    #the gradient was also defined above so this is just a way to get batches of data in and publish it\n    batch,length,vocab_size=predictions.shape\n    grad=predictions.copy()\n    for b in range(batch):\n        for t in range(length):\n            grad[b, t, targets[b,t]]-=1\n    grad/=(batch*length)\n    return grad\n\n###############################################################################\n# Multi-Head Attention\n###############################################################################\n\ndef split_heads(x: np.ndarray, num_heads: int) -&gt; np.ndarray:\n    \"\"\"\n        Attention heads attend to different parts of the data, so this is a function to simply split the data \n        Inputs:\n            x: an ndarray of input data\n            num_heads: the number of attention instances you want\n        Outputs:\n            a reshaped x split into the number of heads\n    \"\"\"\n    #at this point it should be familiar, split the data into batches and make it work\n    batch,length,d_model=x.shape\n    head_dim=d_model//num_heads #the heads attend to different parts of the model\n    return x.reshape(batch, length, num_heads, head_dim).transpose(0,2,1,3) #reshape x so that every ndarray is for heads\n\ndef merge_heads(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n        After the result of multihead attention, you need to recombine them, so this function does it\n        Inputs:\n            x: an ndarray containing information of shape (batch, num_heads, length, head_dim)\n        Outputs:\n            a combined x after (presumably) multihead attention has been done\n    \"\"\"\n    batch,num_heads,length,head_dim=x.shape\n    return x.transpose(0, 2, 1, 3).reshape(batch, length, num_heads*head_dim)\n\ndef scaled_dot_product_attention(attention_matrix: np.ndarray, response_matrix: np.ndarray, information_matrix: np.ndarray, mask: Optional[np.ndarray]=None) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n        This is the official formula of the attention. As we have seen, we use an attention matrix to make each word ask each other word a question.\n        The response of each other word is the response matrix and the information passed on is the information matrix. \n        inputs:\n            attention_matrix: An ndarray of attention values\n            response_matrix: An ndarray of response values\n            information_matrix: An ndarray of information values that is passed on\n            mask: An ndarray of masks, but with Bool values\n        Outputs:\n            a tuple of the information passed on and the attention weights\n    \"\"\" \n    normalization_factor=attention_matrix.shape[-1]\n    scores=np.matmul(attention_matrix, response_matrix.transpose(0,1,3,2))/np.sqrt(normalization_factor) #we are now using 'official' terminology\n    if mask is not None:\n        scores=np.where(mask[np.newaxis, np.newaxis,:,:], -1e9, scores) #mask if required\n    attn_weights=softmax(scores) #softmax the product\n    output=np.matmul(attn_weights, information_matrix) #send information on\n    return output, attn_weights\n\ndef multi_head_attention(batched_input_intrinsic_value_matrix: np.ndarray, batched_input_response_matrix: np.ndarray, batched_input_information_matrix: np.ndarray, batched_input_intrinsic_value_projection_matrix: np.ndarray, batched_input_response_projection_matrix: np.ndarray, batched_input_information_projection_matrix: np.ndarray, final_reshaper_matrix: np.ndarray, num_heads: int, mask: Optional[np.ndarray]=None) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n        Implementation of multihead attention. Simply put, apply attention on smaller parts of the sequence, and then recombine them by concatenation.\n        Because we are splitting the sequence, we need to generate different attention, response, and information matrices for each attention instance.\n        This means we have to project the input into different matrices every time, and the matrix that does this projection is also learned.\n        This is the trick behind multihead attention.\n        Inputs:\n            batched_input_intrinsic_value_matrix: an ndarray of your batched input\n            batched_input_response_matrix: an ndarray that is essentially the same as batched_input_intrinsic_value\n            batched_input_information_matrix: same as above. you have to declare that these matrices exist \n            batched_input_intrinsic_value_projection_matrix: an ndarray that projects the intrinsic value to the size for multihead attention\n            batched_input_response_projection_matrix: same thing for response_matrix\n            batched_input_information_projection_matrix: same thing as above\n        Returns:\n            A tuple of ndarrays that have the concatenated result of multihead attention and also the attention weights respectively. \n    \"\"\"\n    attention_matrix=batched_input_intrinsic_value_matrix @ batched_input_intrinsic_value_projection_matrix #we now use the inbuilt @ operator to do matrix multiplication quickly\n    response_matrix=batched_input_response_matrix @ batched_input_intrinsic_value_projection_matrix\n    information_matrix=batched_input_information_matrix @ batched_input_information_projection_matrix\n\n    #extract dimensions from the attention matrix (query tensor)\n    #attention_matrix` is the query tensor with shape (batch_size, seq_len_q, d_model)\n    batch, lq, d_model=attention_matrix.shape\n\n    #extract the length of the key tensor from the response matrix\n    #response_matrix is the key tensor with shape (batch_size, seq_len_k, d_model)\n    #lk represents seq_len_k (sequence length of the response_matrix), which may differ from lq (seq_len_q)\n    lk=response_matrix.shape[1]\n\n    #we don't explicitly compute `lv` (seq_len_v, length of values) because information_matrix is expected to have the same sequence length as response_matrix\n\n    #reshape and transpose attention_matrix to prepare for multi-head attention\n    #step 1: reshape attention_matrix from (batch_size, seq_len_q, d_model) to (batch_size, seq_len_q, num_heads, head_dim), where head_dim = d_model//num_heads\n    #step 2: transpose to (batch_size, num_heads, seq_len_q, head_dim) for easier computation per head\n    A=attention_matrix.reshape(batch, lq, num_heads, d_model//num_heads).transpose(0,2,1,3)\n\n    #reshape and transpose response_matrix similarly\n    B=response_matrix.reshape(batch, lk, num_heads, d_model//num_heads).transpose(0,2,1,3)\n\n    #reshape and transpose information_matrix similarly\n    C=information_matrix.reshape(batch, lk, num_heads, d_model//num_heads).transpose(0,2,1,3)\n\n    #compute the attention each head\n    #input ndarrays (A, B, B) are now split into individual heads for parallel processing\n    #'mask` is optional and is used to block certain positions (e.g., future positions in autoregressive decoding)\n    out_heads, attn_weights=scaled_dot_product_attention(A, B, C, mask)\n\n    #concatenate the output\n    out=(merge_heads(out_heads))@final_reshaper_matrix  # Apply a linear projection to combine the head outputs into `d_model` dimensions.\n\n    return out, attn_weights\n\n\ndef mha_backprop(\n        dout: np.ndarray,\n        batched_input_intrinsic_value_matrix: np.ndarray,\n        batched_input_response_matrix: np.ndarray,\n        batched_input_information_matrix: np.ndarray, \n        batched_input_intrinsic_value_projection_matrix: np.ndarray,\n        batched_input_response_projection_matrix: np.ndarray, \n        batched_input_information_projection_matrix: np.ndarray, \n        final_projection_matrix: np.ndarray,\n        attention_weights: np.ndarray, \n        num_heads: int, \n        mask: Optional[np.ndarray]=None\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n        This is the big one. This is the function that backpropagates through multihead attention. \n        It computes the gradients of the loss with respect to the sequence matrices and the weight matrices used in the multi-head attention mechanism.\n        Like before, we are in fact feeding it the loss.\n        Inputs:\n            dout: an ndarray of the gradient of the loss with respect to the output of multihead attention\n            batched_input_intrinsic_value_matrix: an ndarray as defined in the multihead attention function above,\n            batched_input_response_matrix: an ndarray as defined in the multihead attention function above ,\n            batched_input_information_matrix: an ndarray as defined in the multihead attention function above,   \n            batched_input_intrinsic_value_projection_matrix: an ndarray as defined in the multihead attention function above,\n            batched_input_response_projection_matrix: an ndarray as defined in the multihead attention function above, \n            batched_input_information_projection_matrix: an ndarray as defined in the multihead attention function above, \n            final_projection_matrix: an ndarray as defined in the multihead attention function above,\n            attention_weights: an ndarray that is the output of multihead attention function, required for backpropagation\n            mask: an optional ndarray of masks (Bool data type)\n        Outputs:\n            the following tuple: Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n            - differential_intrinsic_value_matrix (np.ndarray): Gradient w.r.t. the query input `batched_input_intrinsic_value_matrix`, shape (batch_size, seq_len_q, d_model).\n            - differential_response_matrix (np.ndarray): Gradient w.r.t. the key input `batched_input_response_matrix`, shape (batch_size, seq_len_k, d_model).\n            - differential_information_matrix (np.ndarray): Gradient w.r.t. the value input `batched_input_information_matrix`, shape (batch_size, seq_len_k, d_model).\n            - original_shape_intrinsic_value_differential (np.ndarray): Gradient w.r.t. the query weight matrix `batched_input_intrinsic_value_projection_matrix`, shape (d_model, d_model).\n            - original_shape_response_differential (np.ndarray): Gradient w.r.t. the key weight matrix `batched_input_response_projection_matrix`, shape (d_model, d_model).\n            - original_shape_information_differential (np.ndarray): Gradient w.r.t. the value weight batched_input_information_projection_matrix `W_v`, shape (d_model, d_model).\n            - original_shape_attention_differential (np.ndarray): Gradient w.r.t. the output weight matrix `final_projection_matrix`, shape (d_model, d_model).\n    \"\"\"\n    #throughout the code, a variable starting with 'd' denotes a derivative/gradient, aside from d_model, which is the model dimension (user-defined really)\n    batch, attention_sequence_length, d_model=batched_input_intrinsic_value_matrix.shape #compute the gradient matrix\n    sequence_length=batched_input_response_matrix.shape[1] #length of the sequence, obtained from really any matrix\n    head_dim=d_model//num_heads #just the model head size\n\n    #recompute the forward step\n    attention_matrix=batched_input_intrinsic_value_matrix @ batched_input_intrinsic_value_projection_matrix #standard stuff\n    response_matrix=batched_input_response_matrix @ batched_input_response_projection_matrix\n    information_matrix=batched_input_information_matrix @ batched_input_information_projection_matrix\n\n    #similarly reshaping stuff as done before\n    head_attention_matrix=attention_matrix.reshape(batch, attention_sequence_length, num_heads, head_dim).transpose(0,2,1,3)\n    head_response_matrix=response_matrix.reshape(batch, sequence_length, num_heads, head_dim).transpose(0,2,1,3)\n    head_information_matrix=information_matrix.reshape(batch, sequence_length, num_heads, head_dim).transpose(0,2,1,3)\n\n    d_merged_heads=dout.reshape(batch, attention_sequence_length, d_model) #reshape dout into the correct shape\n    d_out_heads=d_merged_heads.reshape(batch, attention_sequence_length, num_heads, head_dim).transpose(0,2,1,3)\n\n    #check shape consistency, because it makes for easier debugging\n    assert d_out_heads.shape==(batch, num_heads, attention_sequence_length, head_dim), f\"Shape mismatch in d_out_heads: {d_out_heads.shape}\" #first time we've used 'assert'\n\n    #attention weights and value gradients\n    d_attention_weights=np.matmul(d_out_heads, head_information_matrix.transpose(0,1,3,2)) #this is pretty standard fare, start getting the stuff out from backprop\n    d_vh=np.matmul(attention_weights.transpose(0,1,3,2), d_out_heads) #this is also the same thing\n\n    #calculate the differential element of the attention scores, for backpropagation\n    sum_over_j=np.sum(attention_weights*d_attention_weights, axis=-1, keepdims=True)\n    d_scores=attention_weights*(d_attention_weights-sum_over_j)\n\n    #calculate the differential elements for the attention and response matrices, per head\n    d_attention_head=np.matmul(d_scores, head_response_matrix)/np.sqrt(head_dim)\n    d_response_head=np.matmul(d_scores.transpose(0,1,3,2), head_attention_matrix)/np.sqrt(head_dim)\n\n    #combine the elements back for concatenation\n    total_attention_differential=d_attention_head.transpose(0,2,1,3).reshape(batch, attention_sequence_length, d_model)\n    total_response_differential=d_response_head.transpose(0,2,1,3).reshape(batch, sequence_length, d_model)\n    total_information_differential=d_vh.transpose(0,2,1,3).reshape(batch, sequence_length, d_model)\n\n    #now calculate the total gradients for all elements\n    original_shape_intrinsic_value_differential=np.matmul(batched_input_intrinsic_value_matrix.reshape(-1, d_model).T, total_attention_differential.reshape(-1, d_model))\n    original_shape_response_differential=np.matmul(batched_input_response_matrix.reshape(-1, d_model).T, total_response_differential.reshape(-1, d_model))\n    original_shape_information_differential=np.matmul(batched_input_information_matrix.reshape(-1, d_model).T, total_information_differential.reshape(-1, d_model))\n    #original_shape_attention_differential=np.matmul(merge_heads(attention_weights @ information_matrix).reshape(batch*attention_sequence_length, d_model).T, dout.reshape(batch*attention_sequence_length, d_model))\n    C = information_matrix.reshape(batch, sequence_length, num_heads, head_dim).transpose(0, 2, 1, 3)\n\n    original_shape_attention_differential = np.matmul(\n    merge_heads(np.matmul(attention_weights, C)).reshape(batch * attention_sequence_length, d_model).T,\n    dout.reshape(batch * attention_sequence_length, d_model)\n        )\n    differential_intrinsic_value_matrix = total_attention_differential @ batched_input_intrinsic_value_projection_matrix.T\n    differential_response_matrix = total_response_differential @ batched_input_response_projection_matrix.T\n    differential_information_matrix = total_information_differential @ batched_input_information_projection_matrix.T\n\n    return differential_intrinsic_value_matrix, differential_response_matrix, differential_information_matrix, original_shape_intrinsic_value_differential, original_shape_response_differential, original_shape_information_differential, original_shape_attention_differential\n\n\n###############################################################################\n# Feed Forward Network\n###############################################################################\n\n#now we define the feedforward neural network (2 layers) with backprop. we use the ReLU activation function for easy differentials\n#note the different output in the definition of the feedforward network\ndef feed_forward(x: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray) -&gt; Tuple[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\" \n        This is a simple two layer feedforward neural network. The reason a cache is returned at all is because it is immensely helpful in backpropagation\n        Inputs:\n            x: input ndarray of data\n            W1: weight matrix for the first linear transformation\n            b1: bias vector for first layer\n            W2, b2: same as above for second layer\n        Outputs:\n            Tuple[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n            - z2 (np.ndarray): the output tensor after the feed-forward computation of shape (batch_size, seq_len, d_model).\n            - cache (Tuple[np.ndarray, np.ndarray]): a tuple containing:\n                - z1 (np.ndarray): the output of the first linear transformation before the ReLU activation, of shape (batch_size, seq_len, d_ff).\n                - relu (np.ndarray): The output of the ReLU activation, of shape (batch_size, seq_len, d_ff).\n    \"\"\"\n    #this is just straighforward - two matrix multiplications with a ReLU in between\n    z1=x@W1+b1\n    relu=np.maximum(0, z1)\n    z2=relu@W2+b2\n    return z2, (z1, relu)\n\ndef feed_forward_backprop(dz2: np.ndarray, x: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray, cache: Tuple[np.ndarray, np.ndarray]) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n        The backpropagation for a feedforward layer. Note that we are working backwards, so the input variables are defined that way\n        Inputs:\n            dz2: ndarray containing gradient of the loss function with respect to output of the second layer\n            x: input ndarray of data\n            W1-cache: same as above\n        Outputs:\n            Tuple of losses (ndarrays) with respect to x,W1,b1,W2,b2 respectively\n    \"\"\"\n    #backpropagation, step by step, is the exact formula outlines in the tutorial\n    (z1, relu)=cache #now you see why we utilized the cache at all\n    batch, length, d_model=x.shape\n    dW2=np.matmul(relu.reshape(-1, d_model).T, dz2.reshape(-1, d_model))\n    db2=dz2.sum(axis=(0,1))\n    d_relu=dz2@W2.T\n    d_z1=d_relu*(z1&gt;0)\n    dW1=np.matmul(x.reshape(-1, d_model).T, d_z1.reshape(-1, d_model))\n    db1=d_z1.sum(axis=(0,1))\n    dX=d_z1@W1.T\n    return dX, dW1, db1, dW2, db2\n\n###############################################################################\n# Encoder and Decoder Blocks\n###############################################################################\n\n#we now build the actual encoder-decoder layer.\n\ndef encoder_layer(x: np.ndarray, intrinsic_value_projector: np.ndarray, response_projector: np.ndarray, information_projector: np.ndarray, attention_projector: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray, num_heads: int=2) -&gt; Tuple[np.ndarray, Tuple]:\n    \"\"\"\n        Now we build the transformer in earnest from all of our classes. This implements the encoder layer using the structure outlined in the tutorial\n        Inputs:\n            x: ndarray of input data\n            intrinsic_value_projector, response_projector, information_projector, attention_projector: ndarrays containing projection matrices for input data\n            W1, b1, W2, b2: ndarrays of the two-layer neural network\n            num_heads: number of attention heads\n        Outputs:\n            out: output ndarray of the entire encoder\n            cache: a Tuple of all cached values for backpropagation\n\n    \"\"\"\n    #at this point i start using smaller variable names because the program becomes tedious to read. anyway, this code is self-explanatory \n    x_norm, mean1, var1=layer_norm(x)\n    attn_out, attn_w=multi_head_attention(x_norm, x_norm, x_norm, intrinsic_value_projector, response_projector, information_projector, attention_projector, num_heads=num_heads)\n    x2=x+attn_out\n\n    x2_norm, mean2, var2=layer_norm(x2)\n    ff_out, ff_cache=feed_forward(x2_norm,W1,b1,W2,b2)\n    out=x2+ff_out\n    cache=(x, x2, x_norm, x2_norm, mean1, var1, mean2, var2, attn_w, ff_cache)\n    return out, cache\n\ndef decoder_layer(x, encoder_out,\n                  intrinsic_weight_masked, response_weight_masked, information_weight_masked, attention_weight_masked,\n                  intrinsic_crossattention_weight, response_crossattention_weight, information_crossattention_weight, attention_crossattention_weight, W1, b1, W2, b2, mask, num_heads=2):\n    \"\"\"\n        We implement the decoder layer with masked attention.\n        Inputs: \n            x: ndarray of input data\n            intrinsic_weight_masked: projection matrix for masked self-attention for the intrinsic value weight\n            response_weight_masked: projection matrix for masked self-attention for the response weight\n            information_weight_masked: projection matrix for masked self-attention for the information weight\n            attention_weight_masked: projection matrix for masked self-attention for the attention weights\n            intrinsic_crossattention_weight: projection matrix for the intrinsic value weight during cross-attention\n            response_crossattention_weight: projection matrix for the response weight during cross-attention\n            information_crossattention_weight: projection matrix for the information weight during cross-attention\n            attention_crossattention_weight: projection matrix for the attention weights during cross-attention\n            W1,b1,W2,b2, mask: all defined above\n            num_heads: number of attention instances\n        Outputs:\n            Tuple[np.ndarray, Tuple]:\n            - out: output ndarray of the decoder layer, shape (batch_size, seq_len, d_model)\n            - cache (Tuple): cached values for backpropagation, including:\n                - x, x2, x3 : intermediate ndarrays at various stages\n                - x_norm_dec1, x2_norm, x3_norm : layer-normalized ndarrays\n                - mean_dec1, var_dec1, mean_dec2, var_dec2, mean_dec3, var_dec3: Means and variances (in ndarrays) from layer normalization\n                - masked_attn_w, cross_attn_w: attention weight ndarrays from masked self-attention and cross-attention\n                - ff_cache_dec (Tuple): cached values from the feed-forward network\n    \"\"\"\n    #this just implements the decoder layer\n\n    #first, apply layer normalization, then compute multihead attention\n    x_norm_dec1,mean_dec1,var_dec1=layer_norm(x)\n    masked_attn_out, masked_attn_w=multi_head_attention(x_norm_dec1, x_norm_dec1, x_norm_dec1,\n                                                          intrinsic_weight_masked, response_weight_masked, information_weight_masked, attention_weight_masked, \n                                                          num_heads=num_heads, mask=mask)\n    x2=x+masked_attn_out\n    #then layernorm again\n    x2_norm, mean_dec2, var_dec2=layer_norm(x2)\n    cross_attn_out, cross_attn_w=multi_head_attention(x2_norm, encoder_out, encoder_out,\n                                                        intrinsic_crossattention_weight, response_crossattention_weight, information_crossattention_weight, attention_crossattention_weight,\n                                                        num_heads=num_heads, mask=None)\n    x3=x2+cross_attn_out\n    #layer norm thrice\n    x3_norm, mean_dec3, var_dec3=layer_norm(x3)\n    ff_out, ff_cache_dec=feed_forward(x3_norm, W1, b1, W2, b2)\n    out=x3+ff_out\n    #prepare the cache for backprop\n    cache=(x, x2, x3, x_norm_dec1, x2_norm, x3_norm, mean_dec1, var_dec1, mean_dec2, var_dec2, mean_dec3, var_dec3, masked_attn_w, cross_attn_w, ff_cache_dec)\n    return out, cache\n\n###############################################################################\n# Forward and Backprop Through Model (Single Layer Encoder-Decoder)\n###############################################################################\n\n#at this point \n\ndef forward_transformer(enc_in: np.ndarray, dec_in: np.ndarray,\n                        intrinsic_value_weight_enc: np.ndarray, response_weight_enc: np.ndarray, information_weight_enc: np.ndarray, attention_weight_enc: np.ndarray, W1_enc: np.ndarray, b1_enc: np.ndarray, W2_enc: np.ndarray, b2_enc: np.ndarray,\n                        intrinsic_value_weight_dec_masked: np.ndarray, response_weight_dec_masked: np.ndarray, information_weight_dec_masked: np.ndarray, attention_weight_dec_masked: np.ndarray,\n                        intrinsic_value_weight_dec_cross: np.ndarray, response_weight_dec_cross: np.ndarray, information_weight_dec_cross: np.ndarray, attention_weight_dec_cross: np.ndarray,\n                        W1_dec: np.ndarray, b1_dec: np.ndarray, W2_dec: np.ndarray, b2_dec: np.ndarray,\n                        W_embed_out: np.ndarray, b_embed_out: np.ndarray,\n                        src_mask: np.ndarray, tgt_mask: np.ndarray) -&gt; Tuple[np.ndarray, Tuple]:\n    \"\"\"\n        We implement the forward pass for the transformer model. This is fairly straightforward and how we've defined it\n\n        Inputs:\n            enc_in: ndarray of input data to the encoder, shape (batch_size, src_len, d_model)\n            dec_in: ndarray of input data to the decoder, shape (batch_size, tgt_len, d_model)\n            intrinsic_value_weight_enc: projection matrix for the query in the encoder's self-attention\n            response_value_weight_enc: projection matrix for the key in the encoder's self-attention\n            information_value_weight_enc: projection matrix for the value in the encoder's self-attention\n            attention_value_weight_enc: projection matrix for the output in the encoder's self-attention\n            W1_enc, b1_enc, W2_enc, b2_enc: feed-forward network weights and biases for the encoder\n            intrinsic_value_weight_dec_masked: projection matrix for the query in the decoder's masked self-attention\n            response_value_weight_dec_masked: projection matrix for the key in the decoder's masked self-attention\n            information_value_weight_dec_masked: projection matrix for the value in the decoder's masked self-attention\n            attention_value_weight_dec_masked: projection matrix for the output in the decoder's masked self-attention\n            intrinsic_value_weight_dec_cross: projection matrix for the query in the decoder's cross-attention\n            response_value_weight_dec_cross: projection matrix for the key in the decoder's cross-attention\n            information_value_weight_dec_cross: projection matrix for the value in the decoder's cross-attention\n            attention_value_weight_dec_cross: projection matrix for the output in the decoder's cross-attention\n            W1_dec, b1_dec, W2_dec, b2_dec: feed-forward network weights and biases for the decoder\n            W_embed_out: projection matrix for mapping decoder output to the vocabulary space, shape (d_model, vocab_size)\n            b_embed_out: bias vector for mapping decoder output to the vocabulary space, shape (vocab_size,)\n            src_mask: ndarray mask for the encoder, shape (src_len, src_len)\n            tgt_mask: ndarray mask for the decoder, shape (tgt_len, tgt_len)\n\n        Outputs:\n            Tuple[np.ndarray, Tuple]:\n            - probs: output probabilities over the vocabulary, shape (batch_size, tgt_len, vocab_size)\n            - cache (Tuple): cached values for backpropagation, including:\n                - enc_out: ndarray of the encoder output, shape (batch_size, src_len, d_model)\n                - enc_cache: cached intermediate values from the encoder\n                - dec_out: ndarray of the decoder output, shape (batch_size, tgt_len, d_model)\n                - dec_cache: cached intermediate values from the decoder\n    \"\"\"\n\n    #this should be fairly straighforward by now, how it's implemented. just feed your weights in and go through the entire layer\n    enc_out, enc_cache=encoder_layer(enc_in, intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc, W1_enc, b1_enc, W2_enc, b2_enc)\n    dec_out, dec_cache=decoder_layer(dec_in, enc_out, intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,\n                                       intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,\n                                       W1_dec, b1_dec, W2_dec, b2_dec,\n                                       mask=tgt_mask)\n    logits=dec_out@W_embed_out+b_embed_out\n    probs=softmax(logits)\n    return probs, (enc_out, enc_cache, dec_out, dec_cache)\n\ndef backward_transformer(dprobs: np.ndarray,\n    enc_in: np.ndarray,\n    dec_in: np.ndarray,\n    enc_out: np.ndarray,\n    enc_cache: Tuple,\n    dec_out: np.ndarray,\n    dec_cache: Tuple,\n    intrinsic_value_weight_enc: np.ndarray,\n    response_weight_enc: np.ndarray,\n    information_weight_enc: np.ndarray,\n    attention_weight_enc: np.ndarray,\n    W1_enc: np.ndarray,\n    b1_enc: np.ndarray,\n    W2_enc: np.ndarray,\n    b2_enc: np.ndarray,\n    intrinsic_value_weight_dec_masked: np.ndarray,\n    response_weight_dec_masked: np.ndarray,\n    information_weight_dec_masked: np.ndarray,\n    attention_weight_dec_masked: np.ndarray,\n    intrinsic_value_weight_dec_cross: np.ndarray,\n    response_weight_dec_cross: np.ndarray,\n    information_weight_dec_cross: np.ndarray,\n    attention_weight_dec_cross: np.ndarray,\n    W1_dec: np.ndarray,\n    b1_dec: np.ndarray,\n    W2_dec: np.ndarray,\n    b2_dec: np.ndarray,\n    W_embed_out: np.ndarray,\n    b_embed_out: np.ndarray,\n    src_mask: np.ndarray,\n    tgt_mask: np.ndarray) -&gt; Tuple[Dict[str, np.ndarray], np.ndarray]:\n    \"\"\"\n        This is possibly the hardest part in the code. This is complete backpropagation for the transformer, through all layers.\n\n        Inputs:\n            dprobs: ndarray of gradients with respect to output probabilities, shape (batch_size, tgt_len, vocab_size)\n            enc_in: ndarray of input data to the encoder, shape (batch_size, src_len, d_model)\n            dec_in: ndarray of input data to the decoder, shape (batch_size, tgt_len, d_model)\n            enc_out: ndarray of encoder outputs, shape (batch_size, src_len, d_model)\n            enc_cache: cached values from the encoder forward pass\n            dec_out: ndarray of decoder outputs, shape (batch_size, tgt_len, d_model)\n            dec_cache: cached values from the decoder forward pass\n            intrinsic_value_weight_enc: projection matrix for the query in the encoder's self-attention\n            response_weight_enc: projection matrix for the key in the encoder's self-attention\n            information_weight_enc: projection matrix for the value in the encoder's self-attention\n            attention_weight_enc: projection matrix for the output in the encoder's self-attention\n            W1_enc, b1_enc, W2_enc, b2_enc: feed-forward network weights and biases for the encoder\n            intrinsic_value_weight_dec_masked: projection matrix for the query in the decoder's masked self-attention\n            response_weight_dec_masked: projection matrix for the key in the decoder's masked self-attention\n            information_weight_dec_masked: projection matrix for the value in the decoder's masked self-attention\n            attention_weight_dec_masked: projection matrix for the output in the decoder's masked self-attention\n            intrinsic_value_weight_dec_cross: projection matrix for the query in the decoder's cross-attention\n            response_weight_dec_cross: projection matrix for the key in the decoder's cross-attention\n            information_weight_dec_cross: projection matrix for the value in the decoder's cross-attention\n            attention_weight_dec_cross: projection matrix for the output in the decoder's cross-attention\n            W1_dec, b1_dec, W2_dec, b2_dec: feed-forward network weights and biases for the decoder\n            W_embed_out: projection matrix for mapping decoder outputs to the vocabulary, shape (d_model, vocab_size)\n            b_embed_out: bias vector for mapping decoder outputs to the vocabulary, shape (vocab_size,)\n            src_mask: mask for the source sequence, shape (src_len, src_len)\n            tgt_mask: mask for the target sequence, shape (tgt_len, tgt_len)\n\n        Outputs:\n            Tuple[Dict[str, np.ndarray], np.ndarray]:\n            - grads: dictionary containing gradients for all trainable weights and biases, including:\n                - intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc: gradients for encoder self-attention weights\n                - W1_enc, b1_enc, W2_enc, b2_enc: gradients for encoder feed-forward network\n                - intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked: gradients for decoder masked self-attention weights\n                - intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross: gradients for decoder cross-attention weights\n                - W1_dec, b1_dec, W2_dec, b2_dec: gradients for decoder feed-forward network\n                - W_embed_out, b_embed_out: gradients for output projection layer\n            - dx_enc1: gradient with respect to the encoder input, shape (batch_size, src_len, d_model)\n    \"\"\"\n\n    #this is how backprop is implemented\n    batch, length, d_model=dec_out.shape  #first, extract the output of the decoder. what we are really interested in is the model dimension\n    vocab_size=W_embed_out.shape[1]  #and get the vocabulary size\n\n    #start: backprop through the final layer\n    \n    dW_embed_out=dec_out.reshape(-1, d_model).T@dprobs.reshape(-1, vocab_size)  # Shape: (d_model, vocab_size)\n    db_embed_out=dprobs.sum(axis=(0,1))  # Shape: (vocab_size,)\n    d_dec_out=dprobs @ W_embed_out.T  # Gradient w.r.t decoder output, shape (batch, tgt_len, d_model)\n\n    #get all values from the decoder - this is called 'unpacking'\n    (x, x2, x3,\n     x_norm_dec1, x2_norm_dec, x3_norm_dec,\n     mean_dec1, var_dec1, mean_dec2, var_dec2, mean_dec3, var_dec3,\n     masked_attn_w, cross_attn_w, ff_cache_dec)=dec_cache\n\n    #backprop through the feedforward neural network for the decoder\n    d_x3_ff=d_dec_out\n    d_x3_ff, dW1_dec, db1_dec, dW2_dec, db2_dec=feed_forward_backprop(d_x3_ff, x3_norm_dec, W1_dec, b1_dec, W2_dec, b2_dec, ff_cache_dec)\n    dx3_norm=d_x3_ff\n    dx3=layer_norm_backprop(dx3_norm, x3_norm_dec, mean_dec3, var_dec3)\n    d_x3_skip=dx3\n\n    #backprop through crossattention\n    d_x2_cross=d_x3_skip\n    dx_cross_Q, dx_cross_K, dx_cross_V, dintrinsic_value_weight_dec_cross_, dresponse_weight_dec_cross_, dinformation_weight_dec_cross_, dattention_weight_dec_cross_ = mha_backprop(\n        d_x2_cross, x2_norm_dec, enc_out, enc_out,\n        intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,\n        cross_attn_w, num_heads=2\n    )\n    d_x2_skip=dx_cross_Q\n    d_enc_out=dx_cross_K+dx_cross_V\n\n    #backprop through masked selfattention\n    d_x2_masked=d_x2_skip\n    dx_masked_Q, dx_masked_K, dx_masked_V, dintrinsic_value_weight_dec_masked_, dresponse_weight_dec_masked_, dinformation_weight_dec_masked_, dattention_weight_dec_masked_ = mha_backprop(\n        d_x2_masked, x_norm_dec1, x_norm_dec1, x_norm_dec1,\n        intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,\n        masked_attn_w, num_heads=2, mask=tgt_mask\n    )\n    dx_norm_dec1=dx_masked_Q+dx_masked_K+dx_masked_V\n    dx_dec1=layer_norm_backprop(dx_norm_dec1, x_norm_dec1, mean_dec1, var_dec1)\n\n    dx2_norm=d_x2_cross\n    dx2=layer_norm_backprop(dx2_norm, x2_norm_dec, mean_dec2, var_dec2)\n\n    #combine different layers' gradients\n    dx=dx_dec1+dx2\n\n    #unpack encoder values\n    (enc_x, enc_x2, enc_x_norm, enc_x2_norm, enc_mean1, enc_var1, enc_mean2, enc_var2, enc_attn_w, enc_ff_cache)=enc_cache\n\n    #backprop through the encoder\n    d_enc=d_enc_out\n    d_enc_ff, dW1_enc, db1_enc, dW2_enc, db2_enc=feed_forward_backprop(d_enc, enc_x2_norm, W1_enc, b1_enc, W2_enc, b2_enc, enc_ff_cache)\n    d_enc2_norm=d_enc_ff\n    d_enc2=layer_norm_backprop(d_enc2_norm, enc_x2_norm, enc_mean2, enc_var2)\n\n    #backprop through encoder's attention\n    dx_enc_Q, dx_enc_K, dx_enc_V, dintrinsic_value_weight_enc_, dresponse_weight_enc_, dinformation_weight_enc_, dattention_weight_enc_=mha_backprop(\n        d_enc2, enc_x_norm, enc_x_norm, enc_x_norm,\n        intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc,\n        enc_attn_w, num_heads=2\n    )\n    d_enc_norm1=dx_enc_Q+dx_enc_K+dx_enc_V\n    dx_enc1=layer_norm_backprop(d_enc_norm1, enc_x_norm, enc_mean1, enc_var1)\n\n    #combine all gradients in a dictionary \n    grads = {\n        'intrinsic_value_weight_enc': dintrinsic_value_weight_enc_, 'response_weight_enc': dresponse_weight_enc_, 'information_weight_enc': dinformation_weight_enc_, 'attention_weight_enc': dattention_weight_enc_,\n        'W1_enc': dW1_enc, 'b1_enc': db1_enc, 'W2_enc': dW2_enc, 'b2_enc': db2_enc,\n        'intrinsic_value_weight_dec_masked': dintrinsic_value_weight_dec_masked_, 'response_weight_dec_masked': dresponse_weight_dec_masked_, 'information_weight_dec_masked': dinformation_weight_dec_masked_, 'attention_weight_dec_masked': dattention_weight_dec_masked_,\n        'intrinsic_value_weight_dec_cross': dintrinsic_value_weight_dec_cross_, 'response_weight_dec_cross': dresponse_weight_dec_cross_, 'information_weight_dec_cross': dinformation_weight_dec_cross_, 'attention_weight_dec_cross': dattention_weight_dec_cross_,\n        'W1_dec': dW1_dec, 'b1_dec': db1_dec, 'W2_dec': dW2_dec, 'b2_dec': db2_dec,\n        'W_embed_out': dW_embed_out, 'b_embed_out': db_embed_out\n    }\n\n    return grads, dx_enc1\n\n###############################################################################\n# Main Training Setup\n###############################################################################\n\n\n#we are finally done. let's now train the actual transformer\nenglish_sentences=[\n    \"My rabbit likes bananas\"\n]\n\nitalian_sentences=[\n    \"Al mio coniglio piacciono le banane\",\n]\n\neng_tokens=[whitespace_tokenizer(s) for s in english_sentences]\nfor_tokens=[whitespace_tokenizer(s) for s in italian_sentences]\n\neng_word2idx, eng_idx2word=build_vocab(eng_tokens)\nfor_word2idx, for_idx2word=build_vocab(for_tokens)\n\ndef encode(tokens, w2i):\n    return [w2i[t] for t in tokens]\n\neng_encoded=[encode(t, eng_word2idx) for t in eng_tokens]\nfor_encoded=[encode(t, for_word2idx) for t in for_tokens]\n\nmax_eng_len=max(len(x) for x in eng_encoded)\nmax_for_len=max(len(x) for x in for_encoded)\n\n# Add start/end tokens\nstart_token=len(for_word2idx)\nend_token=len(for_word2idx)+1\nfor_word2idx[\"&lt;start&gt;\"]=start_token\nfor_word2idx[\"&lt;end&gt;\"]=end_token\nfor_idx2word[start_token]=\"&lt;start&gt;\"\nfor_idx2word[end_token]=\"&lt;end&gt;\"\n\nfor_idx2word={idx: token for token, idx in for_word2idx.items()}\n\nfor_encoded_input=[[start_token]+seq for seq in for_encoded]\nfor_encoded_target=[seq+[end_token] for seq in for_encoded]\n\nmax_for_len_inp=max(len(s) for s in for_encoded_input)\nmax_for_len_tgt=max(len(s) for s in for_encoded_target)\n\neng_padded=pad_sequences(eng_encoded, max_length=max_eng_len, pad_value=0)\nfor_inp_padded=pad_sequences(for_encoded_input, max_length=max_for_len_inp, pad_value=0)\nfor_tgt_padded=pad_sequences(for_encoded_target, max_length=max_for_len_tgt, pad_value=0)\n\nbatch_size=len(eng_padded)\nsrc_len=eng_padded.shape[1]\ntgt_len=for_inp_padded.shape[1]\nvocab_size_src=len(eng_word2idx)\nvocab_size_tgt=len(for_word2idx)\nd_model=16 #this is arbitrary. many AI companies sell access to their embeddings and dimensions\n\nsrc_embeddings=np.random.randn(vocab_size_src, d_model)*0.01 #i chose 0.01 for a balance between numerical stability and demonstrating the power of transformers\ntgt_embeddings=np.random.randn(vocab_size_tgt, d_model)*0.01\n\n#we haven't actually defined the embedding function- let's embed our vocabulary\ndef embed(x, emb):\n    return emb[x] \n\n#this next section is just defining random matrices\n\nW_embed_out=np.random.randn(d_model, vocab_size_tgt)*0.01 #define this random matrix for embedding\nb_embed_out=np.zeros(vocab_size_tgt) #and the bias of the weights as well\n\nintrinsic_value_weight_enc=np.random.randn(d_model, d_model)*0.01 \nresponse_weight_enc=np.random.randn(d_model, d_model)*0.01\ninformation_weight_enc=np.random.randn(d_model, d_model)*0.01\nattention_weight_enc=np.random.randn(d_model, d_model)*0.01\nW1_enc=np.random.randn(d_model, d_model)*0.01\nb1_enc=np.zeros(d_model)\nW2_enc=np.random.randn(d_model, d_model)*0.01\nb2_enc=np.zeros(d_model)\n\nintrinsic_value_weight_dec_masked=np.random.randn(d_model, d_model)*0.01\nresponse_weight_dec_masked=np.random.randn(d_model, d_model)*0.01\ninformation_weight_dec_masked=np.random.randn(d_model, d_model)*0.01\nattention_weight_dec_masked=np.random.randn(d_model, d_model)*0.01\n\nintrinsic_value_weight_dec_cross=np.random.randn(d_model, d_model)*0.01\nresponse_weight_dec_cross=np.random.randn(d_model, d_model)*0.01\ninformation_weight_dec_cross=np.random.randn(d_model, d_model)*0.01\nattention_weight_dec_cross=np.random.randn(d_model, d_model)*0.01\n\nW1_dec=np.random.randn(d_model, d_model)*0.01\nb1_dec=np.zeros(d_model)\nW2_dec=np.random.randn(d_model, d_model)*0.01\nb2_dec=np.zeros(d_model)\n\n#here we define the learning rate and epochs\nlearning_rate=0.01\nepochs=20\n\nsrc_mask=None\ntgt_mask=create_mask_for_removing_future_dependency(for_inp_padded)\n\n#and implement the generic neural network training\n\nfor epoch in range(epochs):\n    enc_inp=embed(eng_padded, src_embeddings)\n    dec_inp=embed(for_inp_padded, tgt_embeddings)\n\n    #this is the forward pass\n    probs, cache=forward_transformer(enc_inp, dec_inp,\n                                       intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc, W1_enc, b1_enc, W2_enc, b2_enc,\n                                       intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,\n                                       intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,\n                                       W1_dec, b1_dec, W2_dec, b2_dec,\n                                       W_embed_out, b_embed_out,\n                                       src_mask, tgt_mask)\n\n    loss=cross_entropy_loss(probs, for_tgt_padded)\n    print(f\"Epoch {epoch+1}, Loss: {loss}\")\n\n    pred_indices=np.argmax(probs, axis=-1)\n    for b in range(batch_size):\n        predicted_tokens=[for_idx2word[idx] for idx in pred_indices[b]]\n        print(\"Predicted:\", \" \".join(predicted_tokens))\n    \n    dprobs=cross_entropy_derivative(probs, for_tgt_padded)\n\n    #this is the backward pass\n    grads, dx_enc=backward_transformer(dprobs, enc_inp, dec_inp, *cache,\n                                         intrinsic_value_weight_enc, response_weight_enc, information_weight_enc, attention_weight_enc, W1_enc, b1_enc, W2_enc, b2_enc,\n                                         intrinsic_value_weight_dec_masked, response_weight_dec_masked, information_weight_dec_masked, attention_weight_dec_masked,\n                                         intrinsic_value_weight_dec_cross, response_weight_dec_cross, information_weight_dec_cross, attention_weight_dec_cross,\n                                         W1_dec, b1_dec, W2_dec, b2_dec,\n                                         W_embed_out, b_embed_out,\n                                         src_mask, tgt_mask)\n\n    #update all parameters after backprop\n    intrinsic_value_weight_enc-=learning_rate*grads['intrinsic_value_weight_enc']\n    response_weight_enc-=learning_rate*grads['response_weight_enc']\n    information_weight_enc-=learning_rate*grads['information_weight_enc']\n    attention_weight_enc-=learning_rate*grads['attention_weight_enc']\n    W1_enc-=learning_rate*grads['W1_enc']\n    b1_enc-=learning_rate*grads['b1_enc']\n    W2_enc-=learning_rate*grads['W2_enc']\n    b2_enc-=learning_rate*grads['b2_enc']\n\n    intrinsic_value_weight_dec_masked-=learning_rate*grads['intrinsic_value_weight_dec_masked']\n    response_weight_dec_masked-=learning_rate*grads['response_weight_dec_masked']\n    information_weight_dec_masked-=learning_rate*grads['information_weight_dec_masked']\n    attention_weight_dec_masked-=learning_rate*grads['attention_weight_dec_masked']\n\n    intrinsic_value_weight_dec_cross-=learning_rate*grads['intrinsic_value_weight_dec_cross']\n    response_weight_dec_cross-=learning_rate*grads['response_weight_dec_cross']\n    information_weight_dec_cross-=learning_rate*grads['information_weight_dec_cross']\n    attention_weight_dec_cross-=learning_rate*grads['attention_weight_dec_cross']\n\n    W1_dec-=learning_rate*grads['W1_dec']\n    b1_dec-=learning_rate*grads['b1_dec']\n    W2_dec-=learning_rate*grads['W2_dec']\n    b2_dec-=learning_rate*grads['b2_dec']\n\n    W_embed_out-=learning_rate*grads['W_embed_out']\n    b_embed_out-=learning_rate*grads['b_embed_out']\n\n# After training, you can use the decoder in inference mode by feeding\n# previously generated tokens (shifted) as input to the decoder and applying\n# the masked multi-head attention to predict the next token\nEpoch 1, Loss: 2.0795187664172397\nPredicted: banane coniglio banane banane &lt;start&gt; piacciono &lt;end&gt;\nEpoch 2, Loss: 2.0793373726502367\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 3, Loss: 2.0791564355830636\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 4, Loss: 2.078974718625546\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 5, Loss: 2.0787933552602227\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 6, Loss: 2.0786123118403346\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 7, Loss: 2.078431551323341\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 8, Loss: 2.078251032354273\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 9, Loss: 2.0780707083355354\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 10, Loss: 2.0778905181441116\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 11, Loss: 2.077710390742451\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 12, Loss: 2.077530273264538\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 13, Loss: 2.0773500940542484\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 14, Loss: 2.0771697955319044\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 15, Loss: 2.0769892772510743\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 16, Loss: 2.07680845041442\nPredicted: banane coniglio banane banane mio piacciono &lt;end&gt;\nEpoch 17, Loss: 2.0766272061537157\nPredicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;\nEpoch 18, Loss: 2.0764454984062057\nPredicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;\nEpoch 19, Loss: 2.076263321722837\nPredicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;\nEpoch 20, Loss: 2.0760807185682095\nPredicted: banane coniglio banane banane coniglio piacciono &lt;end&gt;\n\n\n\nWell, after all of that, this poor of a performance is what we got? This is true. We were translating only one sequence, after all. Where transformers really excel is at scale, because of the differentiable attention matrix which can be parallelized. But you can see how it gets stuck depending on different learning rates and initialization. One of the difficulties is initialization to make gradient descent helpful. Xavier initialization could have worked, but it is much better to do it with numpy to get real understanding.\n\n\n\nThis aims to be an implementation of transformers that works, end-to-end, in numpy. No pointing to paper that blew up - Attention is All You Need (no, not it isn’t). No awkward discussions of ‘keys’, ‘values’, ‘queries’ (replaced with fundamental names instead). No list of mathematical formulas for different attention mechanisms with ‘choose what you want’. No describing the transformer architecture in a way that is essentially ‘just look at the figure’. No code snippets without explanation for the basics. For the last point, the final implementation is, in fact, annoying. The assumption is that you read this point from start to end. However there is something I have not mentioned at all. That something is automatic differentiation.\n\n\nYou may have noticed just how obnoxious implementing backpropagation was. Every single function had to be done manually. Was there a way we could have done it recursively? The answer is yes. This is called automatic differentiation. Automatic differentiation relies on the technique that all mathematical operations done by a computer are at the end pulled from basic arithmetic functions, since that’s what’s implemented on their circuit boards. Therefore, all computations of functions are inherently limited by this. Even though multiplication is not repeated addition, to a computer it is. Libraries exist entirely to implement backpropagation and automatic differentiation for user-defined functions. One such library is PyTorch. It implements automatic differentiation and abstracts it away, allowing users to define new functions and not worry about how backpropagation deals with that object. How would the transformer from scratch look like in PyTorch? :)"
  },
  {
    "objectID": "blog/2024-12-07-transformers/index.html#footnotes",
    "href": "blog/2024-12-07-transformers/index.html#footnotes",
    "title": "Transformers from scratch in numpy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore on this later↩︎\nFor this reason, layers tend to have the same activation function, as it is easy to parallelize the computation↩︎\nMaking automatic differentiation easier led to the invention of many fundamental inventions in empirical learning↩︎\nhttps://sacred-texts.com/hin/m01/m01002.htm, ‘I am (continued Sauti)…’↩︎\nSee Stirling’s approximation↩︎\nMore on this later↩︎\nMore on this later↩︎\nFor this reason, layers tend to have the same activation function, as it is easy to parallelize the computation↩︎\nMore on this later↩︎\nFor this reason, layers tend to have the same activation function, as it is easy to parallelize the computation↩︎\nMaking automatic differentiation easier led to the invention of many fundamental inventions in empirical learning↩︎\nhttps://sacred-texts.com/hin/m01/m01002.htm, ‘I am (continued Sauti)…’↩︎\nSee Stirling’s approximation↩︎\nForeshadowing↩︎\nTechnically, this is what we have been attempting to do this entire time.↩︎\nMore on this later↩︎\nFor this reason, layers tend to have the same activation function, as it is easy to parallelize the computation↩︎\nMaking automatic differentiation easier led to the invention of many fundamental inventions in empirical learning↩︎\nhttps://sacred-texts.com/hin/m01/m01002.htm, ‘I am (continued Sauti)…’↩︎\nSee Stirling’s approximation↩︎"
  },
  {
    "objectID": "blog/2024-12-01-partitioning/index.html",
    "href": "blog/2024-12-01-partitioning/index.html",
    "title": "Papers explained: Partitioning the Universe’s baryons (Connor et al 2024)",
    "section": "",
    "text": "Connor et al. (2024) recently published a paper that shows a way to partition the universe’s baryons to reveal the underlying cosmic web structure and showed that it contains a lot of gas. Is it possible to explain this paper to mildly intelligent people with an interest in astronomy?\n\n\nApproximately half of the universe’s dark matter (that is, matter that does not interact with electromagnetic radiation such as light, so we can’t see it) resides in structures known as ‘collapsed halos’. Collapsed halos are ellipsoidal or quasi-spherical structures surrounding a central object like a star. The initial universe’s density field was approximately uniform, but quantum fluctuations led to changes in the initial density field at different points. Changes in density eventually led to gravity taking over and clustering the matter into halo-like structures. Significantly less than half of the universe’s baryons (protons and neutrons; electrons are leptons because they interact only using the weak nuclear force and the electromagnetic force) are confined to halos. A small fraction of the baryons are present as stars and in the interstellar medium (ISM) between galaxies. A lot of them are very diffuse (less than 1000 per cubic centimeter) and ionized (less than one in ten thousand is ionized in the baryonic gas) and are located in the halos of galaxy clusters, galaxy groups (literally just some galaxies together but not enough to be a cluster), or individual galaxies. Because this gas is so diffuse and ionized, the quantity and spatial distribution is very difficult to measure. If we measure it really well and are able to provide an accurate answer to the question ‘what is this quantity and spatial distribution?’ then we have an answer to a lot of important questions in galaxy formation, astrophysical feedback (the ejection of matter by stars, black holes, etc into the universe, or the reverse!), and precision cosmology. Recently, people have used Fast Radio Bursts (FRBs), very violent emissions of radio waves first discovered in 2007, to measure the total content of cosmic baryons. The problem in those studies is that they did not contain methods to discriminate between the intergalactic material (IGM) and halo gases. In this paper we present a large cosmological sample of FRB sources localized to their host galaxies. Also we have managed to partition (that is, determine the distribution of) missing baryons into the IGM, galaxy clusters, and galaxies. As a reminder, the term missing baryons refers to the following problem: cosmological models say that the number of baryons in the universe should be so-and-so, but actual measurements show that there are much fewer baryons. Then where are the missing baryons? The study provides a late-Universe (that is, the current universe) measurement of the total baryon density of Ωb​h70\\Omega_{b}​h_{70} as 0.049 $$0.003, where Ωb\\Omega_{b} is the universe’s critical density (the density needed for a flat universe) that is made of baryonic matter. h70h_{70} is simply a scaling factor related to the Hubble constant. The results indicate that feedback processes can expel gas from galaxy halos and into the intergalactic medium, agreeing with the enriched cosmic web scenario (that is, how the cosmic web contains elements heavier than hydrogen and helium) seen in cosmological simulations. We measured a large diffuse baryon fraction and that probably means that the distribution of stellar masses in a population of stars (the initial stellar mass function) is probably not bottom-heavy (it does not predict a large number of low mass stars, which in turn live for longer).\n\n\n\nThe Deep Synoptic Array-110 is a radio inteferometer (a system of radio telescopes that observe the same source at the same time) operating between 1.28-1.53 GHz and is the first radio telescope built to detect and localize FRBs (that is, find out the galaxy they originate from). Doing this helps use FRBs as cosmological tools and unveil their physical origin. 60 new FRBs were discovered by DSA-110 and 39 of them now have an associated host galaxy spectroscopic redshift. Our companion work shows the properties of the host galaxies of a uniformly selected subset of these. The work also includes 9 sources not in that sample. Three new FRBs near or beyond redshift 1 (redshift 1 means ~8-9 billion years old, as the wavelength of light has doubled) are also presented. Because these FRBs are at redshift 1, they constrain the IGM column (that is, the distribution of material along the line of sight) by virtue of their great distance.\nWe add our sample to 30 previously localized FRB sources. The distribution of extragalactic DM (dispersion measure - the TEC, but measured in terms of redshift) and redshift for the full sample is plotted in figure 1; the positions, DM, redshifts, and detection instruments are displayed in extended data table 1. The extragalactic DM of localized FRBs tells us how many diffuse baryons are in what place in the Universe. The total observed DM of an FRB can be split into several components. Let’s take a moment to think about this. When looking at a radio source occurring so far away, we need to consider several things. First, we are looking out of the Milky Way, so the Milky Way’s ISM contributes to the total DM. The Milky Way’s halo also contributes to it. The inter galactic medium along the line of sight is of course always present. There is also some ionized gas in stable halos (virialized halos). Then there are also the contributions from each ionized halo along the line of sight. To account for the expansion of the universe and relativistic motion the factor 11+zhalo\\frac{1}{1+z_{halo}} is applied to the observed dispersion measure. Finally there is the contribution of the FRB host galaxy’s matter (note that since we are now talking about plasmas, it means that we must consider fermionic matter such as electrons too), which may come from its halo, ISM, or any other plasma lying around the galaxy.\nWe can therefore write the observed FRB of a galaxy in the following way: DMobs=DMMW+DMIGM(zs)+∑iNxDMX(Mi,b⟂)1+zi+DMhost1+zs\nDM_{obs}=DM_{MW}+DM_{IGM}(z_s)+\\sum_{i}^{N_x}\\frac{DM_X(M_i,b_{\\perp})}{1+z_i}+\\frac{DM_{host}}{1+z_s}\n\nNote that the b⟂b_{\\perp} term is the physical impact parameter, which is a measure of how close the line of sight passes to higher-mass regions in the intersecting halo. A higher b⟂b_{\\perp} means that the line of sight is on the outskirts of the matter distribution and vice versa. Also, the subscript XX just means intersection. DM_{IGM} generally dominates for sources beyond z≈0.2z \\approx 0.2 unless the FRB is in an ununsual galaxy. Let us now define a ‘cosmological DM’ as DMcos≡DMX+DMIGMDM_{cos}\\equiv DM_{X}+DM_{IGM}. Our goal is to now find out what the average sightline’s DM from the IGM and intervening halos would be.\n\n\n\nThe Total Electron Content is simply the columnar number density of electrons where the column is along a line of sight. It is obtained by integrating the electron density along a path dsds: ∫0lne(s)ds\\int_{0}^{l}n_e(s)ds, where ll is the straight-line distance the radio source and the observer. It is a measure of how the underlying plasma interferes with radio signals passing through it.\nThe dispersion measure is the exact same thing. It is a measure of how the intermediary plasma affects radio waves passing through it. The catch is that the DM is measured on galactic scales; we must take into account the effects of relativity. The dispersion measure is defined as ∫0zne(z)dl\\int_{0}^{z}n_e(z)dl, where zz is the redshift and dldl is the proper distance element. The proper distance is the actual distance between two objects in the universe at a certain point in cosmic time. What does this mean? We know the universe is expanding. If we want to measure the distance between two objects at a certain point in time, we simply freeze the universe and observe it from a god’s-eye perspective. The proper distance is now the distance between the two objects. The proper distance is related to the comoving distance by multiplying with a simple factor (called the scale factor), given the universe is described by the FLRW metric. The differential of the proper distance element is related to the Hubble factor and redshift by the equation dl=cdzH(z)dl=c\\frac{dz}{H(z)}.\nBecause of how the redshift is defined, the physical baryon density scales with redshift as (1+z)3(1+z)^3. The DM is therefore a redshift corrected version of the TEC. It is correct to use it when considering things on intergalactic scales, not on interstellar scales.\n\nSince the DM is a measure of interference by electrons, we need to figure out how to relate this with baryons. We know that the number of protons and electrons are roughly the same in the universe. Of course, some of the baryons could be locked up in stars like neutrons in neutron stars; some of the protons could be flying around on their own without an electron to balance them. Let’s relate our dispersion measure to baryons. First, let fd(z)f_d(z) be the fraction of all baryons lying in the diffuse ionized IGM or halos (not in stars, cold gas, and so on), and let fe(z)f_e(z) be the effective number of free electrons per baryon (if they’re electrically neutral, how can they intefere with radio waves?).\nThen ne(z)=fd(z)fe(z)n_e(z)=f_d(z)f_e(z). The proper baryon density at redshift zz scales as (1+z)3(1+z)^3, so you can write ne(z)=fd(z)fe(z)Ωbρc,0(1+z)3/mpn_e(z)=f_d(z)f_e(z)\\Omega_b \\rho_{c,0} (1+z)^3/m_p\nwhere Ωb\\Omega_b is the present-day baryon density parameter (the cosmic baryon abundance), ρc,0=3H028πG\\rho_{c,0}=\\frac{3H_0^2}{8\\pi G} is the present critical density for a flat universe, and mpm_p is the proton mass (so ρc,0mp\\frac{\\rho_{c,0}}{m_p} counts the number of baryons).\nTherefore, ne(z)=fd(z)fe(z)(Ωb3H028πG(1+z)3)1mpn_e(z)=f_d(z)f_e(z)(\\Omega_b\\frac{3H_0^2}{8 \\pi G}(1+z)^3)\\frac{1}{m_p}.\nThe Hubble parameter at redshift zz for a spatially flat FRW universe with matter fraction Ωm\\Omega_m and cosmological constant (the dark energy parameter) ΩΛ\\Omega_{\\Lambda} is H(z)=H0Ωm(1+z)3+ΩΛH(z)=H_0 \\sqrt{\\Omega_m (1+z)^3+ \\Omega_\\Lambda}\nand dl=cH(z)dzdl=\\frac{c}{H(z)}dz, so dl=cH0dzΩm(1+z)3+ΩΛdl=\\frac{c}{H_0}\\frac{dz}{\\sqrt{\\Omega_m (1+z)^3+ \\Omega_\\Lambda}}\nTherefore the average sightline DM would be &lt;DMcos&gt;=3c8πΩbGH0mp∫0zs(1+z)fd(z)fe(z)Ωm(1+z)3+ΩΛ&lt;DM_{cos}&gt;=\\frac{3c}{8\\pi}\\frac{\\Omega_b}{G}\\frac{H_0}{m_p} \\int_{0}^{z_s}\\frac{(1+z)f_d(z)f_e(z)}{\\sqrt{\\Omega_m (1+z)^3+ \\Omega_\\Lambda}}\nwhere the extra 1+z1+z in the denominator comes from how many factors in the group-delay integral you want to account for.\nWe take fe=0.875f_e=0.875, fdf_d as a constant, and h70≡H0(70kms−1Mpc−1)h_{70}\\equiv H_0(70 km s^{-1} Mpc^{-1}), &lt;DMcos&gt;≈1085zfd(Ωbh700/04703)pccm3&lt;DM_{cos}&gt; \\approx 1085zf_d(\\frac{\\Omega_b h_{70}}{0/04703}) \\frac{pc}{cm^3} for zs≲1z_s \\lesssim 1.\nFor each FRB, we compute the extragalactic DM distribution DMext=DMobs−DMMWDM_{ext}=DM_{obs}-DM_{MW} as a function of the source redshift zsz_s. This means that we compute a 1D likelihood function P(DMex|zs,θ→)P(DM_{ex} | z_s, \\vec{\\theta}), where the model parameters θ→\\vec{\\theta} are given by θ→={fIGM,fX,μhost,σhost}.\\vec{\\theta}=\\{f_{IGM}, f_X, \\mu_{host}, \\sigma_{host}\\}. Here fIGMf_{IGM} is the fraction of baryons in the IGM (ΩIGMΩb\\frac{\\Omega_{IGM}}{\\Omega_b}) and fX=ΩhalosΩbf_X=\\frac{\\Omega_{halos}}{\\Omega_{b}} is the fraction of baryons in the intersected halos referenced (normalized) to redshift 0.1. Why can this be done? It’s because the three components of the dispersion measure - DMIGMDM_{IGM},DMXDM_{X}, and DMMWDM_{MW} all have different dependencies on the redshift (their distributions P(DM|z)P(DM|z) are different) given a sufficiently large sample size. μhost\\mu_{host} and σhost\\sigma_{host} are the mean dispersion measure and scattering coefficient of the host galaxy of the source, respectively. Note that the IGM is defined as the gas outside of virialized (i.e. stable) dark matter halos.\n\n\n\n\nBayes’ theorem allows one to compute the probability of a cause given its effect. Given a prior belief of the hypothesis (the prior), you update it with new data (the likelihood), and compute the probability of the cause given the effect (the posterior probability or posterior) according to the formula P(θ|D)=P(D|θ)P(θ)P(D)P(\\theta|D)=\\frac{P(D|\\theta) P(\\theta)}{P(D)} where P(θ|D)P(\\theta|D) is the posterior, P(D|θ)P(D|\\theta) is the likelihood, P(θ)P(\\theta) is the prior, and P(D)P(D) is the marginal likelihood or evidence - the total probability of the effect itself occurring, considering all possible causes.\n\nWe want to find out the joint distribution that describes the probability that DMexDM_{ex} is a certain value given a certain value of the redshift zz. To do this, we compute the 1D likelihood function given above for each FRB, and multiply them together. (∏inP(DMex,i|zs,i)P(θ→)\\prod_{i}^{n}P(DM_{ex,i}|z_{s,i})P(\\vec{\\theta})). To do this, we apply Markov Chain Monte Carlo (MCMC) methods.\n\n\n\n\nSampling from a probability distribution means randomly selecting points where the likelihood of selecting a given point is defined by the probability distribution. To sample from a one-dimensional probability distribution is easy - suppose someone said to you ‘I have a Gaussian probability distribution with a mean of 3.5 and a standard deviation of 2. You can take as many numbers as you want and I’ll give them to you. Since it is a normal distribution, you’re more likely to get a bunch of numbers around 3.5; once you get a bunch of numbers you can compute how many fell between 3.2 and 3.63, 1.32 and 1.39, and so on’. You could simply get a bunch of numbers (say 3266 numbers) and run some tests on them and go ‘okay, makes sense, I see that I have 299 numbers between 3.2 and 3.63 and 29 between 1.32 and 1.39. This is fairly reasonable’. The numbers you chose are samples of that probability distribution and you chose them one at a time.\nThen the person (let’s say the ghost of your great grandfather) comes up to you and says ‘I have a probability distribution defined as the convolution of the Riemann Zeta function and the Von Mangoldt function for 55681 variables, but the convolution is only computed at roots of the normal numbers between 4 and 48575’. You are stuck. How would you even go about sampling this distribution? You’re not allowed to ask him to give you numbers, because even he doesn’t want to compute things. This is where MCMC methods come in.\nLet us focus on how MCMC methods broadly work. You want to sample from a probability distribution. Why do you want to do this? If you have enough samples then you can compute averages (more precisely, expectation values), answer probabilistic questions, or even just flat out plot it. You say, “Okay, let me start with an initial guess. For lack of a better guess, I’ll start with a completely random guess. Then I look at points near to my guess and pick one. How? Let’s just go with any random point near me. Then I decide whether to move in that direction or not. If yes, I move in that direction and repeat the procedure. Otherwise I pick another point. If I keep moving for long enough I will probably cover the whole probability distribution.”\nMore precisely, given some underlying knowledge of the probability distribution, you can sample from it and construct a rough approximation of the actual probability distribution. This underlying knowledge may also be based on hunches: assume that at least, locally, the distribution is Gaussian.\nThe process of getting this sample from the underlying knowledge is called a Monte Carlo method and the construction of a series of steps (a straightforward example of a random walk) is an example of generating a Markov Chain. The implicit assumption is that the next step only depends on your current state, so this is a first-order Markov chain.\n\nWhat distribution do we assume for the host contribution to the dispersion measure? Let’s go with a log-normal distribution, as it is right-skewed and defined only for positive values. We are going to estimate μhost\\mu_{host} and σhost\\sigma_{host}. The prior values for μhost\\mu_{host} are assumed to be from a uniform probability distribution between 0 and 7 (therefore the DM spans 0 to 1000 pc cm−3pc\\text{ }cm^{-3}) and the same flat prior shape is used for fIGMf_{IGM} and fXf_{X}, but the values range from 0 to 1. The added constraint is fIGM+fX≤1f_{IGM}+f_X\\leq 1.\nFitting this distribution to our dataset gives fIGM=0.80−0.09+0.08f_{IGM}=0.80^{+0.08}_{-0.09} and fX=0.11−0.07+0.10f_{X}=0.11^{+0.10}_{-0.07}. We calculate that μhost=4.90−0.20+0.18\\mu_{host}=4.90^{+0.18}_{-0.20} and σhost=0.56−0.14+0.16\\sigma_{host}=0.56^{+0.16}_{-0.14}. This means that the median rest-frame contribution of the host to the DM is 130−23+25 pc cm−3130^{+25}_{-23}\\text{ } pc \\text{ }cm^{-3} for our sample. We fit subsamples of the data (such as DSA-110 detected sources) and do jackknife resampling (such as leave-one-out and compute every sample, and compute each statistic for every sample set, and take the mean), and end up observing that sources with zs≳0.5z_s\\gtrsim 0.5 constrain the range of the parameters significantly. But our data prefer a large fraction of baryonic material in the IGm and a large total diffuse fraction fdf_d. This is probably because there are too few sources with low DMexDM_{ex} per unit redshift (called the DM cliff). As an example, none of our sources beyond redshift 0.1 has DMexzs&lt;800 pc cm−3\\frac{DM_{ex}}{z_s}&lt;800 \\text{ }pc \\text{ } cm^{-3}. If the extragalactic DM was dominated by intervening halos or host galaxies then P(DMex|zs)P(DM_{ex} | z_s) would be smaller for low DMs because of the increased line-of-sight variance. Incidentally, this would be true for a universe where baryons perfectly trace the dark matter. But the IGM provides a significant statistical floor for DM per unit distance because most sightlines intersect dozens of filaments and even cosmic voids have a considerable electron column present. Our FRB sample rules out scenarios where baryons trace dark matter (fIGMf_{IGM} is low and a large portion of missing baryons are confined to galaxy halos).\nAlthough FRB DMs are affected by ionized gas in galaxy groups and clusters, the most precise constraints on the baryon budget in massive halos come from X-ray and SZ measurements. As a reminder, thermal X-ray emission is ∝∫ne2dl\\propto \\int n_e^2dl and SZ is ∝∫neTedl\\propto \\int n_eT_edl. This means that both measurements are sensitive to large, dense regions of hot gas. FRBs pick up DM from ionized plasmas in the way. The hot baryon fraction in halos (fhotf_{hot}) is a function of halo mass, approaching the cosmological ratio ≈ΩbΩM\\approx \\frac{\\Omega_b}{\\Omega_M} for most massive galaxy clusters. It should be noted that this quantity is less certain for halos below 1014h70−1M⊙10^{14} h_{70}^{-1}M_\\odot, but recent advances in sample sizes and measurement precision has significantly improved our knowledge of both the cluster mass function and fhotf_{hot}. Combining these multiwavelength observations allows us to estimate the fraction of the universe’s baryons in the hot gas of galaxy groups and clusters. We find fICM=3.75±0.5%f_{ICM}=3.75 \\pm 0.5\\% of all baryons in the intracluster medium (ICM). For galaxy groups with 1012.7M⊙≤Mh≤1014M⊙10^{12.7}M_{\\odot} \\leq M_h \\leq 10^{14} M_{\\odot} this number is 5.4±1.0%5.4 \\pm 1.0 \\% and therefore, a simple addition shows that roughly 9% of baryons are in a diffuse ionized state in massive halos.\nWhat about the baryons in galaxies, including stars and cold gas? They are the last major group of baryons left. The majority of the cold gas is neutral atomic hydrogen (along with a little molecular hydrogen and helium). 21cm HI surveys at low redshifts measure the HI mass function which is then integrated to estimate the neutral hydrogen density. Taking ΩHI,ΩH2\\Omega_{HI}, \\Omega_{H_2} from recent surveys we find fHI=9.6−2.3+3.8×10−3,fH2=1.6−0.4+0.8×10−3,fcold=1.1−0.2+0.3×10−2f_{HI}=9.6^{+3.8}_{-2.3} \\times 10^{-3}, f_{H_2}=1.6^{+0.8}_{-0.4} \\times 10^{-3}, f_{cold}=1.1^{+0.3}_{-0.2} \\times 10^{-2}. This means just over one percent of the Universe’s baryons are in cold neutral gas within galaxies. For stars estimating this fraction is significantly more difficult. The fraction itself is larger as well. Most stellar mass is in low-mass stars and we can easily see from earlier discussion that the choice of the initial mass function affects the baryon fraction f*f_* quite a bit. The Salpeter IMF (bottom heavy) can return a value of 14%. Instead we can use the Chabrier IMF and a smooth fit to multiple measurements of the local stellar density to get about 4-7%.\nAfter all of this, we can fully account for the missing baryons by combining the FRB results with other observations. We can also partition the baryons into the IGM, galaxy groups, galaxy clusters, and galaxies. A significant majority of baryonic matter resides in the IGM, outside of virialized halos. From the FRB-independent analysis X-ray groups and clusters, 9.2−1.61.6%9.2^{1.6}_{-1.6}\\% are in an ionized phase occupying massive halos. Roughly one percent are in cold neutral gas in galaxies. The conclusion is that the circumgalactic medium (CGM) of individual galaxies cannot contain a substantial fraction of the baryons in the universe. This is in agreement with detailed studies of individual FRB source sightlines. Our FRBs that intersect one or more foreground galaxy CGM at low impact parameters do not have significant excess dispersion. We find fgas=0.22−0.17+0.23ΩbΩMf_{gas}=0.22^{+0.23}_{-0.17} \\frac{\\Omega_b}{\\Omega_M} for 109M⊙&lt;M&lt;5×1012M⊙10^9 M_\\odot &lt; M &lt; 5 \\times 10^{12} M_\\odot, below the cosmic average.\nWhat do these results mean? The first basic observation is that feedback processes are required to expel gas and/or prevent gas from falling into their potential wells. We cannot differentiate between specific methods but our conclusion of a rich IGM and baryon-deficient CGM is consistent with simulations where feedback suppresses lower-mass baryon halos. As a reminder, fIGMf_{IGM} is about 0.8. This agrees with other simulations. In SIMBA simulation with feedback turned off, fIGM≈0.6f_{IGM}\\approx 0.6 by z&lt;1z&lt;1 and &gt;0.85&gt;0.85 with AGN feedback turned on. In the IllustrisTNG simulation fIGM≈0.8f_{IGM}\\approx 0.8 at low redshifts and baryons were missing from the CGM of Milky Way-like galaxies. Statistical cross-correlations of galaxy surveys with X-ray and kinematic SZ also agreed and showed that there were few baryons in galaxy halos.\nSo far we have shown that fIGMf_{IGM} is high and agrees with simulations. Let’s check if another method leads to the same answer. This other method should not rely on a partition of the cosmic baryons as the quantity is sensitive to both the intergalactic and intervening halo gas. To do this, we can use the average cosmic dispersion derived above. This results in fd=0.93−0.05+0.04f_d=0.93^{+0.04}_{-0.05}, independent of any assumptions about Pcos(DMIGM,DMX)P_{cos}(DM_{IGM}, DM_{X}) and its redshift evolution.\nThe mean cosmological DM of FRBs places a ceiling on the total stellar mass of the universe because f*&lt;1−fd−fcoldf_*&lt;1-f_d-f_{cold}. The results our our analysis suggest that over 90% of baryons are in a diffuse state in the IGM and in dark matter halos or cold gas (read: not in stars). This is independent of galaxy spectral energy distribution modeling, choice of the IMF, and the low mass cutoff for other methods. As we have repeatedly stated, most of the stellar mass is in numerous small stars. Our f*f_* constrains the mean stellar IMF. We place a 90% upper limit on the stellar baryon fraction at low redshifts of f≤9%f\\leq 9\\% and therefore ρ*≤5.6×108M⊙Mpc−3\\rho_* \\leq 5.6 \\times 10^8 M_\\odot Mpc^{-3} and therefore Salpeter IMFs can be ruled out for a low-mass cutoff below 0.10 M⊙M_\\odot.\nIf the Universe’s total cosmic baryon content is made a free parameter then Ωbh70=0.049−0.003+0.004\\Omega_bh_{70}=0.049^{+0.004}_{-0.003}. This measurement at a late time (remember that we are measuring at h70h_{70}) is consistent at the sub-10% level with early Universe constraints of the physical baryon density from Big Bang Nucleosynthesis and the CMB. An equally precise constraint can be obtained for the Hubble constant - H0=71−6+6km s−1 Mpc−1H_0=71^{+6}_{-6} km \\text{ } s^{-1} \\text{ } Mpc^{-1} with the caveat that disactually calculating this value H0H_0 from Ωb\\Omega_b requires fixing the baryon density parameter at the value it would be in the early universe. A possible future task could be cross-correlating a large sample of FRB DMs against other measurements that indicate the large-scale structure. This will enable even tighter bounds of some cosmological parameters and also allow measuring astrophysical feedback."
  },
  {
    "objectID": "blog/2024-12-01-partitioning/index.html#about",
    "href": "blog/2024-12-01-partitioning/index.html#about",
    "title": "Papers explained: Partitioning the Universe’s baryons (Connor et al 2024)",
    "section": "",
    "text": "Connor et al. (2024) recently published a paper that shows a way to partition the universe’s baryons to reveal the underlying cosmic web structure and showed that it contains a lot of gas. Is it possible to explain this paper to mildly intelligent people with an interest in astronomy?\n\n\nApproximately half of the universe’s dark matter (that is, matter that does not interact with electromagnetic radiation such as light, so we can’t see it) resides in structures known as ‘collapsed halos’. Collapsed halos are ellipsoidal or quasi-spherical structures surrounding a central object like a star. The initial universe’s density field was approximately uniform, but quantum fluctuations led to changes in the initial density field at different points. Changes in density eventually led to gravity taking over and clustering the matter into halo-like structures. Significantly less than half of the universe’s baryons (protons and neutrons; electrons are leptons because they interact only using the weak nuclear force and the electromagnetic force) are confined to halos. A small fraction of the baryons are present as stars and in the interstellar medium (ISM) between galaxies. A lot of them are very diffuse (less than 1000 per cubic centimeter) and ionized (less than one in ten thousand is ionized in the baryonic gas) and are located in the halos of galaxy clusters, galaxy groups (literally just some galaxies together but not enough to be a cluster), or individual galaxies. Because this gas is so diffuse and ionized, the quantity and spatial distribution is very difficult to measure. If we measure it really well and are able to provide an accurate answer to the question ‘what is this quantity and spatial distribution?’ then we have an answer to a lot of important questions in galaxy formation, astrophysical feedback (the ejection of matter by stars, black holes, etc into the universe, or the reverse!), and precision cosmology. Recently, people have used Fast Radio Bursts (FRBs), very violent emissions of radio waves first discovered in 2007, to measure the total content of cosmic baryons. The problem in those studies is that they did not contain methods to discriminate between the intergalactic material (IGM) and halo gases. In this paper we present a large cosmological sample of FRB sources localized to their host galaxies. Also we have managed to partition (that is, determine the distribution of) missing baryons into the IGM, galaxy clusters, and galaxies. As a reminder, the term missing baryons refers to the following problem: cosmological models say that the number of baryons in the universe should be so-and-so, but actual measurements show that there are much fewer baryons. Then where are the missing baryons? The study provides a late-Universe (that is, the current universe) measurement of the total baryon density of Ωb​h70\\Omega_{b}​h_{70} as 0.049 $$0.003, where Ωb\\Omega_{b} is the universe’s critical density (the density needed for a flat universe) that is made of baryonic matter. h70h_{70} is simply a scaling factor related to the Hubble constant. The results indicate that feedback processes can expel gas from galaxy halos and into the intergalactic medium, agreeing with the enriched cosmic web scenario (that is, how the cosmic web contains elements heavier than hydrogen and helium) seen in cosmological simulations. We measured a large diffuse baryon fraction and that probably means that the distribution of stellar masses in a population of stars (the initial stellar mass function) is probably not bottom-heavy (it does not predict a large number of low mass stars, which in turn live for longer).\n\n\n\nThe Deep Synoptic Array-110 is a radio inteferometer (a system of radio telescopes that observe the same source at the same time) operating between 1.28-1.53 GHz and is the first radio telescope built to detect and localize FRBs (that is, find out the galaxy they originate from). Doing this helps use FRBs as cosmological tools and unveil their physical origin. 60 new FRBs were discovered by DSA-110 and 39 of them now have an associated host galaxy spectroscopic redshift. Our companion work shows the properties of the host galaxies of a uniformly selected subset of these. The work also includes 9 sources not in that sample. Three new FRBs near or beyond redshift 1 (redshift 1 means ~8-9 billion years old, as the wavelength of light has doubled) are also presented. Because these FRBs are at redshift 1, they constrain the IGM column (that is, the distribution of material along the line of sight) by virtue of their great distance.\nWe add our sample to 30 previously localized FRB sources. The distribution of extragalactic DM (dispersion measure - the TEC, but measured in terms of redshift) and redshift for the full sample is plotted in figure 1; the positions, DM, redshifts, and detection instruments are displayed in extended data table 1. The extragalactic DM of localized FRBs tells us how many diffuse baryons are in what place in the Universe. The total observed DM of an FRB can be split into several components. Let’s take a moment to think about this. When looking at a radio source occurring so far away, we need to consider several things. First, we are looking out of the Milky Way, so the Milky Way’s ISM contributes to the total DM. The Milky Way’s halo also contributes to it. The inter galactic medium along the line of sight is of course always present. There is also some ionized gas in stable halos (virialized halos). Then there are also the contributions from each ionized halo along the line of sight. To account for the expansion of the universe and relativistic motion the factor 11+zhalo\\frac{1}{1+z_{halo}} is applied to the observed dispersion measure. Finally there is the contribution of the FRB host galaxy’s matter (note that since we are now talking about plasmas, it means that we must consider fermionic matter such as electrons too), which may come from its halo, ISM, or any other plasma lying around the galaxy.\nWe can therefore write the observed FRB of a galaxy in the following way: DMobs=DMMW+DMIGM(zs)+∑iNxDMX(Mi,b⟂)1+zi+DMhost1+zs\nDM_{obs}=DM_{MW}+DM_{IGM}(z_s)+\\sum_{i}^{N_x}\\frac{DM_X(M_i,b_{\\perp})}{1+z_i}+\\frac{DM_{host}}{1+z_s}\n\nNote that the b⟂b_{\\perp} term is the physical impact parameter, which is a measure of how close the line of sight passes to higher-mass regions in the intersecting halo. A higher b⟂b_{\\perp} means that the line of sight is on the outskirts of the matter distribution and vice versa. Also, the subscript XX just means intersection. DM_{IGM} generally dominates for sources beyond z≈0.2z \\approx 0.2 unless the FRB is in an ununsual galaxy. Let us now define a ‘cosmological DM’ as DMcos≡DMX+DMIGMDM_{cos}\\equiv DM_{X}+DM_{IGM}. Our goal is to now find out what the average sightline’s DM from the IGM and intervening halos would be.\n\n\n\nThe Total Electron Content is simply the columnar number density of electrons where the column is along a line of sight. It is obtained by integrating the electron density along a path dsds: ∫0lne(s)ds\\int_{0}^{l}n_e(s)ds, where ll is the straight-line distance the radio source and the observer. It is a measure of how the underlying plasma interferes with radio signals passing through it.\nThe dispersion measure is the exact same thing. It is a measure of how the intermediary plasma affects radio waves passing through it. The catch is that the DM is measured on galactic scales; we must take into account the effects of relativity. The dispersion measure is defined as ∫0zne(z)dl\\int_{0}^{z}n_e(z)dl, where zz is the redshift and dldl is the proper distance element. The proper distance is the actual distance between two objects in the universe at a certain point in cosmic time. What does this mean? We know the universe is expanding. If we want to measure the distance between two objects at a certain point in time, we simply freeze the universe and observe it from a god’s-eye perspective. The proper distance is now the distance between the two objects. The proper distance is related to the comoving distance by multiplying with a simple factor (called the scale factor), given the universe is described by the FLRW metric. The differential of the proper distance element is related to the Hubble factor and redshift by the equation dl=cdzH(z)dl=c\\frac{dz}{H(z)}.\nBecause of how the redshift is defined, the physical baryon density scales with redshift as (1+z)3(1+z)^3. The DM is therefore a redshift corrected version of the TEC. It is correct to use it when considering things on intergalactic scales, not on interstellar scales.\n\nSince the DM is a measure of interference by electrons, we need to figure out how to relate this with baryons. We know that the number of protons and electrons are roughly the same in the universe. Of course, some of the baryons could be locked up in stars like neutrons in neutron stars; some of the protons could be flying around on their own without an electron to balance them. Let’s relate our dispersion measure to baryons. First, let fd(z)f_d(z) be the fraction of all baryons lying in the diffuse ionized IGM or halos (not in stars, cold gas, and so on), and let fe(z)f_e(z) be the effective number of free electrons per baryon (if they’re electrically neutral, how can they intefere with radio waves?).\nThen ne(z)=fd(z)fe(z)n_e(z)=f_d(z)f_e(z). The proper baryon density at redshift zz scales as (1+z)3(1+z)^3, so you can write ne(z)=fd(z)fe(z)Ωbρc,0(1+z)3/mpn_e(z)=f_d(z)f_e(z)\\Omega_b \\rho_{c,0} (1+z)^3/m_p\nwhere Ωb\\Omega_b is the present-day baryon density parameter (the cosmic baryon abundance), ρc,0=3H028πG\\rho_{c,0}=\\frac{3H_0^2}{8\\pi G} is the present critical density for a flat universe, and mpm_p is the proton mass (so ρc,0mp\\frac{\\rho_{c,0}}{m_p} counts the number of baryons).\nTherefore, ne(z)=fd(z)fe(z)(Ωb3H028πG(1+z)3)1mpn_e(z)=f_d(z)f_e(z)(\\Omega_b\\frac{3H_0^2}{8 \\pi G}(1+z)^3)\\frac{1}{m_p}.\nThe Hubble parameter at redshift zz for a spatially flat FRW universe with matter fraction Ωm\\Omega_m and cosmological constant (the dark energy parameter) ΩΛ\\Omega_{\\Lambda} is H(z)=H0Ωm(1+z)3+ΩΛH(z)=H_0 \\sqrt{\\Omega_m (1+z)^3+ \\Omega_\\Lambda}\nand dl=cH(z)dzdl=\\frac{c}{H(z)}dz, so dl=cH0dzΩm(1+z)3+ΩΛdl=\\frac{c}{H_0}\\frac{dz}{\\sqrt{\\Omega_m (1+z)^3+ \\Omega_\\Lambda}}\nTherefore the average sightline DM would be &lt;DMcos&gt;=3c8πΩbGH0mp∫0zs(1+z)fd(z)fe(z)Ωm(1+z)3+ΩΛ&lt;DM_{cos}&gt;=\\frac{3c}{8\\pi}\\frac{\\Omega_b}{G}\\frac{H_0}{m_p} \\int_{0}^{z_s}\\frac{(1+z)f_d(z)f_e(z)}{\\sqrt{\\Omega_m (1+z)^3+ \\Omega_\\Lambda}}\nwhere the extra 1+z1+z in the denominator comes from how many factors in the group-delay integral you want to account for.\nWe take fe=0.875f_e=0.875, fdf_d as a constant, and h70≡H0(70kms−1Mpc−1)h_{70}\\equiv H_0(70 km s^{-1} Mpc^{-1}), &lt;DMcos&gt;≈1085zfd(Ωbh700/04703)pccm3&lt;DM_{cos}&gt; \\approx 1085zf_d(\\frac{\\Omega_b h_{70}}{0/04703}) \\frac{pc}{cm^3} for zs≲1z_s \\lesssim 1.\nFor each FRB, we compute the extragalactic DM distribution DMext=DMobs−DMMWDM_{ext}=DM_{obs}-DM_{MW} as a function of the source redshift zsz_s. This means that we compute a 1D likelihood function P(DMex|zs,θ→)P(DM_{ex} | z_s, \\vec{\\theta}), where the model parameters θ→\\vec{\\theta} are given by θ→={fIGM,fX,μhost,σhost}.\\vec{\\theta}=\\{f_{IGM}, f_X, \\mu_{host}, \\sigma_{host}\\}. Here fIGMf_{IGM} is the fraction of baryons in the IGM (ΩIGMΩb\\frac{\\Omega_{IGM}}{\\Omega_b}) and fX=ΩhalosΩbf_X=\\frac{\\Omega_{halos}}{\\Omega_{b}} is the fraction of baryons in the intersected halos referenced (normalized) to redshift 0.1. Why can this be done? It’s because the three components of the dispersion measure - DMIGMDM_{IGM},DMXDM_{X}, and DMMWDM_{MW} all have different dependencies on the redshift (their distributions P(DM|z)P(DM|z) are different) given a sufficiently large sample size. μhost\\mu_{host} and σhost\\sigma_{host} are the mean dispersion measure and scattering coefficient of the host galaxy of the source, respectively. Note that the IGM is defined as the gas outside of virialized (i.e. stable) dark matter halos.\n\n\n\n\nBayes’ theorem allows one to compute the probability of a cause given its effect. Given a prior belief of the hypothesis (the prior), you update it with new data (the likelihood), and compute the probability of the cause given the effect (the posterior probability or posterior) according to the formula P(θ|D)=P(D|θ)P(θ)P(D)P(\\theta|D)=\\frac{P(D|\\theta) P(\\theta)}{P(D)} where P(θ|D)P(\\theta|D) is the posterior, P(D|θ)P(D|\\theta) is the likelihood, P(θ)P(\\theta) is the prior, and P(D)P(D) is the marginal likelihood or evidence - the total probability of the effect itself occurring, considering all possible causes.\n\nWe want to find out the joint distribution that describes the probability that DMexDM_{ex} is a certain value given a certain value of the redshift zz. To do this, we compute the 1D likelihood function given above for each FRB, and multiply them together. (∏inP(DMex,i|zs,i)P(θ→)\\prod_{i}^{n}P(DM_{ex,i}|z_{s,i})P(\\vec{\\theta})). To do this, we apply Markov Chain Monte Carlo (MCMC) methods.\n\n\n\n\nSampling from a probability distribution means randomly selecting points where the likelihood of selecting a given point is defined by the probability distribution. To sample from a one-dimensional probability distribution is easy - suppose someone said to you ‘I have a Gaussian probability distribution with a mean of 3.5 and a standard deviation of 2. You can take as many numbers as you want and I’ll give them to you. Since it is a normal distribution, you’re more likely to get a bunch of numbers around 3.5; once you get a bunch of numbers you can compute how many fell between 3.2 and 3.63, 1.32 and 1.39, and so on’. You could simply get a bunch of numbers (say 3266 numbers) and run some tests on them and go ‘okay, makes sense, I see that I have 299 numbers between 3.2 and 3.63 and 29 between 1.32 and 1.39. This is fairly reasonable’. The numbers you chose are samples of that probability distribution and you chose them one at a time.\nThen the person (let’s say the ghost of your great grandfather) comes up to you and says ‘I have a probability distribution defined as the convolution of the Riemann Zeta function and the Von Mangoldt function for 55681 variables, but the convolution is only computed at roots of the normal numbers between 4 and 48575’. You are stuck. How would you even go about sampling this distribution? You’re not allowed to ask him to give you numbers, because even he doesn’t want to compute things. This is where MCMC methods come in.\nLet us focus on how MCMC methods broadly work. You want to sample from a probability distribution. Why do you want to do this? If you have enough samples then you can compute averages (more precisely, expectation values), answer probabilistic questions, or even just flat out plot it. You say, “Okay, let me start with an initial guess. For lack of a better guess, I’ll start with a completely random guess. Then I look at points near to my guess and pick one. How? Let’s just go with any random point near me. Then I decide whether to move in that direction or not. If yes, I move in that direction and repeat the procedure. Otherwise I pick another point. If I keep moving for long enough I will probably cover the whole probability distribution.”\nMore precisely, given some underlying knowledge of the probability distribution, you can sample from it and construct a rough approximation of the actual probability distribution. This underlying knowledge may also be based on hunches: assume that at least, locally, the distribution is Gaussian.\nThe process of getting this sample from the underlying knowledge is called a Monte Carlo method and the construction of a series of steps (a straightforward example of a random walk) is an example of generating a Markov Chain. The implicit assumption is that the next step only depends on your current state, so this is a first-order Markov chain.\n\nWhat distribution do we assume for the host contribution to the dispersion measure? Let’s go with a log-normal distribution, as it is right-skewed and defined only for positive values. We are going to estimate μhost\\mu_{host} and σhost\\sigma_{host}. The prior values for μhost\\mu_{host} are assumed to be from a uniform probability distribution between 0 and 7 (therefore the DM spans 0 to 1000 pc cm−3pc\\text{ }cm^{-3}) and the same flat prior shape is used for fIGMf_{IGM} and fXf_{X}, but the values range from 0 to 1. The added constraint is fIGM+fX≤1f_{IGM}+f_X\\leq 1.\nFitting this distribution to our dataset gives fIGM=0.80−0.09+0.08f_{IGM}=0.80^{+0.08}_{-0.09} and fX=0.11−0.07+0.10f_{X}=0.11^{+0.10}_{-0.07}. We calculate that μhost=4.90−0.20+0.18\\mu_{host}=4.90^{+0.18}_{-0.20} and σhost=0.56−0.14+0.16\\sigma_{host}=0.56^{+0.16}_{-0.14}. This means that the median rest-frame contribution of the host to the DM is 130−23+25 pc cm−3130^{+25}_{-23}\\text{ } pc \\text{ }cm^{-3} for our sample. We fit subsamples of the data (such as DSA-110 detected sources) and do jackknife resampling (such as leave-one-out and compute every sample, and compute each statistic for every sample set, and take the mean), and end up observing that sources with zs≳0.5z_s\\gtrsim 0.5 constrain the range of the parameters significantly. But our data prefer a large fraction of baryonic material in the IGm and a large total diffuse fraction fdf_d. This is probably because there are too few sources with low DMexDM_{ex} per unit redshift (called the DM cliff). As an example, none of our sources beyond redshift 0.1 has DMexzs&lt;800 pc cm−3\\frac{DM_{ex}}{z_s}&lt;800 \\text{ }pc \\text{ } cm^{-3}. If the extragalactic DM was dominated by intervening halos or host galaxies then P(DMex|zs)P(DM_{ex} | z_s) would be smaller for low DMs because of the increased line-of-sight variance. Incidentally, this would be true for a universe where baryons perfectly trace the dark matter. But the IGM provides a significant statistical floor for DM per unit distance because most sightlines intersect dozens of filaments and even cosmic voids have a considerable electron column present. Our FRB sample rules out scenarios where baryons trace dark matter (fIGMf_{IGM} is low and a large portion of missing baryons are confined to galaxy halos).\nAlthough FRB DMs are affected by ionized gas in galaxy groups and clusters, the most precise constraints on the baryon budget in massive halos come from X-ray and SZ measurements. As a reminder, thermal X-ray emission is ∝∫ne2dl\\propto \\int n_e^2dl and SZ is ∝∫neTedl\\propto \\int n_eT_edl. This means that both measurements are sensitive to large, dense regions of hot gas. FRBs pick up DM from ionized plasmas in the way. The hot baryon fraction in halos (fhotf_{hot}) is a function of halo mass, approaching the cosmological ratio ≈ΩbΩM\\approx \\frac{\\Omega_b}{\\Omega_M} for most massive galaxy clusters. It should be noted that this quantity is less certain for halos below 1014h70−1M⊙10^{14} h_{70}^{-1}M_\\odot, but recent advances in sample sizes and measurement precision has significantly improved our knowledge of both the cluster mass function and fhotf_{hot}. Combining these multiwavelength observations allows us to estimate the fraction of the universe’s baryons in the hot gas of galaxy groups and clusters. We find fICM=3.75±0.5%f_{ICM}=3.75 \\pm 0.5\\% of all baryons in the intracluster medium (ICM). For galaxy groups with 1012.7M⊙≤Mh≤1014M⊙10^{12.7}M_{\\odot} \\leq M_h \\leq 10^{14} M_{\\odot} this number is 5.4±1.0%5.4 \\pm 1.0 \\% and therefore, a simple addition shows that roughly 9% of baryons are in a diffuse ionized state in massive halos.\nWhat about the baryons in galaxies, including stars and cold gas? They are the last major group of baryons left. The majority of the cold gas is neutral atomic hydrogen (along with a little molecular hydrogen and helium). 21cm HI surveys at low redshifts measure the HI mass function which is then integrated to estimate the neutral hydrogen density. Taking ΩHI,ΩH2\\Omega_{HI}, \\Omega_{H_2} from recent surveys we find fHI=9.6−2.3+3.8×10−3,fH2=1.6−0.4+0.8×10−3,fcold=1.1−0.2+0.3×10−2f_{HI}=9.6^{+3.8}_{-2.3} \\times 10^{-3}, f_{H_2}=1.6^{+0.8}_{-0.4} \\times 10^{-3}, f_{cold}=1.1^{+0.3}_{-0.2} \\times 10^{-2}. This means just over one percent of the Universe’s baryons are in cold neutral gas within galaxies. For stars estimating this fraction is significantly more difficult. The fraction itself is larger as well. Most stellar mass is in low-mass stars and we can easily see from earlier discussion that the choice of the initial mass function affects the baryon fraction f*f_* quite a bit. The Salpeter IMF (bottom heavy) can return a value of 14%. Instead we can use the Chabrier IMF and a smooth fit to multiple measurements of the local stellar density to get about 4-7%.\nAfter all of this, we can fully account for the missing baryons by combining the FRB results with other observations. We can also partition the baryons into the IGM, galaxy groups, galaxy clusters, and galaxies. A significant majority of baryonic matter resides in the IGM, outside of virialized halos. From the FRB-independent analysis X-ray groups and clusters, 9.2−1.61.6%9.2^{1.6}_{-1.6}\\% are in an ionized phase occupying massive halos. Roughly one percent are in cold neutral gas in galaxies. The conclusion is that the circumgalactic medium (CGM) of individual galaxies cannot contain a substantial fraction of the baryons in the universe. This is in agreement with detailed studies of individual FRB source sightlines. Our FRBs that intersect one or more foreground galaxy CGM at low impact parameters do not have significant excess dispersion. We find fgas=0.22−0.17+0.23ΩbΩMf_{gas}=0.22^{+0.23}_{-0.17} \\frac{\\Omega_b}{\\Omega_M} for 109M⊙&lt;M&lt;5×1012M⊙10^9 M_\\odot &lt; M &lt; 5 \\times 10^{12} M_\\odot, below the cosmic average.\nWhat do these results mean? The first basic observation is that feedback processes are required to expel gas and/or prevent gas from falling into their potential wells. We cannot differentiate between specific methods but our conclusion of a rich IGM and baryon-deficient CGM is consistent with simulations where feedback suppresses lower-mass baryon halos. As a reminder, fIGMf_{IGM} is about 0.8. This agrees with other simulations. In SIMBA simulation with feedback turned off, fIGM≈0.6f_{IGM}\\approx 0.6 by z&lt;1z&lt;1 and &gt;0.85&gt;0.85 with AGN feedback turned on. In the IllustrisTNG simulation fIGM≈0.8f_{IGM}\\approx 0.8 at low redshifts and baryons were missing from the CGM of Milky Way-like galaxies. Statistical cross-correlations of galaxy surveys with X-ray and kinematic SZ also agreed and showed that there were few baryons in galaxy halos.\nSo far we have shown that fIGMf_{IGM} is high and agrees with simulations. Let’s check if another method leads to the same answer. This other method should not rely on a partition of the cosmic baryons as the quantity is sensitive to both the intergalactic and intervening halo gas. To do this, we can use the average cosmic dispersion derived above. This results in fd=0.93−0.05+0.04f_d=0.93^{+0.04}_{-0.05}, independent of any assumptions about Pcos(DMIGM,DMX)P_{cos}(DM_{IGM}, DM_{X}) and its redshift evolution.\nThe mean cosmological DM of FRBs places a ceiling on the total stellar mass of the universe because f*&lt;1−fd−fcoldf_*&lt;1-f_d-f_{cold}. The results our our analysis suggest that over 90% of baryons are in a diffuse state in the IGM and in dark matter halos or cold gas (read: not in stars). This is independent of galaxy spectral energy distribution modeling, choice of the IMF, and the low mass cutoff for other methods. As we have repeatedly stated, most of the stellar mass is in numerous small stars. Our f*f_* constrains the mean stellar IMF. We place a 90% upper limit on the stellar baryon fraction at low redshifts of f≤9%f\\leq 9\\% and therefore ρ*≤5.6×108M⊙Mpc−3\\rho_* \\leq 5.6 \\times 10^8 M_\\odot Mpc^{-3} and therefore Salpeter IMFs can be ruled out for a low-mass cutoff below 0.10 M⊙M_\\odot.\nIf the Universe’s total cosmic baryon content is made a free parameter then Ωbh70=0.049−0.003+0.004\\Omega_bh_{70}=0.049^{+0.004}_{-0.003}. This measurement at a late time (remember that we are measuring at h70h_{70}) is consistent at the sub-10% level with early Universe constraints of the physical baryon density from Big Bang Nucleosynthesis and the CMB. An equally precise constraint can be obtained for the Hubble constant - H0=71−6+6km s−1 Mpc−1H_0=71^{+6}_{-6} km \\text{ } s^{-1} \\text{ } Mpc^{-1} with the caveat that disactually calculating this value H0H_0 from Ωb\\Omega_b requires fixing the baryon density parameter at the value it would be in the early universe. A possible future task could be cross-correlating a large sample of FRB DMs against other measurements that indicate the large-scale structure. This will enable even tighter bounds of some cosmological parameters and also allow measuring astrophysical feedback."
  },
  {
    "objectID": "blog/2025-02-10-ml-research/index.html",
    "href": "blog/2025-02-10-ml-research/index.html",
    "title": "How to get into AI research as an outsider",
    "section": "",
    "text": "This post offers my perspective on how to break into AI research. Although it is divided into several chapters, I will primarily discuss two things. The first is what AI research is really like, and the second is what to do in AI research depending on your aims."
  },
  {
    "objectID": "blog/2025-02-10-ml-research/index.html#preliminaries",
    "href": "blog/2025-02-10-ml-research/index.html#preliminaries",
    "title": "How to get into AI research as an outsider",
    "section": "Preliminaries",
    "text": "Preliminaries\nLet’s not beat around the bush. The reason a lot of people want to get into AI research is because they want to be rich, respected, ‘in the know’, or be able to say ‘I did it first’; primarily the first reason. It’s the same with any fad. It was the same with cryptocurrency. Bitcoin opened at $10,077.40 on November 29, 2017 and nearly doubled in price three weeks later, before correcting to below $10,000 on February 2 of the following year. It felt like practically every financial technology company launched an initial coin offering (think initial public offering, but crypto) in the summer of 2018 based on nothing more than a whitepaper proving how their cryptocurrency would beat all the other cryptocurrencies on the market. Nearly all of the coins ended up being fraudulent schemes designed to extract money from unsuspecting customers. After regulations curtailed private investments that enabled these launches the volume of coins being traded plummeted.\nWhy did this happen? The main reason was that every financial company wanted to get in on this shiny new money-making opportunity because of fear of missing out. What did they do? They hired en masse, trying to set up teams that had domain knowledge. If they didn’t hire, no one else would. Venture capitalists and private equity offered money freely to people who presented murky plans detailing expected return on investment (often 100x or more) and promised the moon. What ended up happening? Bitcoin didn’t replace paper currency. The world didn’t move to public-ledger-only transactions. Governments introduced new regulations to tax cryptocurrency earnings. Large companies rolled out public projects for facilitating transactions with a verifiable token.\nIt’s the same with AI. The only major difference is that cryptocurrency was still locked behind a door that only people with the right knowledge could open. AI is accessible and usable by everyone, even children. Let me take a step back and define what exactly ‘AI’ means, and what people think it means. A person employed as an AI engineer, ML engineer, Data Scientist, or something similar primarily works on one of the following things. I personally categorize them in the following way:\n\nA data scientist is primarily concerned with statistical inference and hypothesis testing.\nA machine learning engineer is primarily concerned with writing code that builds models to solve a particular task.\nAn AI engineer is primarily someone that takes together open-source tools people have built and stacks them on top of each other like LEGO blocks.\n\nThese are not mutually exclusive. I know many ML engineers that only write backend software and use models available online. I know a few data scientists that primarily take models others have built and use them on tasks. I myself was an AI engineer that worked on model finetuning and data analytics."
  },
  {
    "objectID": "blog/2025-02-10-ml-research/index.html#what-ai-research-is-like",
    "href": "blog/2025-02-10-ml-research/index.html#what-ai-research-is-like",
    "title": "How to get into AI research as an outsider",
    "section": "What AI research is like",
    "text": "What AI research is like\nIn 2024 there were just under an average of 20000 submissions per month on arXiv. A paper published two years earlier showed the number of papers submitted per month in four ‘AI’ categories doubled every two years or so. By my count, there are about 155 separate categories on arXiv’s front page. This means that in 2024, 20000 papers were submitted per month to 155 categories. Under the fairly biased and weak assumption that the abovementioned trends hold, we can say that there were 8000 papers on AI submitted per month in 2024 to arXiv, or about 40% of all submissions were in these four categories.\nWe need to make a distinction between different types of AI research. A straightforward one can be made by classifying research as applied machine learning and non-applied machine learning.\nApplied machine learning is the application of pre-researched methods to a task. Papers such as Day-ahead regional solar power forecasting with hierarchical temporal convolutional neural networks are a prime example. The authors construct a model to do something, then apply it to the dataset and evaluate the results.\nNon-applied machine learning creates general methods and applies them to a variety of tasks. Deep Residual Learning for Image Recognition is an example. They create residual connections and show empirically that they are a huge improvement over all pre-existing models.\nWhen people say they want to get into AI research, they often mean non-applied machine learning research."
  },
  {
    "objectID": "blog/2025-02-10-ml-research/index.html#the-reality-of-research",
    "href": "blog/2025-02-10-ml-research/index.html#the-reality-of-research",
    "title": "How to get into AI research as an outsider",
    "section": "The reality of research",
    "text": "The reality of research\nBefore you think about AI research, you should probably know what research, the field, is like. A lot of people are drawn to research because they see it as a calling. But research as a field is populated by humans, and humans are biased. The big research jobs available at big companies (Applied Scientists at Amazon, Research Scientists at NVIDIA) are only available to people with PhDs. Because they are the biggest and highest-paying research jobs, they tend to take PhDs only from high-ranking universities.\nThere is no difference between AI research and other types of research, at least on the ‘research’ side. It still involves learning your field from first principles, reproducing other peoples’ work, coming up with hypotheses about what can be improved in the field and how to improve it, and finally rigorously testing everything before publishing."
  },
  {
    "objectID": "blog/2025-02-10-ml-research/index.html#practicalities",
    "href": "blog/2025-02-10-ml-research/index.html#practicalities",
    "title": "How to get into AI research as an outsider",
    "section": "Practicalities",
    "text": "Practicalities\n\nTooling\nOne of the big things when being introduced to a new field is learning the tooling. One of the good things about machine learning is that it is easily accessible. With tools like Google Colab, Paperspace’s Gradient, and Kaggle Notebooks, it is very easy for people to start working with models and datasets. The common theme among these is Python. Python has become synonymous with machine learning, with PyTorch being the library of choice for implementing most general-purpose models.\nIf you want to work more on the data science side of things, R is better than Python because of the sheer variety of algorithms implemented in it. If you want to work in scientific computing, Jax and Julia are better options.\nThe next part is the mathematics. People need to know the basics - undergraduate-level multivariate calculus, statistics, optimization theory, and possibly some formal mathematical proving before working in machine learning. A popular book is Mathematics of Machine Learning. The two holy books of the field are ESL and ISL. To get into deep learning, read Deep Learning.\n\n\nEducational requirements\nDo you need to be in higher education to start working in AI research? The answer without any caveats is no. AI is one of the few fields that allows people unaffiliated with an institution to submit research works to a conference. But you have a much higher chance of your research getting results if you are in higher education.\n\n\nIf you’re looking to start\nIf you know someone who needs a problem solved with machine learning, start working. Identifying which problems need to be solved with machine learning is an art in itself.\nIf you’re an undergraduate student then you should actively reach out to professors. Many professors are happy to take undergraduate students and assign them to a project. If you’re a Master’s student then it’s significantly more difficult: you have to prove to professors that 1. you have the technical skill and 2. enough domain knowledge to immediately start making a contribution.\nA good technique is to cold email a lot of people. More than 99% of the time you will not get a reply. Be honest about your skills and shortcomings and hope for the best.\n\n\nMindset\nI can throw around buzzwords like resilience, mental fortitude, determination, discipline, but that’s not quite what’s required. Like anything, you do need to have some amount of natural talent in thinking through a problem before tackling it. The second thing is that you need to have Sitzfleisch, the ability to carry on under any circumstances. You only get this with practice and liking the work you do. Hard work, when properly applied, is the only thing that makes you successful."
  },
  {
    "objectID": "textbooks.html",
    "href": "textbooks.html",
    "title": "Textbooks",
    "section": "",
    "text": "Online textbooks and lecture notes."
  },
  {
    "objectID": "textbooks.html#published",
    "href": "textbooks.html#published",
    "title": "Textbooks",
    "section": "Published",
    "text": "Published\nNo textbooks published yet."
  },
  {
    "objectID": "textbooks.html#in-progress",
    "href": "textbooks.html#in-progress",
    "title": "Textbooks",
    "section": "In Progress",
    "text": "In Progress\nTextbooks in progress are kept in private repositories until release."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Presentations and slides from conferences, seminars, and workshops.\nNo talks uploaded yet."
  },
  {
    "objectID": "guide.html",
    "href": "guide.html",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "A comprehensive guide to working with this Quarto-based personal website.\n\n\n\nPrerequisites\nDirectory Structure\nQuick Start\nBuilding the Site\nLayout and Design\nBlog Posts\nTalks (Presentations)\nBooks (Textbooks)\nCV and Resume\nLanding Pages\nStyling\nDeployment\nTroubleshooting\n\n\n\n\n\n\n\n\nQuarto (v1.3+): https://quarto.org/docs/get-started/\nquarto --version\n\n\n\n\n\nTypst: https://typst.app/\ntypst --version\n\n\n\n\n\nChrome/Chromium (for RevealJS → PDF conversion)\n\n\n\n\n# Install Quarto (Linux/macOS via Homebrew)\nbrew install quarto\n\n# Install Typst\nbrew install typst\n\n# Or download from:\n# - Quarto: https://quarto.org/docs/download/\n# - Typst: https://github.com/typst/typst/releases\n\n\n\n\n\nksd3.github.io/\n├── _quarto.yml           # Main site configuration\n├── styles.css            # Custom CSS (dark mode, layout, typography)\n├── guide.md              # This guide\n│\n├── index.qmd             # Home page\n├── research.qmd          # Research page\n├── talks.qmd             # Talks listing page\n├── textbooks.qmd         # Books listing page\n├── cv.qmd                # Resume/CV landing page (embeds PDFs)\n│\n├── blog/                 # Blog posts\n│   ├── index.qmd         # Blog listing (auto-generated)\n│   ├── 2024-05-01-bu-courses/   # Example: Courses post\n│   ├── 2024-12-01-partitioning/\n│   └── YYYY-MM-DD-slug/  # Post template\n│       └── index.qmd\n│\n├── talks/                # Presentations\n│   └── _template/        # Template for new talks\n│       ├── index.qmd     # RevealJS + Typst output\n│       └── images/\n│\n├── books/                # Textbooks\n│   └── _template/        # Template for new books\n│       ├── _quarto.yml   # Book config\n│       ├── index.qmd\n│       ├── chapter1.qmd\n│       └── chapter2.qmd\n│\n├── cv/                   # CV and Resume (Typst PDF only)\n│   ├── cv.qmd            # Full academic CV → cv.pdf\n│   └── resume.qmd        # One-page resume → resume.pdf\n│\n├── images/               # Shared images\n├── videos/               # Video files\n└── docs/                 # Generated output (deploy this)\n\n\n\n\n# Build CV/Resume PDFs first (required for cv.qmd page)\nquarto render cv/resume.qmd\nquarto render cv/cv.qmd\n\n# Preview the site locally (hot reload)\nquarto preview\n\n# Build the entire site\nquarto render\nThe site opens at http://localhost:4000 during preview.\n\n\n\n\n\n\n# Build PDFs first, then site\nquarto render cv/resume.qmd && quarto render cv/cv.qmd && quarto render\n\n\n\n# Single page\nquarto render research.qmd\n\n# Single blog post\nquarto render blog/2024-05-01-bu-courses/index.qmd\n\n# Single talk (generates HTML + PDF)\nquarto render talks/YYYY-talk-name/\n\n# Single book (generates HTML + PDF)\nquarto render books/my-book/\n\n# CV/Resume PDFs\nquarto render cv/cv.qmd\nquarto render cv/resume.qmd\n\n\n\n\n\n\n\nThe site uses a wide layout inspired by gwern.net:\n┌─────────────────────────────────────────────────────────┐\n│  Home   Blog   Research   Textbooks   Talks   Resume/CV │  ← navbar\n├────────────┬────────────────────────────────────────────┤\n│ ON THIS    │                                            │\n│ PAGE       │  Main Content                              │\n│            │                                            │\n│ Section 1  │  Wide layout (up to 1400px)               │\n│ Section 2  │  Serif body text, sans-serif headings      │\n│ Section 3  │                                            │\n└────────────┴────────────────────────────────────────────┘\n   TOC sidebar    (fixed, shows page sections)\n\n\n\n\nWide content area: Up to 1400px max-width\nLeft TOC sidebar: Shows current page sections (sticky, scrollable)\nDark mode: Auto-detects system preference\nTypography: Serif body (Source Serif Pro), sans headings (Source Sans Pro)\nNavbar: Top navigation for site sections\n\n\n\n\nDark mode is automatic based on system preference (prefers-color-scheme). No toggle needed—it follows your OS setting.\n\n\n\n\n\n\n\n\nCreate a directory:\nmkdir blog/YYYY-MM-DD-slug\nCreate blog/YYYY-MM-DD-slug/index.qmd:\n---\ntitle: \"Post Title\"\nauthor: \"Kshitij Duraphe\"\ndate: \"2025-01-18\"\ncategories: [category1, category2]\ndescription: \"Brief description for listings\"\n---\n\nYour content here in markdown...\nPreview:\nquarto preview blog/YYYY-MM-DD-slug/index.qmd\n\n\n\n\n\nAuto-sorting by date (newest first)\nCategories with counts\nRSS feed at /blog/index.xml\nTOC sidebar for long posts\n\n\n\n\nblog/2025-01-18-my-post/\n├── index.qmd           # Main content\n├── images/             # Post-specific images\n│   └── figure1.png\n└── data/               # Optional data files\n\n\n\n\n\nTalks output RevealJS (HTML slides) and Typst (PDF download).\n\n\n\nCopy the template:\ncp -r talks/_template talks/YYYY-talk-name\nEdit talks/YYYY-talk-name/index.qmd:\n---\ntitle: \"Talk Title\"\nsubtitle: \"Conference Name 2025\"\nauthor: \"Kshitij Duraphe\"\ndate: \"2025-01-18\"\nformat:\n  revealjs:\n    theme: simple\n    slide-number: true\n  typst:\n    papersize: presentation-16-9\n---\n\n## Slide 1\nContent here...\n\n## Slide 2\nMore content...\nBuild:\nquarto render talks/YYYY-talk-name/\nAdd entry to talks.qmd:\n### Talk Title\n*January 2025 — Conference Name*\n\nBrief description.\n\n[View Slides](talks/YYYY-talk-name/index.html) | [PDF](talks/YYYY-talk-name/index.pdf)\n\n\n\n\n## Regular Slide\nContent here.\n\n## Smaller Text {.smaller}\nLots of content...\n\n## Two Columns\n:::: {.columns}\n::: {.column width=\"50%\"}\nLeft content\n:::\n::: {.column width=\"50%\"}\nRight content\n:::\n::::\n\n## Speaker Notes\nContent visible to audience.\n\n::: {.notes}\nNotes only visible in speaker view (press 'S').\n:::\n\n\n\n\n\nBooks output HTML (read online) and Typst (PDF download).\n\n\n\nCopy the template:\ncp -r books/_template books/my-book\nEdit books/my-book/_quarto.yml:\nproject:\n  type: book\n  output-dir: _book\n\nbook:\n  title: \"Book Title\"\n  author: \"Kshitij Duraphe\"\n  chapters:\n    - index.qmd\n    - chapter1.qmd\n    - chapter2.qmd\n\nformat:\n  html:\n    theme: cosmo\n  typst:\n    toc: true\n    number-sections: true\nBuild:\nquarto render books/my-book/\nAdd entry to textbooks.qmd:\n### Book Title\nDescription here.\n\n[Read Online](books/my-book/_book/index.html) | [PDF](books/my-book/_book/Book-Title.pdf)\n\n\n\n\n\nCross-references: @sec-intro, @fig-chart, @eq-formula\nBibliography: Add references.bib, cite with [@key]\nChapter navigation: Auto-generated sidebar\n\n\n\n\n\n\nCV and Resume are Typst-only (PDF output). The landing page embeds the PDFs.\n\n\n\nEdit content in cv/cv.qmd or cv/resume.qmd\nBuild PDFs:\nquarto render cv/cv.qmd\nquarto render cv/resume.qmd\nThe cv.qmd landing page automatically embeds and links to the PDFs\n\n\n\n\nFind templates at https://typst.app/universe (search “cv” or “resume”).\n---\ntitle: \"Resume\"\nauthor: \"Kshitij Duraphe\"\nformat:\n  typst:\n    template: my-template.typ\n    # Or use a package:\n    include-in-header: |\n      #import \"@preview/modern-cv:0.7.0\": *\n---\n\n\n\n# Name\n\nemail@example.com | github.com/username | linkedin.com/in/username\n\n---\n\n## Education\n\n**University** | *Degree* | Year\n- Details\n\n## Experience\n\n**Company** | *Role* | Dates\n- Achievement with metrics\n\n## Skills\n\n**Languages:** Python, R, Julia\n\n\n\n\n\nLanding pages are .qmd files in the root directory.\n\n\n\nCreate newpage.qmd:\n---\ntitle: \"Page Title\"\n---\n\nContent here...\nAdd to navbar in _quarto.yml:\nwebsite:\n  navbar:\n    left:\n      - href: newpage.qmd\n        text: New Page\n\n\n\n\n::: {layout=\"[30,70]\"}\n![](images/photo.jpg)\n\nText content in the larger column.\n:::\n\n\n\n\n\n\n\nThe site uses styles.css with:\n\nCSS variables for colors (light/dark mode)\nWide layout (1400px max)\nFixed TOC sidebar (200px)\nSerif/sans typography\n\n\n\n\n:root {\n  --bg-color: #fefefe;\n  --text-color: #2c2c2c;\n  --link-color: #1a5f7a;\n  --border-color: #ddd;\n  --bg-subtle: #f7f7f7;\n  --sidebar-width: 200px;\n  --content-max-width: 1400px;\n}\n\n@media (prefers-color-scheme: dark) {\n  :root {\n    --bg-color: #181818;\n    --text-color: #e0e0e0;\n    --link-color: #6db3c9;\n    /* ... */\n  }\n}\n\n\n\nUse CSS classes for multi-column layouts:\n::: {.columns-2}\n- Item 1\n- Item 2\n- Item 3\n- Item 4\n:::\n\n\n\n\n\n\n\n\nBuild the site:\nquarto render cv/resume.qmd && quarto render cv/cv.qmd && quarto render\nCommit and push:\ngit add .\ngit commit -m \"Update site\"\ngit push\nIn GitHub repo Settings → Pages:\n\nSource: Deploy from a branch\nBranch: main\nFolder: /docs\n\n\n\n\n\nCreate .github/workflows/publish.yml:\nname: Build and Deploy\n\non:\n  push:\n    branches: [main]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Install Typst\n        run: |\n          curl -fsSL https://typst.community/typst-install/install.sh | sh\n          echo \"$HOME/.local/bin\" &gt;&gt; $GITHUB_PATH\n\n      - name: Build PDFs\n        run: |\n          quarto render cv/resume.qmd\n          quarto render cv/cv.qmd\n\n      - name: Render Site\n        run: quarto render\n\n      - name: Deploy\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./docs\n\n\n\n\n\n\n\n“Typst not found”\nbrew install typst\n# Or: https://github.com/typst/typst/releases\nPDFs not showing on cv.qmd page\n# Build the PDFs first\nquarto render cv/resume.qmd\nquarto render cv/cv.qmd\nTOC sidebar not appearing - Check that page doesn’t have toc: false in front matter - Sidebar only shows on pages with headings\nDark mode not working - Check your OS dark mode setting - Clear browser cache\nCSS not updating\nrm -rf .quarto\nquarto render\n\n\n\n\n\n\n\n\n\n\n\n\nTask\nCommand\n\n\n\n\nPreview site\nquarto preview\n\n\nBuild everything\nquarto render cv/resume.qmd && quarto render cv/cv.qmd && quarto render\n\n\nBuild single page\nquarto render page.qmd\n\n\nBuild talk\nquarto render talks/name/\n\n\nBuild book\nquarto render books/name/\n\n\nBuild CV/Resume\nquarto render cv/cv.qmd\n\n\n\n\n\n\n\n\nQuarto Documentation\nQuarto Websites\nQuarto Books\nQuarto Presentations\nRevealJS Options\nTypst Documentation\nTypst Universe (Templates)"
  },
  {
    "objectID": "guide.html#table-of-contents",
    "href": "guide.html#table-of-contents",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "Prerequisites\nDirectory Structure\nQuick Start\nBuilding the Site\nLayout and Design\nBlog Posts\nTalks (Presentations)\nBooks (Textbooks)\nCV and Resume\nLanding Pages\nStyling\nDeployment\nTroubleshooting"
  },
  {
    "objectID": "guide.html#prerequisites",
    "href": "guide.html#prerequisites",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "Quarto (v1.3+): https://quarto.org/docs/get-started/\nquarto --version\n\n\n\n\n\nTypst: https://typst.app/\ntypst --version\n\n\n\n\n\nChrome/Chromium (for RevealJS → PDF conversion)\n\n\n\n\n# Install Quarto (Linux/macOS via Homebrew)\nbrew install quarto\n\n# Install Typst\nbrew install typst\n\n# Or download from:\n# - Quarto: https://quarto.org/docs/download/\n# - Typst: https://github.com/typst/typst/releases"
  },
  {
    "objectID": "guide.html#directory-structure",
    "href": "guide.html#directory-structure",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "ksd3.github.io/\n├── _quarto.yml           # Main site configuration\n├── styles.css            # Custom CSS (dark mode, layout, typography)\n├── guide.md              # This guide\n│\n├── index.qmd             # Home page\n├── research.qmd          # Research page\n├── talks.qmd             # Talks listing page\n├── textbooks.qmd         # Books listing page\n├── cv.qmd                # Resume/CV landing page (embeds PDFs)\n│\n├── blog/                 # Blog posts\n│   ├── index.qmd         # Blog listing (auto-generated)\n│   ├── 2024-05-01-bu-courses/   # Example: Courses post\n│   ├── 2024-12-01-partitioning/\n│   └── YYYY-MM-DD-slug/  # Post template\n│       └── index.qmd\n│\n├── talks/                # Presentations\n│   └── _template/        # Template for new talks\n│       ├── index.qmd     # RevealJS + Typst output\n│       └── images/\n│\n├── books/                # Textbooks\n│   └── _template/        # Template for new books\n│       ├── _quarto.yml   # Book config\n│       ├── index.qmd\n│       ├── chapter1.qmd\n│       └── chapter2.qmd\n│\n├── cv/                   # CV and Resume (Typst PDF only)\n│   ├── cv.qmd            # Full academic CV → cv.pdf\n│   └── resume.qmd        # One-page resume → resume.pdf\n│\n├── images/               # Shared images\n├── videos/               # Video files\n└── docs/                 # Generated output (deploy this)"
  },
  {
    "objectID": "guide.html#quick-start",
    "href": "guide.html#quick-start",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "# Build CV/Resume PDFs first (required for cv.qmd page)\nquarto render cv/resume.qmd\nquarto render cv/cv.qmd\n\n# Preview the site locally (hot reload)\nquarto preview\n\n# Build the entire site\nquarto render\nThe site opens at http://localhost:4000 during preview."
  },
  {
    "objectID": "guide.html#building-the-site",
    "href": "guide.html#building-the-site",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "# Build PDFs first, then site\nquarto render cv/resume.qmd && quarto render cv/cv.qmd && quarto render\n\n\n\n# Single page\nquarto render research.qmd\n\n# Single blog post\nquarto render blog/2024-05-01-bu-courses/index.qmd\n\n# Single talk (generates HTML + PDF)\nquarto render talks/YYYY-talk-name/\n\n# Single book (generates HTML + PDF)\nquarto render books/my-book/\n\n# CV/Resume PDFs\nquarto render cv/cv.qmd\nquarto render cv/resume.qmd"
  },
  {
    "objectID": "guide.html#layout-and-design",
    "href": "guide.html#layout-and-design",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "The site uses a wide layout inspired by gwern.net:\n┌─────────────────────────────────────────────────────────┐\n│  Home   Blog   Research   Textbooks   Talks   Resume/CV │  ← navbar\n├────────────┬────────────────────────────────────────────┤\n│ ON THIS    │                                            │\n│ PAGE       │  Main Content                              │\n│            │                                            │\n│ Section 1  │  Wide layout (up to 1400px)               │\n│ Section 2  │  Serif body text, sans-serif headings      │\n│ Section 3  │                                            │\n└────────────┴────────────────────────────────────────────┘\n   TOC sidebar    (fixed, shows page sections)\n\n\n\n\nWide content area: Up to 1400px max-width\nLeft TOC sidebar: Shows current page sections (sticky, scrollable)\nDark mode: Auto-detects system preference\nTypography: Serif body (Source Serif Pro), sans headings (Source Sans Pro)\nNavbar: Top navigation for site sections\n\n\n\n\nDark mode is automatic based on system preference (prefers-color-scheme). No toggle needed—it follows your OS setting."
  },
  {
    "objectID": "guide.html#blog-posts",
    "href": "guide.html#blog-posts",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "Create a directory:\nmkdir blog/YYYY-MM-DD-slug\nCreate blog/YYYY-MM-DD-slug/index.qmd:\n---\ntitle: \"Post Title\"\nauthor: \"Kshitij Duraphe\"\ndate: \"2025-01-18\"\ncategories: [category1, category2]\ndescription: \"Brief description for listings\"\n---\n\nYour content here in markdown...\nPreview:\nquarto preview blog/YYYY-MM-DD-slug/index.qmd\n\n\n\n\n\nAuto-sorting by date (newest first)\nCategories with counts\nRSS feed at /blog/index.xml\nTOC sidebar for long posts\n\n\n\n\nblog/2025-01-18-my-post/\n├── index.qmd           # Main content\n├── images/             # Post-specific images\n│   └── figure1.png\n└── data/               # Optional data files"
  },
  {
    "objectID": "guide.html#talks-presentations",
    "href": "guide.html#talks-presentations",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "Talks output RevealJS (HTML slides) and Typst (PDF download).\n\n\n\nCopy the template:\ncp -r talks/_template talks/YYYY-talk-name\nEdit talks/YYYY-talk-name/index.qmd:\n---\ntitle: \"Talk Title\"\nsubtitle: \"Conference Name 2025\"\nauthor: \"Kshitij Duraphe\"\ndate: \"2025-01-18\"\nformat:\n  revealjs:\n    theme: simple\n    slide-number: true\n  typst:\n    papersize: presentation-16-9\n---\n\n## Slide 1\nContent here...\n\n## Slide 2\nMore content...\nBuild:\nquarto render talks/YYYY-talk-name/\nAdd entry to talks.qmd:\n### Talk Title\n*January 2025 — Conference Name*\n\nBrief description.\n\n[View Slides](talks/YYYY-talk-name/index.html) | [PDF](talks/YYYY-talk-name/index.pdf)\n\n\n\n\n## Regular Slide\nContent here.\n\n## Smaller Text {.smaller}\nLots of content...\n\n## Two Columns\n:::: {.columns}\n::: {.column width=\"50%\"}\nLeft content\n:::\n::: {.column width=\"50%\"}\nRight content\n:::\n::::\n\n## Speaker Notes\nContent visible to audience.\n\n::: {.notes}\nNotes only visible in speaker view (press 'S').\n:::"
  },
  {
    "objectID": "guide.html#books-textbooks",
    "href": "guide.html#books-textbooks",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "Books output HTML (read online) and Typst (PDF download).\n\n\n\nCopy the template:\ncp -r books/_template books/my-book\nEdit books/my-book/_quarto.yml:\nproject:\n  type: book\n  output-dir: _book\n\nbook:\n  title: \"Book Title\"\n  author: \"Kshitij Duraphe\"\n  chapters:\n    - index.qmd\n    - chapter1.qmd\n    - chapter2.qmd\n\nformat:\n  html:\n    theme: cosmo\n  typst:\n    toc: true\n    number-sections: true\nBuild:\nquarto render books/my-book/\nAdd entry to textbooks.qmd:\n### Book Title\nDescription here.\n\n[Read Online](books/my-book/_book/index.html) | [PDF](books/my-book/_book/Book-Title.pdf)\n\n\n\n\n\nCross-references: @sec-intro, @fig-chart, @eq-formula\nBibliography: Add references.bib, cite with [@key]\nChapter navigation: Auto-generated sidebar"
  },
  {
    "objectID": "guide.html#cv-and-resume",
    "href": "guide.html#cv-and-resume",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "CV and Resume are Typst-only (PDF output). The landing page embeds the PDFs.\n\n\n\nEdit content in cv/cv.qmd or cv/resume.qmd\nBuild PDFs:\nquarto render cv/cv.qmd\nquarto render cv/resume.qmd\nThe cv.qmd landing page automatically embeds and links to the PDFs\n\n\n\n\nFind templates at https://typst.app/universe (search “cv” or “resume”).\n---\ntitle: \"Resume\"\nauthor: \"Kshitij Duraphe\"\nformat:\n  typst:\n    template: my-template.typ\n    # Or use a package:\n    include-in-header: |\n      #import \"@preview/modern-cv:0.7.0\": *\n---\n\n\n\n# Name\n\nemail@example.com | github.com/username | linkedin.com/in/username\n\n---\n\n## Education\n\n**University** | *Degree* | Year\n- Details\n\n## Experience\n\n**Company** | *Role* | Dates\n- Achievement with metrics\n\n## Skills\n\n**Languages:** Python, R, Julia"
  },
  {
    "objectID": "guide.html#landing-pages",
    "href": "guide.html#landing-pages",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "Landing pages are .qmd files in the root directory.\n\n\n\nCreate newpage.qmd:\n---\ntitle: \"Page Title\"\n---\n\nContent here...\nAdd to navbar in _quarto.yml:\nwebsite:\n  navbar:\n    left:\n      - href: newpage.qmd\n        text: New Page\n\n\n\n\n::: {layout=\"[30,70]\"}\n![](images/photo.jpg)\n\nText content in the larger column.\n:::"
  },
  {
    "objectID": "guide.html#styling",
    "href": "guide.html#styling",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "The site uses styles.css with:\n\nCSS variables for colors (light/dark mode)\nWide layout (1400px max)\nFixed TOC sidebar (200px)\nSerif/sans typography\n\n\n\n\n:root {\n  --bg-color: #fefefe;\n  --text-color: #2c2c2c;\n  --link-color: #1a5f7a;\n  --border-color: #ddd;\n  --bg-subtle: #f7f7f7;\n  --sidebar-width: 200px;\n  --content-max-width: 1400px;\n}\n\n@media (prefers-color-scheme: dark) {\n  :root {\n    --bg-color: #181818;\n    --text-color: #e0e0e0;\n    --link-color: #6db3c9;\n    /* ... */\n  }\n}\n\n\n\nUse CSS classes for multi-column layouts:\n::: {.columns-2}\n- Item 1\n- Item 2\n- Item 3\n- Item 4\n:::"
  },
  {
    "objectID": "guide.html#deployment",
    "href": "guide.html#deployment",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "Build the site:\nquarto render cv/resume.qmd && quarto render cv/cv.qmd && quarto render\nCommit and push:\ngit add .\ngit commit -m \"Update site\"\ngit push\nIn GitHub repo Settings → Pages:\n\nSource: Deploy from a branch\nBranch: main\nFolder: /docs\n\n\n\n\n\nCreate .github/workflows/publish.yml:\nname: Build and Deploy\n\non:\n  push:\n    branches: [main]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Install Typst\n        run: |\n          curl -fsSL https://typst.community/typst-install/install.sh | sh\n          echo \"$HOME/.local/bin\" &gt;&gt; $GITHUB_PATH\n\n      - name: Build PDFs\n        run: |\n          quarto render cv/resume.qmd\n          quarto render cv/cv.qmd\n\n      - name: Render Site\n        run: quarto render\n\n      - name: Deploy\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./docs"
  },
  {
    "objectID": "guide.html#troubleshooting",
    "href": "guide.html#troubleshooting",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "“Typst not found”\nbrew install typst\n# Or: https://github.com/typst/typst/releases\nPDFs not showing on cv.qmd page\n# Build the PDFs first\nquarto render cv/resume.qmd\nquarto render cv/cv.qmd\nTOC sidebar not appearing - Check that page doesn’t have toc: false in front matter - Sidebar only shows on pages with headings\nDark mode not working - Check your OS dark mode setting - Clear browser cache\nCSS not updating\nrm -rf .quarto\nquarto render"
  },
  {
    "objectID": "guide.html#quick-reference",
    "href": "guide.html#quick-reference",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "Task\nCommand\n\n\n\n\nPreview site\nquarto preview\n\n\nBuild everything\nquarto render cv/resume.qmd && quarto render cv/cv.qmd && quarto render\n\n\nBuild single page\nquarto render page.qmd\n\n\nBuild talk\nquarto render talks/name/\n\n\nBuild book\nquarto render books/name/\n\n\nBuild CV/Resume\nquarto render cv/cv.qmd"
  },
  {
    "objectID": "guide.html#resources",
    "href": "guide.html#resources",
    "title": "Quarto Website Guide",
    "section": "",
    "text": "Quarto Documentation\nQuarto Websites\nQuarto Books\nQuarto Presentations\nRevealJS Options\nTypst Documentation\nTypst Universe (Templates)"
  }
]